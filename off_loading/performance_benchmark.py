#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
æ¯”è¾ƒä¼ ç»Ÿæ–¹æ³•ä¸CPU Offloadingæ–¹æ³•çš„æ€§èƒ½å·®å¼‚
æµ‹è¯•æ¨¡å‹é¢„æµ‹ä¸Šé™çš„æå‡æ•ˆæœ
"""

import os
import sys
import time
import psutil
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import json
import subprocess
import argparse
from typing import Dict, List, Tuple, Optional

# GPUå†…å­˜ç›‘æ§
try:
    import pynvml
    pynvml.nvmlInit()
    NVML_AVAILABLE = True
except ImportError:
    NVML_AVAILABLE = False
    print("âš ï¸  pynvmlä¸å¯ç”¨ï¼Œå°†æ— æ³•ç›‘æ§GPUå†…å­˜")


class PerformanceBenchmark:
    """
    æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»
    æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„æ¨¡å‹æ€§èƒ½
    """
    
    def __init__(self, base_script_path: str):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        Args:
            base_script_path: åŸºç¡€è„šæœ¬è·¯å¾„ï¼ˆåŸå§‹wind_1.pyæˆ–ä¼˜åŒ–åçš„wind_offloading.pyï¼‰
        """
        self.base_script_path = base_script_path
        self.results = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºç»“æœç›®å½•
        self.results_dir = Path(f'benchmark_results_{self.timestamp}')
        self.results_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“ ç»“æœå°†ä¿å­˜åˆ°: {self.results_dir}")

    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'cpu_count': psutil.cpu_count(),
            'memory_total_gb': psutil.virtual_memory().total / (1024**3),
            'python_version': sys.version,
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info['cuda_version'] = torch.version.cuda
            info['gpu_count'] = torch.cuda.device_count()
            info['gpu_names'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
            
            if NVML_AVAILABLE:
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                info['gpu_memory_total_gb'] = meminfo.total / (1024**3)
        
        return info

    def monitor_resources(self, duration: float = 1.0) -> Dict:
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        # CPUå’Œå†…å­˜ç›‘æ§
        cpu_percent = psutil.cpu_percent(interval=duration)
        memory = psutil.virtual_memory()
        
        result = {
            'cpu_percent': cpu_percent,
            'memory_used_gb': memory.used / (1024**3),
            'memory_percent': memory.percent,
        }
        
        # GPUç›‘æ§ - å¢å¼ºç‰ˆæœ¬
        if torch.cuda.is_available():
            try:
                # æ–¹æ³•1ï¼šä½¿ç”¨torch.cudaè·å–GPUå†…å­˜ä¿¡æ¯
                gpu_mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                gpu_mem_reserved = torch.cuda.memory_reserved(0) / (1024**3)
                
                # ä½¿ç”¨æœ€å¤§å€¼ä½œä¸ºGPUå†…å­˜ä½¿ç”¨é‡
                gpu_memory_used = max(gpu_mem_allocated, gpu_mem_reserved)
                
                result.update({
                    'gpu_memory_used_gb': gpu_memory_used,
                    'gpu_memory_allocated_gb': gpu_mem_allocated,
                    'gpu_memory_reserved_gb': gpu_mem_reserved,
                    # 'gpu_memory_cached_gb': gpu_mem_cached,
                })
                
                # æ–¹æ³•2ï¼šä½¿ç”¨nvidia-smiä½œä¸ºå¤‡ç”¨éªŒè¯
                try:
                    import subprocess
                    nvidia_smi_output = subprocess.check_output([
                        'nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'
                    ], text=True, timeout=5)
                    
                    lines = nvidia_smi_output.strip().split('\n')
                    for line in lines:
                        if line.strip():
                            parts = line.split(',')
                            if len(parts) >= 2:
                                gpu_memory_used_mb = float(parts[0].strip())
                                gpu_memory_total_mb = float(parts[1].strip())
                                nvidia_smi_gpu_mem = gpu_memory_used_mb / 1024  # è½¬æ¢ä¸ºGB
                                
                                # ä½¿ç”¨nvidia-smiå’Œtorch.cudaçš„æœ€å¤§å€¼
                                result['gpu_memory_used_gb'] = max(result['gpu_memory_used_gb'], nvidia_smi_gpu_mem)
                                result['gpu_memory_used_gb_nvidia_smi'] = nvidia_smi_gpu_mem
                                result['gpu_memory_total_gb'] = gpu_memory_total_mb / 1024
                                break
                                
                except Exception as e:
                    print(f"   nvidia-smiç›‘æ§å¤±è´¥: {e}")
                    pass
                
                # æ–¹æ³•3ï¼šä½¿ç”¨pynvmlï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if NVML_AVAILABLE:
                    try:
                        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                        meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)
                        
                        pynvml_gpu_mem = meminfo.used / (1024**3)
                        result['gpu_memory_used_gb'] = max(result['gpu_memory_used_gb'], pynvml_gpu_mem)
                        
                        result.update({
                            'gpu_memory_used_gb_pynvml': pynvml_gpu_mem,
                            'gpu_memory_percent': (meminfo.used / meminfo.total) * 100,
                            'gpu_utilization_percent': utilization.gpu,
                        })
                    except Exception as e:
                        print(f"   pynvmlç›‘æ§å¤±è´¥: {e}")
                        pass
                
                # å¦‚æœä»ç„¶ä¸º0ï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°GPUçŠ¶æ€
                if result['gpu_memory_used_gb'] == 0:
                    torch.cuda.synchronize()  # å¼ºåˆ¶åŒæ­¥GPUæ“ä½œ
                    torch.cuda.empty_cache()  # æ¸…ç©ºç¼“å­˜ä½†ä¸é‡Šæ”¾å†…å­˜
                    
                    # å†æ¬¡å°è¯•è·å–å†…å­˜ä¿¡æ¯
                    gpu_mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    gpu_mem_reserved = torch.cuda.memory_reserved(0) / (1024**3)
                    result['gpu_memory_used_gb'] = max(gpu_mem_allocated, gpu_mem_reserved)
                
            except Exception as e:
                print(f"âš ï¸ GPUç›‘æ§é”™è¯¯: {e}")
                # å³ä½¿å‡ºé”™ï¼Œä¹Ÿå°è¯•è·å–åŸºæœ¬ä¿¡æ¯
                try:
                    gpu_mem_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                    result['gpu_memory_used_gb'] = gpu_mem_allocated
                except Exception:
                    result['gpu_memory_used_gb'] = 0
        else:
            result['gpu_memory_used_gb'] = 0
        
        return result

    def run_single_test(self, 
                       test_config: Dict,
                       verbose_child: bool = False, 
                       timeout: Optional[int] = None) -> Dict:
        """
        è¿è¡Œå•ä¸ªæµ‹è¯•é…ç½®
        Args:
            test_config: æµ‹è¯•é…ç½®å­—å…¸
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸è®¾ç½®è¶…æ—¶
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        import threading
        print(f"\nğŸ”¬ å¼€å§‹æµ‹è¯•: {test_config['name']}")
        print(f"   é…ç½®: {test_config}")
        
        # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
        cmd = [sys.executable, self.base_script_path]
        for key, value in test_config.get('args', {}).items():
            if isinstance(value, bool):
                if value: cmd.append(f'--{key}')
            else:
                cmd.extend([f'--{key}', str(value)])

        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        start_time = time.time()

        try:
            process = subprocess.Popen(
                cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                text=True, encoding='utf-8', errors='replace',
                cwd=os.path.dirname(os.path.abspath(self.base_script_path))
            )

            stdout_lines, stderr_lines = [], []
            def reader_thread(pipe, storage, pipe_name):
                for line in pipe:
                    if verbose_child: # <--- æ ¹æ®verboseå‚æ•°å†³å®šæ˜¯å¦æ‰“å°
                        print(f"  [{pipe_name}] > {line.strip()}")
                    storage.append(line)

            stdout_thread = threading.Thread(target=reader_thread, args=(process.stdout, stdout_lines, "STDOUT"))
            stderr_thread = threading.Thread(target=reader_thread, args=(process.stderr, stderr_lines, "STDERR"))
            stdout_thread.start()
            stderr_thread.start()
            
            peak_gpu_mem_gb = 0
            peak_ram_mem_gb = 0
            
            spinner = ['-', '\\', '|', '/']
            spin_idx = 0
            
            print("ğŸ“Š å¼€å§‹å¹¶è¡Œç›‘æ§...", end='', flush=True)
            while process.poll() is None:
                if timeout is not None and time.time() - start_time > timeout:
                    process.terminate()
                    raise subprocess.TimeoutExpired(cmd, timeout)

                current_resources = self.monitor_resources(duration=0.5)
                
                # é‡æ–°åŠ å…¥å³°å€¼å†…å­˜çš„å®æ—¶æ‰“å°
                if 'gpu_memory_used_gb' in current_resources:
                    current_gpu_mem = current_resources.get('gpu_memory_used_gb', 0)
                    if current_gpu_mem > peak_gpu_mem_gb:
                        peak_gpu_mem_gb = current_gpu_mem
                        # \r å›åˆ°è¡Œé¦–ï¼Œæ‰“å°æ–°å³°å€¼ï¼Œend=''ä¸æ¢è¡Œï¼Œflushç«‹å³æ˜¾ç¤º
                        print(f"\rğŸ“Š ç›‘æ§ä¸­... ğŸ“ˆ æ–°GPUå³°å€¼: {peak_gpu_mem_gb:.2f}GB", end='', flush=True)

                if 'memory_used_gb' in current_resources:
                    peak_ram_mem_gb = max(peak_ram_mem_gb, current_resources.get('memory_used_gb', 0))
                
                # åœ¨å®‰é™æ¨¡å¼ä¸‹æ‰“å°ä¸€ä¸ªæ—‹è½¬çš„â€œç­‰å¾…â€å…‰æ ‡ï¼Œè¡¨ç¤ºç¨‹åºä»åœ¨è¿è¡Œ
                if not verbose_child:
                    print(f"\rğŸ“Š ç›‘æ§ä¸­... {spinner[spin_idx % len(spinner)]}", end='', flush=True)
                    spin_idx += 1

                time.sleep(0.5)

            # æ¸…ç†æœ€åä¸€è¡Œç›‘æ§è¾“å‡º
            print("\r" + " " * 50 + "\r", end='')

            stdout_thread.join()
            stderr_thread.join()
            process.wait()
            
            # (åç»­ç»“æœå¤„ç†é€»è¾‘ä¸å˜)
            # ...
            end_time = time.time()
            runtime = end_time - start_time
            return_code = process.returncode
            stdout = "".join(stdout_lines)
            stderr = "".join(stderr_lines)
            
            mse, mape = self._parse_performance_metrics(stdout + stderr)
            test_result = {
                'test_name': test_config['name'], 'config': test_config, 'success': return_code == 0,
                'runtime_seconds': runtime, 'max_memory_gb': peak_ram_mem_gb,
                'max_gpu_memory_gb': peak_gpu_mem_gb, 'mse': mse, 'mape': mape,
                'stdout': stdout, 'stderr': stderr, 'return_code': return_code,
            }
            if return_code == 0:
                print(f"âœ… æµ‹è¯•å®Œæˆ: {runtime:.2f}ç§’, å³°å€¼GPUå†…å­˜: {peak_gpu_mem_gb:.2f}GB")
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: è¿”å›ç  {return_code}")

        except Exception as e:
            # ...
            test_result = {'test_name': test_config['name'], 'config': test_config, 'success': False, 'error': str(e)}

        return test_result

    def _parse_performance_metrics(self, output: str) -> Tuple[Optional[float], Optional[float]]:
        """ä»è¾“å‡ºä¸­è§£ææ€§èƒ½æŒ‡æ ‡"""
        import re
        mse, mape = None, None
        
        print("ğŸ” è§£ææ€§èƒ½æŒ‡æ ‡...")
        
        mse_pattern = re.compile(r"(?:MSE|æœ€ç»ˆMSE)(?:\s*\([^)]+\))?:\s*([0-9.]+)")
        mape_pattern = re.compile(r"(?:MAPE|æœ€ç»ˆMAPE)(?:\s*\([^)]+\))?:\s*([0-9.]+)")

        # ä»åå¾€å‰æœç´¢ï¼Œä»¥è·å–æœ€ç»ˆçš„æ€»ç»“æ€§æŒ‡æ ‡
        for line in reversed(output.split('\n')):
            if mse is None:
                match = mse_pattern.search(line)
                if match:
                    try:
                        mse = float(match.group(1))
                    except ValueError:
                        pass
            
            if mape is None:
                match = mape_pattern.search(line)
                if match:
                    try:
                        mape = float(match.group(1))
                    except ValueError:
                        pass
            
            # å¦‚æœä¸¤ä¸ªæŒ‡æ ‡éƒ½æ‰¾åˆ°äº†ï¼Œå°±æå‰ç»“æŸ
            if mse is not None and mape is not None:
                break

        print(f"ğŸ“Š è§£æç»“æœ: MSE={mse}, MAPE={mape}")
        return mse, mape

    def run_sequence_length_test(self, method_name: str, base_config: Dict) -> List[Dict]:
        """
        è¿è¡Œåºåˆ—é•¿åº¦ä¸Šé™æµ‹è¯•ï¼šé€æ­¥å¢åŠ åºåˆ—é•¿åº¦ç›´åˆ°å†…å­˜ä¸è¶³
        """
        print(f"\nğŸ¯ å¼€å§‹{method_name}åºåˆ—é•¿åº¦ä¸Šé™æµ‹è¯•")
        print(f"ğŸ” æµ‹è¯•ç­–ç•¥ï¼šé€æ­¥å¢åŠ åºåˆ—é•¿åº¦ï¼Œæ‰¾åˆ°å¤„ç†ä¸Šé™")
        
        # å®šä¹‰ä¸åŒåºåˆ—é•¿åº¦çš„æµ‹è¯•é…ç½®
        # ä»è¾ƒå°çš„åºåˆ—é•¿åº¦å¼€å§‹ï¼Œé€æ­¥å¢åŠ 
        sequence_lengths = [1152, 1440, 1555, 1670, 1728, 2016]  # åŸºç¡€åºåˆ—é•¿åº¦
        
        # å¦‚æœæ˜¯CPU Offloadingç‰ˆæœ¬ï¼Œæµ‹è¯•æ›´å¤§çš„åºåˆ—é•¿åº¦
        if 'offloading' in method_name:
            sequence_lengths.extend([2304])  # æ›´å¤§çš„åºåˆ—é•¿åº¦
        
        scale_configs = []
        
        # ä¸ºæ¯ä¸ªåºåˆ—é•¿åº¦åˆ›å»ºæµ‹è¯•é…ç½®
        for seq_len in sequence_lengths:
            # æ ¹æ®åºåˆ—é•¿åº¦è°ƒæ•´å…¶ä»–å‚æ•°ä»¥ä¿æŒæµ‹è¯•çš„åˆç†æ€§
            # è¾ƒé•¿åºåˆ—ä½¿ç”¨è¾ƒå°çš„batch_sizeä»¥èŠ‚çœå†…å­˜
            if seq_len <= 144:
                batch_size = 64
                hidden_size = 264
                n_head = 8
            elif seq_len <= 576:
                batch_size = 32
                hidden_size = 264
                n_head = 8
            elif seq_len <= 2304:
                batch_size = 16
                hidden_size = 264
                n_head = 8
            else:
                batch_size = 8
                hidden_size = 128
                n_head = 4
            
            config = {
                'name': f'{method_name}_seq_{seq_len}',
                'args': {
                    **base_config,
                    'seq_length': seq_len,
                    'c_out': seq_len,  # é¢„æµ‹é•¿åº¦ç­‰äºåºåˆ—é•¿åº¦
                    'hidden_size': hidden_size,
                    'n_head': n_head,
                    'batch_size': batch_size,
                    'epochs': 2,  # ä½¿ç”¨è¾ƒå°‘epochsä»¥èŠ‚çœæ—¶é—´
                    'max_gpu_memory': '1GiB',
                    'patience': 1,  # å¿«é€Ÿåœæ­¢
                }
            }
            scale_configs.append(config)
        
        print(f"ğŸ“Š å°†æµ‹è¯• {len(scale_configs)} ç§åºåˆ—é•¿åº¦é…ç½®:")
        for config in scale_configs:
            args = config['args']
            print(f"   - åºåˆ—é•¿åº¦: {args['seq_length']}, æ‰¹æ¬¡: {args['batch_size']}, éšè—å±‚: {args['hidden_size']}")
            
        print(f"\nâš¡ å¼€å§‹é€æ­¥æµ‹è¯•ï¼Œé‡åˆ°å†…å­˜ä¸è¶³å°†åœæ­¢è¯¥æ–¹æ³•çš„åç»­æµ‹è¯•")
        
        scale_results = []
        max_successful_seq_length = 0
        
        for i, config in enumerate(scale_configs):
            seq_len = config['args']['seq_length']
            print(f"\nğŸ§ª æµ‹è¯•è¿›åº¦: [{i+1}/{len(scale_configs)}] åºåˆ—é•¿åº¦ {seq_len}")
            
            result = self.run_single_test(config, verbose_child=self.verbose_child)  # æ— è¶…æ—¶é™åˆ¶
            scale_results.append(result)
            self.results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            self._save_intermediate_results()
            
            if result.get('success', False):
                max_successful_seq_length = seq_len
                print(f"âœ… åºåˆ—é•¿åº¦ {seq_len} æµ‹è¯•æˆåŠŸ!")
                print(f"   å½“å‰{method_name}æœ€å¤§æˆåŠŸåºåˆ—é•¿åº¦: {max_successful_seq_length}")
            else:
                print(f"âŒ åºåˆ—é•¿åº¦ {seq_len} æµ‹è¯•å¤±è´¥!")
                print(f"ğŸ’¡ {method_name}çš„åºåˆ—é•¿åº¦ä¸Šé™ä¸º: {max_successful_seq_length}")
                
                # å¦‚æœæ˜¯å†…å­˜ä¸è¶³é”™è¯¯ï¼Œåœæ­¢åç»­æ›´å¤§åºåˆ—é•¿åº¦çš„æµ‹è¯•
                if 'out of memory' in str(result.get('error', '')).lower() or \
                   'cuda out of memory' in str(result.get('stdout', '')).lower():
                    print(f"ğŸ›‘ æ£€æµ‹åˆ°å†…å­˜ä¸è¶³ï¼Œåœæ­¢{method_name}çš„åç»­æµ‹è¯•")
                    break
                else:
                    print(f"âš ï¸  å…¶ä»–ç±»å‹é”™è¯¯ï¼Œç»§ç»­æµ‹è¯•ä¸‹ä¸€ä¸ªåºåˆ—é•¿åº¦...")
        
        print(f"\nğŸ“Š {method_name}åºåˆ—é•¿åº¦æµ‹è¯•æ€»ç»“:")
        print(f"   æœ€å¤§æˆåŠŸåºåˆ—é•¿åº¦: {max_successful_seq_length}")
        print(f"   æˆåŠŸæµ‹è¯•æ•°é‡: {len([r for r in scale_results if r.get('success', False)])}/{len(scale_configs)}")
        
        return scale_results

    def compare_methods(self, 
                       original_script: str, 
                       offloading_script: str,
                       base_config: Dict) -> Dict:
        """
        æ¯”è¾ƒåŸå§‹æ–¹æ³•å’ŒCPU Offloadingæ–¹æ³•
        """
        print("\n" + "="*60)
        print("ğŸ¥Š å¼€å§‹æ–¹æ³•å¯¹æ¯”æµ‹è¯•")
        print("="*60)
        
        results_comparison = {
            'system_info': self.get_system_info(),
            'timestamp': self.timestamp,
            'original_results': [],
            'offloading_results': [],
            'comparison_summary': {}
        }
        
        # æµ‹è¯•åŸå§‹æ–¹æ³•
        print("\n1ï¸âƒ£  æµ‹è¯•åŸå§‹æ–¹æ³•...")
        self.base_script_path = original_script
        original_results = self.run_sequence_length_test('original', base_config)
        results_comparison['original_results'] = original_results
        
        # æµ‹è¯•CPU Offloadingæ–¹æ³•
        print("\n2ï¸âƒ£  æµ‹è¯•CPU Offloadingæ–¹æ³•...")
        self.base_script_path = offloading_script
        offloading_results = self.run_sequence_length_test('offloading', base_config)
        results_comparison['offloading_results'] = offloading_results
        
        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        comparison_summary = self._analyze_comparison(original_results, offloading_results)
        results_comparison['comparison_summary'] = comparison_summary
        
        return results_comparison

    def _analyze_comparison(self, original_results: List[Dict], offloading_results: List[Dict]) -> Dict:
        """åˆ†æå¯¹æ¯”ç»“æœ"""
        summary = {
            'successful_tests': {
                'original': len([r for r in original_results if r.get('success', False)]),
                'offloading': len([r for r in offloading_results if r.get('success', False)])
            },
            'max_scale_achieved': {
                'original': None,
                'offloading': None
            },
            'performance_comparison': {},
            'resource_usage': {},
        }
        
        # æ‰¾åˆ°æˆåŠŸè¿è¡Œçš„æœ€å¤§è§„æ¨¡
        successful_original = [r for r in original_results if r.get('success', False)]
        successful_offloading = [r for r in offloading_results if r.get('success', False)]
        
        if successful_original:
            # æŒ‰seq_lengthæ’åºæ‰¾æœ€å¤§æˆåŠŸçš„é…ç½®
            max_original = max(successful_original, 
                             key=lambda x: x['config']['args'].get('seq_length', 0))
            summary['max_scale_achieved']['original'] = {
                'seq_length': max_original['config']['args'].get('seq_length'),
                'hidden_size': max_original['config']['args'].get('hidden_size'),
                'batch_size': max_original['config']['args'].get('batch_size'),
                'mse': max_original.get('mse'),
                'runtime': max_original.get('runtime_seconds')
            }
        
        if successful_offloading:
            max_offloading = max(successful_offloading,
                               key=lambda x: x['config']['args'].get('seq_length', 0))
            summary['max_scale_achieved']['offloading'] = {
                'seq_length': max_offloading['config']['args'].get('seq_length'),
                'hidden_size': max_offloading['config']['args'].get('hidden_size'),
                'batch_size': max_offloading['config']['args'].get('batch_size'),
                'mse': max_offloading.get('mse'),
                'runtime': max_offloading.get('runtime_seconds')
            }
        
        # è®¡ç®—åºåˆ—é•¿åº¦ä¸Šé™æå‡
        if summary['max_scale_achieved']['original'] and summary['max_scale_achieved']['offloading']:
            orig_seq_len = summary['max_scale_achieved']['original']['seq_length']
            off_seq_len = summary['max_scale_achieved']['offloading']['seq_length']
            
            summary['sequence_length_improvement'] = {
                'seq_length_ratio': off_seq_len / orig_seq_len if orig_seq_len > 0 else float('inf'),
                'improvement_percentage': ((off_seq_len - orig_seq_len) / orig_seq_len * 100) if orig_seq_len > 0 else float('inf'),
                'absolute_improvement': off_seq_len - orig_seq_len
            }
        
        return summary

    def _save_intermediate_results(self):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        results_file = self.results_dir / 'intermediate_results.json'
        with open(results_file, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)

    def generate_report(self, comparison_results: Dict):
        """ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_file = self.results_dir / 'full_results.json'
        with open(results_file, 'w') as f:
            json.dump(comparison_results, f, indent=2, default=str)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_file = self.results_dir / 'performance_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("ğŸš€ é£ç”µé¢„æµ‹æ¨¡å‹CPU Offloadingæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n")
            f.write(f"æµ‹è¯•æ—¶é—´: {self.timestamp}\n")
            f.write(f"Pythonç‰ˆæœ¬: {comparison_results['system_info']['python_version']}\n")
            f.write(f"PyTorchç‰ˆæœ¬: {comparison_results['system_info']['pytorch_version']}\n")
            f.write(f"GPUå¯ç”¨: {comparison_results['system_info']['cuda_available']}\n")
            
            if comparison_results['system_info']['cuda_available']:
                f.write(f"GPUå‹å·: {comparison_results['system_info']['gpu_names']}\n")
                gpu_mem_total = comparison_results['system_info'].get('gpu_memory_total_gb', 'N/A')
                if isinstance(gpu_mem_total, (int, float)):
                    f.write(f"GPUæ˜¾å­˜: {gpu_mem_total:.1f}GB\n")
                else:
                    f.write(f"GPUæ˜¾å­˜: {gpu_mem_total}\n")
                # f.write(f"GPUæ˜¾å­˜: {comparison_results['system_info'].get('gpu_memory_total_gb', 'N/A'):.1f}GB\n")
            
            memory_total = comparison_results['system_info'].get('memory_total_gb', 'N/A')
            if isinstance(memory_total, (int, float)):
                f.write(f"ç³»ç»Ÿå†…å­˜: {memory_total:.1f}GB\n")
            else:
                f.write(f"ç³»ç»Ÿå†…å­˜: {memory_total}\n")
            # f.write(f"ç³»ç»Ÿå†…å­˜: {comparison_results['system_info']['memory_total_gb']:.1f}GB\n")
            
            f.write("\nğŸ“ˆ æµ‹è¯•ç»“æœæ‘˜è¦:\n")
            f.write("-" * 40 + "\n")
            
            summary = comparison_results['comparison_summary']
            
            # æˆåŠŸæµ‹è¯•æ•°é‡
            f.write(f"åŸå§‹æ–¹æ³•æˆåŠŸæµ‹è¯•: {summary['successful_tests']['original']}\n")
            f.write(f"CPU OffloadingæˆåŠŸæµ‹è¯•: {summary['successful_tests']['offloading']}\n")
            
            # åºåˆ—é•¿åº¦ä¸Šé™å¯¹æ¯”
            f.write("\nğŸ¯ åºåˆ—é•¿åº¦å¤„ç†ä¸Šé™å¯¹æ¯”:\n")
            if summary['max_scale_achieved']['original']:
                orig = summary['max_scale_achieved']['original']
                f.write(f"åŸå§‹æ–¹æ³•æœ€å¤§åºåˆ—é•¿åº¦:\n")
                f.write(f"  - åºåˆ—é•¿åº¦: {orig['seq_length']}\n")
                f.write(f"  - éšè—å±‚å¤§å°: {orig['hidden_size']}\n")
                f.write(f"  - æ‰¹æ¬¡å¤§å°: {orig['batch_size']}\n")
                f.write(f"  - MSE: {orig['mse']:.6f}\n")
                f.write(f"  - è¿è¡Œæ—¶é—´: {orig['runtime']:.2f}ç§’\n")
            
            if summary['max_scale_achieved']['offloading']:
                off = summary['max_scale_achieved']['offloading']
                f.write(f"CPU Offloadingæœ€å¤§åºåˆ—é•¿åº¦:\n")
                f.write(f"  - åºåˆ—é•¿åº¦: {off['seq_length']}\n")
                f.write(f"  - éšè—å±‚å¤§å°: {off['hidden_size']}\n")
                f.write(f"  - æ‰¹æ¬¡å¤§å°: {off['batch_size']}\n")
                f.write(f"  - MSE: {off['mse']:.6f}\n")
                f.write(f"  - è¿è¡Œæ—¶é—´: {off['runtime']:.2f}ç§’\n")
            
            # åºåˆ—é•¿åº¦ä¸Šé™æå‡
            if 'sequence_length_improvement' in summary:
                imp = summary['sequence_length_improvement']
                f.write(f"\nğŸ‰ åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›æå‡:\n")
                f.write(f"åºåˆ—é•¿åº¦ä¸Šé™æå‡: {imp['improvement_percentage']:.1f}%\n")
                f.write(f"åºåˆ—é•¿åº¦å€æ•°: {imp['seq_length_ratio']:.2f}x\n")
                f.write(f"ç»å¯¹æå‡: +{imp['absolute_improvement']} ä¸ªæ—¶é—´æ­¥é•¿\n")
            
            f.write("\n" + "=" * 80 + "\n")
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self._create_visualizations(comparison_results)
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print(f"   ğŸ“Š å®Œæ•´æ•°æ®: {results_file}")
        print(f"   ğŸ“ˆ å›¾è¡¨ç›®å½•: {self.results_dir}")

    def _create_visualizations(self, comparison_results: Dict):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            # æ€§èƒ½å¯¹æ¯”å›¾
            self._plot_performance_comparison(comparison_results)
            
            # èµ„æºä½¿ç”¨å¯¹æ¯”å›¾
            self._plot_resource_usage(comparison_results)
            
            # åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›å›¾
            self._plot_sequence_length_capability(comparison_results)
            
        except Exception as e:
            print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")

    def _plot_performance_comparison(self, results: Dict):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‡†å¤‡æ•°æ®
        methods = []
        mse_values = []
        runtime_values = []
        
        for method, result_list in [('Original', results['original_results']), 
                                   ('CPU Offloading', results['offloading_results'])]:
            successful = [r for r in result_list if r.get('success', False) and r.get('mse') is not None]
            if successful:
                methods.append(method)
                mse_values.append([r['mse'] for r in successful])
                runtime_values.append([r['runtime_seconds'] for r in successful])
        
        # MSEå¯¹æ¯”
        if mse_values:
            ax1.boxplot(mse_values, labels=methods)
            ax1.set_title('MSEå¯¹æ¯”', fontsize=14, fontweight='bold')
            ax1.set_ylabel('MSE')
            ax1.grid(True, alpha=0.3)
        
        # è¿è¡Œæ—¶é—´å¯¹æ¯”
        if runtime_values:
            ax2.boxplot(runtime_values, labels=methods)
            ax2.set_title('è¿è¡Œæ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_ylabel('æ—¶é—´ (ç§’)')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_resource_usage(self, results: Dict):
        """ç»˜åˆ¶èµ„æºä½¿ç”¨å¯¹æ¯”å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å‡†å¤‡æ•°æ®
        methods = []
        memory_usage = []
        gpu_memory_usage = []
        
        for method, result_list in [('Original', results['original_results']), 
                                   ('CPU Offloading', results['offloading_results'])]:
            successful = [r for r in result_list if r.get('success', False)]
            if successful:
                methods.append(method)
                memory_usage.append([r.get('max_memory_gb', 0) for r in successful])
                gpu_memory_usage.append([r.get('max_gpu_memory_gb', 0) for r in successful])
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        if memory_usage:
            ax1.boxplot(memory_usage, labels=methods)
            ax1.set_title('ç³»ç»Ÿå†…å­˜ä½¿ç”¨å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax1.set_ylabel('å†…å­˜ (GB)')
            ax1.grid(True, alpha=0.3)
        
        # GPUå†…å­˜ä½¿ç”¨å¯¹æ¯”
        if gpu_memory_usage:
            ax2.boxplot(gpu_memory_usage, labels=methods)
            ax2.set_title('GPUå†…å­˜ä½¿ç”¨å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_ylabel('GPUå†…å­˜ (GB)')
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'resource_usage.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_sequence_length_capability(self, results: Dict):
        """ç»˜åˆ¶åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›å›¾"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # å›¾1ï¼šåºåˆ—é•¿åº¦ vs MSE
        for method_name, result_list in [('Original', results['original_results']), 
                                        ('CPU Offloading', results['offloading_results'])]:
            successful = [r for r in result_list if r.get('success', False)]
            if successful:
                seq_lengths = [r['config']['args'].get('seq_length', 0) for r in successful]
                mse_values = [r.get('mse', float('inf')) for r in successful]
                
                # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                valid_data = [(s, m) for s, m in zip(seq_lengths, mse_values) if m != float('inf')]
                if valid_data:
                    seq_lengths, mse_values = zip(*valid_data)
                    ax1.plot(seq_lengths, mse_values, 'o-', label=method_name, linewidth=2, markersize=8)
        
        ax1.set_xlabel('åºåˆ—é•¿åº¦', fontsize=12)
        ax1.set_ylabel('MSE', fontsize=12)
        ax1.set_title('åºåˆ—é•¿åº¦ vs é¢„æµ‹ç²¾åº¦', fontsize=14, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_xscale('log')  # ä½¿ç”¨å¯¹æ•°åæ ‡æ›´å¥½åœ°æ˜¾ç¤ºä¸åŒé‡çº§çš„åºåˆ—é•¿åº¦
        
        # å›¾2ï¼šæˆåŠŸçš„æœ€å¤§åºåˆ—é•¿åº¦å¯¹æ¯”
        methods = []
        max_seq_lengths = []
        
        for method_name, result_list in [('Original', results['original_results']), 
                                        ('CPU Offloading', results['offloading_results'])]:
            successful = [r for r in result_list if r.get('success', False)]
            if successful:
                max_seq_len = max([r['config']['args'].get('seq_length', 0) for r in successful])
                methods.append(method_name)
                max_seq_lengths.append(max_seq_len)
        
        if max_seq_lengths:
            bars = ax2.bar(methods, max_seq_lengths, color=['#1f77b4', '#ff7f0e'], alpha=0.7)
            ax2.set_ylabel('æœ€å¤§åºåˆ—é•¿åº¦', fontsize=12)
            ax2.set_title('åºåˆ—é•¿åº¦å¤„ç†ä¸Šé™å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='y')
            
            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, value in zip(bars, max_seq_lengths):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{int(value)}', ha='center', va='bottom', fontsize=11, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'sequence_length_capability.png', dpi=300, bbox_inches='tight')
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é£ç”µé¢„æµ‹æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•')
    script_dir = os.path.dirname(os.path.abspath(__file__))

    parser.add_argument('--original_script', type=str, default=os.path.join(script_dir, '../new_hier/wind_1.py'), 
                       help='åŸå§‹è„šæœ¬è·¯å¾„')
    parser.add_argument('--offloading_script', type=str, default=os.path.join(script_dir, '../new_hier/wind_offloading.py'), 
                       help='CPU Offloadingè„šæœ¬è·¯å¾„')
    parser.add_argument('--quick_test', action='store_true', 
                       help='è¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼ˆè¾ƒå°‘epochsï¼‰')
    parser.add_argument('--csv_path', type=str, default=None,
                       help='CSVæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æŸ¥æ‰¾ï¼‰')
    parser.add_argument('--verbose_child', action='store_true', 
                    help='å®æ—¶æ‰“å°å­è¿›ç¨‹çš„è¯¦ç»†è¾“å‡ºï¼ˆå¦‚tqdmè¿›åº¦æ¡ï¼‰')
    args = parser.parse_args()
    
    print("ğŸš€ é£ç”µé¢„æµ‹æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    # æŸ¥æ‰¾CSVæ–‡ä»¶è·¯å¾„
    if args.csv_path is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        csv_path = os.path.join(script_dir, '../data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
        if not os.path.exists(csv_path):
            print(f"âŒ æ‰¾ä¸åˆ°CSVæ–‡ä»¶: {csv_path}")
            return
    else:
        csv_path = args.csv_path
    
    print(f"ğŸ“ ä½¿ç”¨CSVæ•°æ®æ–‡ä»¶: {csv_path}")
    
    # åŸºç¡€é…ç½®
    base_config = {
        'gpu': 0,
        'seed': 42,
        'patience': 3,
        'save_model': False,  # æµ‹è¯•æ—¶ä¸ä¿å­˜æ¨¡å‹
        'hyperparam_id': 'benchmark',  # åŸºå‡†æµ‹è¯•æ ‡è¯†
    }
    
    if args.quick_test:
        print("âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        base_config.update({
            'epochs': 2,
            'patience': 2,
        })
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = PerformanceBenchmark(args.original_script)
    benchmark.verbose_child = args.verbose_child
    try:
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        comparison_results = benchmark.compare_methods(
            args.original_script,
            args.offloading_script,
            base_config
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        benchmark.generate_report(comparison_results)
        
        print("\nğŸ‰ åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š æŸ¥çœ‹ç»“æœç›®å½•: {benchmark.results_dir}")
        
        # æ˜¾ç¤ºå…³é”®ç»“è®º
        summary = comparison_results['comparison_summary']
        if 'sequence_length_improvement' in summary:
            improvement = summary['sequence_length_improvement']['improvement_percentage']
            seq_improvement = summary['sequence_length_improvement']['absolute_improvement']
            print(f"ğŸ† å…³é”®å‘ç°ï¼šCPU Offloadingä½¿åºåˆ—é•¿åº¦å¤„ç†èƒ½åŠ›æå‡äº† {improvement:.1f}%")
            print(f"ğŸ¯ å…·ä½“æå‡ï¼šåºåˆ—é•¿åº¦ä¸Šé™å¢åŠ äº† {seq_improvement} ä¸ªæ—¶é—´æ­¥é•¿")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main() 