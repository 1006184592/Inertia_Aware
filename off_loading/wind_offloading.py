import os
# Set CUDA launch blocking for debugging
# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

import pickle
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from adap_auto import adap_auto
from price.case_118 import price_case
from evaluate import MSE, MAPE
import torch.multiprocessing as mp
import time
import argparse
import random
from pathlib import Path
from typing import Tuple, Optional

# --- æ ¸å¿ƒä¿®æ”¹ï¼šå¼•å…¥Accelerateåº“å®ç°CPU Offloading ---
from accelerate import Accelerator, dispatch_model
from accelerate.utils import get_balanced_memory
from tqdm import tqdm
from dynamic_data_processor import create_dynamic_data


class WindPowerOffloadingPredictor:
    """
    é›†æˆCPU OffloadingæŠ€æœ¯çš„é£ç”µåŠŸç‡é¢„æµ‹ç³»ç»Ÿ
    çªç ´GPUæ˜¾å­˜é™åˆ¶ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡åºåˆ—é¢„æµ‹
    """
    
    def __init__(self, args, csv_path: str):
        """
        åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ
        Args:
            args: å‘½ä»¤è¡Œå‚æ•°
            csv_path: CSVæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.args = args
        self.csv_path = csv_path
        
        # --- æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨Acceleratoræ¥ç®¡ç†è®¾å¤‡å’Œå†…å­˜ ---
        print("ğŸš€ åˆå§‹åŒ–CPU Offloadingç³»ç»Ÿ...")
        # ç”±äºFFTè®¡ç®—çš„é™åˆ¶ï¼Œæš‚æ—¶ç¦ç”¨æ··åˆç²¾åº¦
        self.accelerator = Accelerator(
            mixed_precision='fp16',  # ç¦ç”¨æ··åˆç²¾åº¦é¿å…FFTé—®é¢˜
            gradient_accumulation_steps=1
        )
        
        print(f"ğŸ“± Acceleratorè®¾å¤‡: {self.accelerator.device}")
        print(f"ğŸ”§ è¿›ç¨‹æ•°é‡: {self.accelerator.num_processes}")
        print(f"âš¡ æ··åˆç²¾åº¦æ¨¡å¼: {self.accelerator.mixed_precision}")
        
        # è®¾ç½®è®¾å¤‡
        self.device = self.accelerator.device
        
        # åˆå§‹åŒ–æ•°æ®ç›¸å…³å±æ€§
        self.scaler = None
        self.use_standardized = False
        self.feature_index = None
        
        # æ¨¡å‹ç›¸å…³
        self.model = None
        self.optimizer = None
        self.loss_function = nn.MSELoss()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_mse = float('inf')
        self.patience_counter = 0
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def setup_data(self):
        """è®¾ç½®å’ŒåŠ è½½æ•°æ®ï¼ˆä½¿ç”¨åŠ¨æ€æ•°æ®å¤„ç†å™¨ï¼‰"""
        print("ğŸ“Š å¼€å§‹åŠ¨æ€æ•°æ®è®¾ç½®...")
        
        print(f"ğŸ”§ ä½¿ç”¨åŠ¨æ€æ•°æ®å¤„ç†ç³»ç»Ÿ")
        print(f"   åºåˆ—é•¿åº¦: {self.args.seq_length}")
        print(f"   é¢„æµ‹é•¿åº¦: {self.args.c_out}")
        print(f"   æ•°æ®æ¥æº: {self.csv_path}")
        
        # åˆ›å»ºåŠ¨æ€æ•°æ®
        try:
            model_data = create_dynamic_data(
                csv_path=self.csv_path,
                seq_length=self.args.seq_length,
                pred_length=self.args.c_out,  # é¢„æµ‹é•¿åº¦
                split_ratio=self.args.split_ratio,
                standardize=True,  # æ€»æ˜¯è¿›è¡Œæ ‡å‡†åŒ–
                save_dir=None,  # ä¸ä¿å­˜ä¸­é—´æ–‡ä»¶
                verbose=True
            )
            
            # æå–æ•°æ®ï¼ˆä¸ç«‹å³ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œè®©Accelerateå¤„ç†ï¼‰
            self.X_train = model_data['X_train']
            self.y_train = model_data['y_train']
            self.X_test = model_data['X_test']
            self.y_test = model_data['y_test']
            self.train_dict = model_data['train_edge_indices']
            self.test_dict = model_data['test_edge_indices']
            self.scaler = model_data['scaler']
            self.feature_names = model_data['feature_names']
            self.num_features = model_data['num_features']
            
            # æ ‡è®°ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
            self.use_standardized = True
            
            # ä¸ºäº†å…¼å®¹æ€§ï¼Œåˆ›å»ºfeature_index
            self.feature_index = {feature: index for index, feature in enumerate(self.feature_names)}
            
            print(f"âœ… åŠ¨æ€æ•°æ®å¤„ç†å®Œæˆ")
            print(f"   è®­ç»ƒé›†: X{self.X_train.shape}, y{self.y_train.shape}")
            print(f"   æµ‹è¯•é›†: X{self.X_test.shape}, y{self.y_test.shape}")
            print(f"   ç‰¹å¾æ•°é‡: {self.num_features}")
            print("âœ… æ•°æ®è®¾ç½®å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ åŠ¨æ€æ•°æ®å¤„ç†å¤±è´¥: {e}")
            print("è¯·æ£€æŸ¥CSVæ–‡ä»¶è·¯å¾„å’Œå‚æ•°è®¾ç½®")
            raise e

    def setup_model_with_offloading(self):
        """
        è®¾ç½®æ¨¡å‹å¹¶åº”ç”¨CPU OffloadingæŠ€æœ¯ï¼ˆæ¢å¤ä¸ºå…¨è‡ªåŠ¨ã€é«˜æ€§èƒ½ç‰ˆæœ¬ï¼‰
        """
        print("ğŸ”§ æ„å»ºæ”¯æŒCPU Offloadingçš„æ¨¡å‹...")
        
        # 1. åœ¨CPUä¸Šåˆå§‹åŒ–æ¨¡å‹
        model = adap_auto(
            n_head=self.args.n_head,
            hidden_size=self.args.hidden_size,
            factor=self.args.factor,
            dropout=self.args.dropout,
            conv_hidden_size=self.args.conv_hidden_size,
            MovingAvg_window=self.args.moving_avg_window,
            activation=self.args.activation,
            encoder_layers=self.args.encoder_layers,
            decoder_layers=self.args.decoder_layers,
            c_in=self.num_features,
            seq_lenth=self.args.seq_length,
            c_out=self.args.c_out,
            gruop_dec=self.args.group_dec
        )
        
        # 2. --- æ¢å¤ä¸ºå…¨è‡ªåŠ¨ã€é«˜æ€§èƒ½çš„Offloadingæµç¨‹ ---
        try:
            from accelerate.utils import infer_auto_device_map
            print("ğŸ—ºï¸  ç”Ÿæˆæ™ºèƒ½è®¾å¤‡æ˜ å°„...")
            print(f"   æŒ‡å®šçš„GPUæ˜¾å­˜ä¸Šé™: {self.args.max_gpu_memory}")
            device_map = infer_auto_device_map(
                model,
                max_memory={0: self.args.max_gpu_memory},
                no_split_module_classes=getattr(model, '_no_split_modules', [])
            )
            print("ğŸ“‹ æˆåŠŸç”Ÿæˆçš„è®¾å¤‡æ˜ å°„:", device_map)
            print("ğŸ”„ åº”ç”¨CPU Offloadingè®¾å¤‡æ˜ å°„ (å¯ç”¨å¼‚æ­¥æµæ°´çº¿)...")
            # self.model = dispatch_model(
            #     model, 
            #     device_map="auto",
            #     max_memory={0: self.args.max_gpu_memory} # æ˜¾å­˜é™åˆ¶åœ¨è¿™é‡Œç”Ÿæ•ˆ
            # )
            self.model = dispatch_model(model, device_map=device_map)
            print("âœ… å…¨è‡ªåŠ¨CPU Offloadingè®¾ç½®æˆåŠŸï¼")
            # æ‰“å°ä¸€ä¸‹ Accelerate è‡ªåŠ¨ç”Ÿæˆçš„è®¾å¤‡æ˜ å°„ï¼Œçœ‹çœ‹å®ƒæŠŠå±‚æ”¾åˆ°äº†å“ªé‡Œ
            print("ğŸ“‹ Accelerateè‡ªåŠ¨ç”Ÿæˆçš„è®¾å¤‡æ˜ å°„:")
            print(self.model.hf_device_map)
            # ä½¿ç”¨ get_balanced_memory å¹¶ä¼ å…¥ä»å‘½ä»¤è¡Œè·å–çš„æ˜¾å­˜é™åˆ¶
            # device_map = get_balanced_memory(
            #     model,
            #     max_memory={0: self.args.max_gpu_memory}, # <--- åœ¨è¿™é‡Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
            #     no_split_module_classes=getattr(model, '_no_split_modules', [])
            # )
            
            # print("ğŸ“‹ ç”Ÿæˆçš„è®¾å¤‡æ˜ å°„:", device_map)
            # print("ğŸ”„ åº”ç”¨CPU Offloadingè®¾å¤‡æ˜ å°„ (å¯ç”¨å¼‚æ­¥æµæ°´çº¿)...")
    
            # # ä½¿ç”¨ dispatch_model æ¥å¯ç”¨æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–
            # self.model = dispatch_model(model, device_map=device_map)
            
            # print("âœ… å…¨è‡ªåŠ¨CPU Offloadingè®¾ç½®æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âš ï¸ è‡ªåŠ¨Offloadingå¤±è´¥: {e}ã€‚")
            # å¦‚æœè‡ªåŠ¨åˆ†é…å¤±è´¥ï¼Œå¯ä»¥è€ƒè™‘å›é€€åˆ°çº¯CPUæ¨¡å¼
            print("ğŸ“± å›é€€åˆ°çº¯CPUæ¨¡å¼...")
            self.model = model.to('cpu')
    
        # è®¾ç½®ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            self.model.parameters(), 
            lr=self.args.lr, 
            weight_decay=self.args.weight_decay
        )
        
        # ä½¿ç”¨Accelerateå‡†å¤‡æ¨¡å‹å’Œä¼˜åŒ–å™¨
        if hasattr(self.model, 'hf_device_map'):
            print("ğŸ“‹ æ¨¡å‹å·²ä½¿ç”¨device_mapï¼Œä»…å‡†å¤‡ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ã€‚")
            self.optimizer = self.accelerator.prepare(self.optimizer)
        else:
            print("ğŸ“‹ æ ‡å‡†æ¨¡å¼ï¼Œä½¿ç”¨accelerator.prepareå‡†å¤‡æ‰€æœ‰ç»„ä»¶ã€‚")
            self.model, self.optimizer = self.accelerator.prepare(
                self.model, self.optimizer
            )

    def create_offloading_dataloader(self):
        """åˆ›å»ºæ”¯æŒCPU Offloadingçš„æ•°æ®åŠ è½½å™¨"""
        print("ğŸ“¦ åˆ›å»ºé«˜æ•ˆæ•°æ®åŠ è½½å™¨...")
        
        # åˆ›å»ºè®­ç»ƒæ•°æ®é›†
        train_dataset = TensorDataset(self.X_train, self.y_train)
        
        # --- ä½¿ç”¨è¾ƒå°çš„batch_sizeä»¥é…åˆCPU Offloading ---
        # CPU-GPUæ•°æ®ä¼ è¾“æœ‰å¼€é”€ï¼Œè¾ƒå°çš„batchå¯ä»¥å‡å°‘å•æ¬¡ä¼ è¾“å»¶è¿Ÿ
        effective_batch_size = min(self.args.batch_size, 64)  # é™åˆ¶æœ€å¤§batch_size
        
        train_dataloader = DataLoader(
            train_dataset, 
            batch_size=effective_batch_size,
            shuffle=True,
            pin_memory=True,  # åŠ é€ŸCPU-GPUä¼ è¾“
            num_workers=0     # å¤šè¿›ç¨‹æ•°æ®åŠ è½½
        )
        
        # ä¿å­˜åŸå§‹batch_sizeä¿¡æ¯
        self.original_batch_size = effective_batch_size
        
        # ä½¿ç”¨Accelerateå‡†å¤‡æ•°æ®åŠ è½½å™¨
        self.train_dataloader = self.accelerator.prepare(train_dataloader)
        
        print(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
        print("âœ… æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ")

    def train_with_offloading(self):
        """
        ä½¿ç”¨CPU OffloadingæŠ€æœ¯è¿›è¡Œæ¨¡å‹è®­ç»ƒ
        æ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹å’Œæ•°æ®
        """
        print("ğŸ‹ï¸  å¼€å§‹CPU Offloadingè®­ç»ƒ...")
        print(f"ğŸ¯ ç›®æ ‡epochs: {self.args.epochs}")
        print(f"â° æ—©åœè€å¿ƒå€¼: {self.args.patience}")
        
        for epoch in range(self.args.epochs):
            start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_loss = 0
            batch_count = 0
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            progress_bar = tqdm(
                self.train_dataloader, 
                desc=f"Epoch {epoch+1}/{self.args.epochs}",
                leave=False
            )
            
            for batch_idx, (inputs, targets) in enumerate(progress_bar):
                # è·å–å¯¹åº”çš„è¾¹å­—å…¸
                start_idx = batch_idx * self.original_batch_size
                end_idx = start_idx + len(inputs)
                dicts = self.train_dict[start_idx:end_idx]
                
                # --- æ ¸å¿ƒï¼šæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆAccelerateè‡ªåŠ¨å¤„ç†è®¾å¤‡æ˜ å°„ï¼‰---
                self.optimizer.zero_grad()
                
                # è¿™é‡Œæ˜¯CPU Offloadingçš„é­”æ³•æ—¶åˆ»ï¼
                # å½“è°ƒç”¨modelæ—¶ï¼Œå¼‚æ­¥æµæ°´çº¿å¼€å§‹å·¥ä½œ
                with self.accelerator.autocast():  # æ··åˆç²¾åº¦è®¡ç®—
                    model_output = self.model(inputs, dicts).squeeze(-1)
                    loss = self.loss_function(model_output, targets)
                    
                    # L1æ­£åˆ™åŒ–
                    l1_norm = sum(p.abs().sum() for p in self.model.parameters())
                    loss = loss + self.args.l1_lambda * l1_norm
                
                # åå‘ä¼ æ’­ï¼ˆAccelerateè‡ªåŠ¨å¤„ç†æ¢¯åº¦ç¼©æ”¾ï¼‰
                self.accelerator.backward(loss)
                self.optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.4f}',
                    'Avg_Loss': f'{total_loss/batch_count:.4f}'
                })
                
                # å®šæœŸæ¸…ç†GPUç¼“å­˜
                if batch_idx % 10 == 0:
                    torch.cuda.empty_cache()
            
            # éªŒè¯é˜¶æ®µ
            val_mse = self._validate_with_offloading()
            
            # æ—©åœæ£€æŸ¥
            if self._check_early_stopping(val_mse, epoch):
                break
            
            # è®¡ç®—epochæ—¶é—´
            end_time = time.time()
            epoch_time = end_time - start_time
            
            print(f"ğŸ“Š Epoch {epoch + 1}/{self.args.epochs}")
            print(f"   æŸå¤±: {total_loss / len(self.train_dataloader):.4f}")
            print(f"   éªŒè¯MSE: {val_mse:.4f}")
            print(f"   ç”¨æ—¶: {epoch_time:.2f}ç§’")
            print(f"   æœ€ä½³MSE: {self.best_mse:.4f}")
            
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")

    def _validate_with_offloading(self) -> float:
        """ä½¿ç”¨CPU Offloadingè¿›è¡ŒéªŒè¯"""
        self.model.eval()
        X_test_on_device = self.X_test.to(self.accelerator.device)
        with torch.no_grad():
            # --- CPU Offloadingåœ¨è¿™é‡Œä¹Ÿå‘æŒ¥ä½œç”¨ ---
            # å¤§è§„æ¨¡éªŒè¯æ•°æ®çš„å¤„ç†å˜å¾—å¯èƒ½
            prediction = self.model(X_test_on_device, self.test_dict).squeeze(-1)
            
            if self.use_standardized and self.scaler is not None:
                # æ ‡å‡†åŒ–æ•°æ®çš„å¤„ç†
                y_test_original = self._inverse_transform_power(
                    self.y_test.cpu().numpy(), self.scaler, power_feature_idx=0
                )
                prediction_original = self._inverse_transform_power(
                    prediction.cpu().numpy(), self.scaler, power_feature_idx=0
                )
                val_mse = MSE(y_test_original, prediction_original)
            else:
                # åŸå§‹æ•°æ®çš„å¤„ç†
                val_mse = MSE(self.y_test.cpu().numpy(), prediction.cpu().numpy())
        
        # æ¸…ç†å†…å­˜
        torch.cuda.empty_cache()
        return val_mse

    def predict_with_offloading(self) -> Tuple[np.ndarray, np.ndarray, float, float]:
        """
        ä½¿ç”¨CPU Offloadingè¿›è¡Œæœ€ç»ˆé¢„æµ‹
        Returns:
            (predictions, targets, mse, mape): é¢„æµ‹ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡
        """
        print("ğŸ”® å¼€å§‹CPU Offloadingé¢„æµ‹...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        # model_path = self._get_model_path()
        # if model_path.exists():
        #     print(f"ğŸ“¥ åŠ è½½æœ€ä½³æ¨¡å‹: {model_path}")
        #     # æ³¨æ„ï¼šåŠ è½½å¸¦æœ‰device_mapçš„æ¨¡å‹éœ€è¦ç‰¹æ®Šå¤„ç†
        #     try:
        #         self.model = torch.load(str(model_path), map_location='cpu')
        #         if not hasattr(self.model, 'hf_device_map'):
        #             self.model = self.model.to(self.device)
        #     except Exception as e:
        #         print(f"âš ï¸  æ¨¡å‹åŠ è½½è­¦å‘Š: {e}")
        
        self.model.eval()
        X_test_on_device = self.X_test.to(self.accelerator.device)
        with torch.no_grad():
            # --- CPU Offloadingè®©æˆ‘ä»¬èƒ½å¤„ç†æ›´å¤§çš„æµ‹è¯•é›† ---
            print("âš¡ æ‰§è¡Œå¤§è§„æ¨¡é¢„æµ‹ï¼ˆCPU OffloadingåŠ é€Ÿï¼‰...")
            prediction = self.model(X_test_on_device, self.test_dict).squeeze(-1)
        
        # è½¬æ¢ä¸ºnumpyè¿›è¡Œè¯„ä¼°
        y_test_np = self.y_test.cpu().numpy()
        prediction_np = prediction.detach().cpu().numpy()
        
        if self.use_standardized and self.scaler is not None:
            # åæ ‡å‡†åŒ–å¤„ç†
            print("ğŸ”„ æ‰§è¡Œåæ ‡å‡†åŒ–...")
            y_test_original = self._inverse_transform_power(y_test_np, self.scaler, 0)
            prediction_original = self._inverse_transform_power(prediction_np, self.scaler, 0)
            
            mse_result = MSE(y_test_original, prediction_original)
            mape_result = MAPE(y_test_original, prediction_original)
            
            print(f"ğŸ“ˆ åŸå§‹å°ºåº¦ - MSE: {mse_result:.6f}, MAPE: {mape_result:.6f}")
            return prediction_original, y_test_original, mse_result, mape_result
        else:
            # åŸå§‹æ•°æ®å¤„ç†
            mse_result = MSE(y_test_np, prediction_np)
            mape_result = MAPE(y_test_np, prediction_np)
            
            print(f"ğŸ“ˆ MSE: {mse_result:.6f}, MAPE: {mape_result:.6f}")
            return prediction_np, y_test_np, mse_result, mape_result

    def save_results(self, predictions: np.ndarray, targets: np.ndarray, mse: float, mape: float):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜é¢„æµ‹ç»“æœ...")
        
        # åˆ›å»ºç»“æœç›®å½•
        try:
            base_dir = Path('/home/forecasting/pts/results/fujian')
            predictions_dir = base_dir / 'forecasting_offloading_dynamic'
            predictions_dir.mkdir(parents=True, exist_ok=True)
            
            # è®¡ç®—è¯¯å·®
            error = targets - predictions
            
            # åˆ›å»ºç»“æœDataFrame
            data = {
                'adap_auto_offloading': predictions.flatten(),
                'real': targets.flatten(),
                'error': error.flatten()
            }
            df_results = pd.DataFrame(data)
            
            # ä½¿ç”¨åºåˆ—é•¿åº¦ä½œä¸ºæ ‡è¯†ç¬¦
            model_identifier = f'seq{self.args.seq_length}_pred{self.args.c_out}_{self.args.hyperparam_id}'
            
            # ä¿å­˜CSVæ–‡ä»¶
            predictions_path = predictions_dir / f'prediction_adap_auto_offloading_{model_identifier}.csv'
            df_results.to_csv(str(predictions_path), index=False)
            
            print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {predictions_path}")
        
            # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
            self._save_performance_report(predictions_path.parent, mse, mape)
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œä¹Ÿä¸è¦è®©æ•´ä¸ªç¨‹åºå´©æºƒ
            pass
    def _save_performance_report(self, save_dir: Path, mse: float, mape: float):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        model_identifier = f'seq{self.args.seq_length}_pred{self.args.c_out}_{self.args.hyperparam_id}'
        report_path = save_dir / f'performance_report_offloading_{model_identifier}.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write("ğŸš€ CPU Offloadingé£ç”µé¢„æµ‹æ€§èƒ½æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n")
            f.write(f"æ•°æ®å¤„ç†æ¨¡å¼: åŠ¨æ€æ»‘åŠ¨çª—å£\n")
            f.write(f"åºåˆ—é•¿åº¦: {self.args.seq_length}\n")
            f.write(f"é¢„æµ‹é•¿åº¦: {self.args.c_out}\n")
            f.write(f"è¶…å‚æ•°ID: {self.args.hyperparam_id}\n")
            f.write(f"æ•°æ®ç±»å‹: {'æ ‡å‡†åŒ–æ•°æ®' if self.use_standardized else 'åŸå§‹æ•°æ®'}\n")
            f.write(f"ä½¿ç”¨Scaler: {'æ˜¯' if self.scaler is not None else 'å¦'}\n")
            f.write(f"ç‰¹å¾æ•°é‡: {self.num_features}\n")
            f.write(f"è®­ç»ƒé›†å¤§å°: {self.X_train.shape}\n")
            f.write(f"æµ‹è¯•é›†å¤§å°: {self.X_test.shape}\n")
            f.write(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"MSE: {mse:.6f}\n")
            f.write(f"MAPE: {mape:.6f}\n")
            f.write(f"æœ€ä½³éªŒè¯MSE: {self.best_mse:.6f}\n")
            f.write(f"\nğŸ”§ æ¨¡å‹é…ç½®:\n")
            f.write(f"éšè—å±‚å¤§å°: {self.args.hidden_size}\n")
            f.write(f"æ³¨æ„åŠ›å¤´æ•°: {self.args.n_head}\n")
            f.write(f"ç¼–ç å™¨å±‚æ•°: {self.args.encoder_layers}\n")
            f.write(f"è§£ç å™¨å±‚æ•°: {self.args.decoder_layers}\n")
            f.write(f"å­¦ä¹ ç‡: {self.args.lr}\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {self.args.batch_size}\n")
            f.write(f"Dropout: {self.args.dropout}\n")
        
        print(f"ğŸ“‹ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

    # è¾…åŠ©æ–¹æ³•
    def _is_standardized_data(self, train_data_path: str, val_data_path: str) -> bool:
        """æ£€æµ‹æ•°æ®æ˜¯å¦ä¸ºæ ‡å‡†åŒ–æ•°æ®"""
        train_filename = os.path.basename(train_data_path)
        val_filename = os.path.basename(val_data_path)
        
        print(f"ğŸ” æ–‡ä»¶åæ£€æµ‹: {train_filename}, {val_filename}")
        
        if train_filename.startswith('std') or val_filename.startswith('std'):
            print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶åä»¥'std'å¼€å¤´)")
            return True
        
        if 'std' in train_filename.lower() or 'std' in val_filename.lower():
            print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶ååŒ…å«'std')")
            return True
        
        standardized_keywords = ['standard', 'standardized', 'norm', 'normalized']
        for keyword in standardized_keywords:
            if keyword in train_filename.lower() or keyword in val_filename.lower():
                print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶ååŒ…å«'{keyword}')")
                return True
        
        print(f"âŒ æ£€æµ‹ç»“æœ: åŸå§‹æ•°æ®")
        return False

    def _load_scaler(self):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        data_dir = os.path.dirname(self.data_paths['train_dir'])
        scaler_path = os.path.join(data_dir, 'scaler.pkl')
        
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            print(f"âœ… å·²åŠ è½½æ ‡å‡†åŒ–å™¨ï¼Œç‰¹å¾æ•°é‡: {len(self.scaler.mean_)}")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æ ‡å‡†åŒ–æ•°æ®ä½†æœªæ‰¾åˆ°scaler.pklæ–‡ä»¶")
            self.use_standardized = False

    def _inverse_transform_power(self, data: np.ndarray, scaler, power_feature_idx: int = 0) -> np.ndarray:
        """å¯¹åŠŸç‡æ•°æ®è¿›è¡Œåæ ‡å‡†åŒ–"""
        mean = scaler.mean_[power_feature_idx]
        scale = scaler.scale_[power_feature_idx]
        return data * scale + mean

    def _check_early_stopping(self, val_mse: float, epoch: int) -> bool:
        """æ£€æŸ¥æ—©åœæ¡ä»¶"""
        if val_mse < self.best_mse:
            self.best_mse = val_mse
            self.patience_counter = 0
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if self.args.save_model:
                self._save_best_model()
            return False
        else:
            self.patience_counter += 1
            
        if self.patience_counter >= self.args.patience:
            print(f'ğŸ›‘ æ—©åœäºepoch {epoch+1}')
            return True
        
        return False

    def _save_best_model(self):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        model_path = self._get_model_path()
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å¯¹äºä½¿ç”¨device_mapçš„æ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šä¿å­˜æ–¹å¼
        try:
            torch.save(self.model, str(model_path))
        except Exception as e:
            print(f"âš ï¸  æ¨¡å‹ä¿å­˜è­¦å‘Š: {e}")

    def _get_model_path(self) -> Path:
        """è·å–æ¨¡å‹ä¿å­˜è·¯å¾„"""
        base_dir = Path('/home/forecasting/pts/results/adap_auto')
        models_dir = base_dir / 'models_offloading'
        model_identifier = f'seq{self.args.seq_length}_pred{self.args.c_out}_{self.args.hyperparam_id}'
        return models_dir / f'best_model_adap_auto_offloading_{model_identifier}.pth'


def seed_everything(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def main():
    """ä¸»å‡½æ•°ï¼šé›†æˆCPU Offloadingçš„é£ç”µé¢„æµ‹"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='é£ç”µåŠŸç‡é¢„æµ‹ - CPU Offloadingç‰ˆæœ¬')
    parser.add_argument('--gpu', type=int, default=1, help='GPUè®¾å¤‡ID')
    parser.add_argument('--epochs', type=int, default=1, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.0002, help='å­¦ä¹ ç‡')
    parser.add_argument('--l1_lambda', type=float, default=0.15, help='L1æ­£åˆ™åŒ–ç³»æ•°')
    parser.add_argument('--weight_decay', type=float, default=0.15, help='L2æƒé‡è¡°å‡')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropoutç‡')
    parser.add_argument('--patience', type=int, default=5, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--split_ratio', type=float, default=0.99, help='è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†æ¯”ä¾‹')
    parser.add_argument('--seed', type=int, default=2, help='éšæœºç§å­')
    parser.add_argument('--dataset_name', type=str, default='6-0_1', help='æ•°æ®é›†åç§°')
    parser.add_argument('--hyperparam_id', type=str, default='offloading', help='è¶…å‚æ•°ç»„åˆID')
    parser.add_argument('--save_model', action='store_true', default=True, help='ä¿å­˜æœ€ä½³æ¨¡å‹')
    parser.add_argument('--max_gpu_memory', type=str, default='10GiB', help='GPUæ˜¾å­˜é™åˆ¶')
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--n_head', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--hidden_size', type=int, default=264, help='éšè—å±‚å¤§å°')
    parser.add_argument('--factor', type=int, default=2, help='æ³¨æ„åŠ›å› å­')
    parser.add_argument('--conv_hidden_size', type=int, default=32, help='å·ç§¯éšè—å±‚å¤§å°')
    parser.add_argument('--moving_avg_window', type=int, default=3, help='ç§»åŠ¨å¹³å‡çª—å£å¤§å°')
    parser.add_argument('--activation', type=str, default='gelu', help='æ¿€æ´»å‡½æ•°')
    parser.add_argument('--encoder_layers', type=int, default=1, help='ç¼–ç å™¨å±‚æ•°')
    parser.add_argument('--decoder_layers', type=int, default=1, help='è§£ç å™¨å±‚æ•°')
    parser.add_argument('--seq_length', type=int, default=36, help='åºåˆ—é•¿åº¦')
    parser.add_argument('--c_out', type=int, default=1, help='è¾“å‡ºé€šé“æ•°')
    parser.add_argument('--group_dec', action='store_true', default=True, help='ä½¿ç”¨ç»„è§£ç å™¨')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(seed=args.seed)
    
    # è®¾ç½®CSVæ•°æ®è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    csv_path = os.path.join(script_dir, f'../data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
    
    # è®¾ç½®å¤šè¿›ç¨‹
    mp.set_start_method('spawn', force=True)
    
    print("ğŸŒŠ =" * 30)
    print("ğŸŒŠ CPU Offloadingé£ç”µé¢„æµ‹ç³»ç»Ÿå¯åŠ¨")
    print("ğŸŒŠ =" * 30)
    print(f"ğŸ¯ åºåˆ—é•¿åº¦: {args.seq_length}")
    print(f"ğŸ¯ é¢„æµ‹é•¿åº¦: {args.c_out}")
    print(f"ğŸ¯ GPUæ˜¾å­˜é™åˆ¶: {args.max_gpu_memory}")
    print(f"ğŸ¯ è¶…å‚æ•°ID: {args.hyperparam_id}")
    print(f"ğŸ¯ æ•°æ®æº: {csv_path}")
    
    try:
        # åˆå§‹åŒ–é¢„æµ‹ç³»ç»Ÿ
        print("DEBUG: æ­¥éª¤1 - åˆå§‹åŒ–Predictorç±»...")
        predictor = WindPowerOffloadingPredictor(args, csv_path)
        print("DEBUG: æ­¥éª¤1 - å®Œæˆã€‚")
        
        # è®¾ç½®æ•°æ®
        print("DEBUG: æ­¥éª¤2 - å¼€å§‹æ•°æ®è®¾ç½® (setup_data)...")
        predictor.setup_data()
        print("DEBUG: æ­¥éª¤2 - å®Œæˆã€‚")
        
        # è®¾ç½®æ¨¡å‹å’ŒCPU Offloading
        print("DEBUG: æ­¥éª¤3 - å¼€å§‹æ¨¡å‹è®¾ç½® (setup_model_with_offloading)...")
        predictor.setup_model_with_offloading()
        print("DEBUG: æ­¥éª¤3 - å®Œæˆã€‚")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("DEBUG: æ­¥éª¤4 - å¼€å§‹åˆ›å»ºæ•°æ®åŠ è½½å™¨ (create_offloading_dataloader)...")
        predictor.create_offloading_dataloader()
        print("DEBUG: æ­¥éª¤4 - å®Œæˆã€‚")
        
        # è®­ç»ƒæ¨¡å‹
        print("DEBUG: æ­¥éª¤5 - å³å°†å¼€å§‹è®­ç»ƒ (train_with_offloading)...")
        predictor.train_with_offloading()
        print("DEBUG: æ­¥éª¤5 - è®­ç»ƒå·²ç»“æŸã€‚")
        
        # é¢„æµ‹å’Œè¯„ä¼°
        predictions, targets, mse, mape = predictor.predict_with_offloading()
        
        # ä¿å­˜ç»“æœ
        predictor.save_results(predictions, targets, mse, mape)
        
        # è¾“å‡ºæœ€ç»ˆæ€»ç»“
        print("\n" + "ğŸ‰" * 60)
        print("ğŸ‰ CPU Offloadingé£ç”µé¢„æµ‹å®Œæˆ!")
        print("ğŸ‰" * 60)
        print(f"âœ¨ æ•°æ®ç±»å‹: {'æ ‡å‡†åŒ–æ•°æ®' if predictor.use_standardized else 'åŸå§‹æ•°æ®'}")
        print(f"âœ¨ æœ€ç»ˆMSE: {mse:.6f}")
        print(f"âœ¨ æœ€ç»ˆMAPE: {mape:.6f}")
        print(f"âœ¨ çªç ´äº†ä¼ ç»ŸGPUæ˜¾å­˜é™åˆ¶ï¼")
        print("ğŸ‰" * 60)
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nğŸ’¥ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    import sys
    exit_code = main()
    sys.exit(exit_code) 