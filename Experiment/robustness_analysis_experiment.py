import os
import sys
import warnings
# Set CUDA launch blocking for debugging
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# æŠ‘åˆ¶ statsmodels çš„ FutureWarning è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning, module='statsmodels')
warnings.filterwarnings('ignore', message='verbose is deprecated since functions should not print results')

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶å‘ä¸Šé€€ä¸€å±‚ï¼ˆçˆ¶ç›®å½•ï¼‰
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)  # å°†çˆ¶ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„

import pickle
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from adap_auto import adap_auto
from evaluate import MSE, MAPE
import torch.multiprocessing as mp
import time
import argparse
import random
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from dynamic_data_processor import create_dynamic_data
from datetime import datetime
import calendar
from scipy import interpolate

def seed_everything(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def load_preprocessed_data(data_path):
    """
    åŠ è½½é¢„å¤„ç†å¥½çš„æ•°æ®
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŠ è½½çš„æ•°æ®
    """
    print(f"ğŸ“‚ åŠ è½½é¢„å¤„ç†æ•°æ®: {data_path}")
    
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
    
    with open(data_path, 'rb') as f:
        data = pickle.load(f)
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   æ•°æ®é›†: {data['dataset_name']}")
    print(f"   é¢„æµ‹å°ºåº¦: {data['prediction_scale']}")
    print(f"   ç‰¹å¾æ•°é‡: {data['num_features']}")
    print(f"   ç‰¹å¾åç§°: {data['feature_names']}")
    print(f"   è®­ç»ƒé›†: X{data['X_train'].shape}, y{data['y_train'].shape}")
    print(f"   æµ‹è¯•é›†: X{data['X_test'].shape}, y{data['y_test'].shape}")
    print(f"   åˆ›å»ºæ—¶é—´: {data['created_at']}")
    
    return data

def find_csv_data_path(dataset_name):
    """
    æŸ¥æ‰¾å¯¹åº”æ•°æ®é›†çš„CSVæ–‡ä»¶è·¯å¾„
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        CSVæ–‡ä»¶è·¯å¾„
    """
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    if dataset_name.lower() == 'fujian':
        csv_path = os.path.join(script_dir, '../../data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
    elif dataset_name.lower() == 'dswe':
        csv_path = os.path.join(script_dir, '../../data/DSWE/Offshore Wind Farm Dataset1(WT5).csv')  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    if not os.path.exists(csv_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
    
    return csv_path

def inverse_transform_power(data, scaler, power_feature_idx=0):
    """
    å¯¹åŠŸç‡æ•°æ®è¿›è¡Œåæ ‡å‡†åŒ–
    Args:
        data: æ ‡å‡†åŒ–åçš„æ•°æ® (numpy array)
        scaler: sklearn StandardScalerå¯¹è±¡
        power_feature_idx: åŠŸç‡ç‰¹å¾åœ¨scalerä¸­çš„ç´¢å¼•
    Returns:
        åæ ‡å‡†åŒ–åçš„åŠŸç‡æ•°æ®
    """
    mean = scaler.mean_[power_feature_idx]
    scale = scaler.scale_[power_feature_idx]
    return data * scale + mean

def add_gaussian_noise(data, noise_level=0.05, seed=42):
	"""
	ä¸ºæ•°æ®æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆæŒ‰ç‰¹å¾åˆ†åˆ«åŸºäºå„è‡ªæ ‡å‡†å·®ï¼‰ï¼Œæ›´ç¨³å¥åœ°é€‚é…åŸå§‹å°ºåº¦æ•°æ®ã€‚
	Args:
		data: è¾“å…¥æ•°æ® (numpy array)ï¼Œå½¢çŠ¶ [N, L, C]
		noise_level: å™ªå£°å¼ºåº¦ï¼ˆç›¸å¯¹äºå„ç‰¹å¾æ ‡å‡†å·®çš„æ¯”ä¾‹ï¼‰
		seed: éšæœºç§å­
	Returns:
		æ·»åŠ å™ªå£°åçš„æ•°æ®ï¼ˆnumpy arrayï¼‰
	"""
	np.random.seed(seed)
	# æŒ‰ç‰¹å¾ç»´åº¦è®¡ç®—æ ‡å‡†å·®ï¼Œå½¢çŠ¶ [1,1,C]
	feature_std = np.std(data, axis=(0, 1), keepdims=True)
	# é˜²æ­¢æŸäº›ç‰¹å¾stdä¸º0
	feature_std = feature_std + 1e-8
	noise = np.random.normal(0, noise_level * feature_std, size=data.shape)
	return data + noise

def create_missing_data(data, missing_ratio=0.05, seed=42):
    """
    éšæœºåˆ›å»ºç¼ºå¤±æ•°æ®å¹¶ç”¨æ’å€¼å¡«å……
    
    Args:
        data: è¾“å…¥æ•°æ® (numpy array)
        missing_ratio: ç¼ºå¤±æ•°æ®æ¯”ä¾‹
        seed: éšæœºç§å­
    
    Returns:
        å¤„ç†ç¼ºå¤±æ•°æ®åçš„æ•°æ®
    """
    np.random.seed(seed)
    data_copy = data.copy()
    
    # å¯¹æ¯ä¸ªç‰¹å¾ç‹¬ç«‹å¤„ç†
    for feature_idx in range(data.shape[-1]):  # æœ€åä¸€ä¸ªç»´åº¦æ˜¯ç‰¹å¾ç»´åº¦
        # éšæœºé€‰æ‹©ç¼ºå¤±ä½ç½®
        total_samples = data.shape[0] * data.shape[1]  # batch_size * seq_length
        missing_count = int(total_samples * missing_ratio)
        
        # ç”Ÿæˆéšæœºç¼ºå¤±ä½ç½®
        missing_indices = np.random.choice(total_samples, missing_count, replace=False)
        
        # å°†ç¼ºå¤±ä½ç½®è½¬æ¢ä¸ºäºŒç»´ç´¢å¼•
        batch_indices = missing_indices // data.shape[1]
        time_indices = missing_indices % data.shape[1]
        
        # åˆ›å»ºç¼ºå¤±æ•°æ®
        for batch_idx, time_idx in zip(batch_indices, time_indices):
            data_copy[batch_idx, time_idx, feature_idx] = np.nan
        
        # å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œæ’å€¼å¡«å……
        for batch_idx in range(data.shape[0]):
            series = data_copy[batch_idx, :, feature_idx]
            if np.isnan(series).any():
                # ä½¿ç”¨çº¿æ€§æ’å€¼å¡«å……ç¼ºå¤±å€¼
                valid_indices = ~np.isnan(series)
                if np.sum(valid_indices) > 1:  # è‡³å°‘éœ€è¦ä¸¤ä¸ªæœ‰æ•ˆç‚¹è¿›è¡Œæ’å€¼
                    f = interpolate.interp1d(
                        np.where(valid_indices)[0], 
                        series[valid_indices], 
                        kind='linear', 
                        bounds_error=False, 
                        fill_value='extrapolate'
                    )
                    data_copy[batch_idx, :, feature_idx] = f(np.arange(len(series)))
                else:
                    # å¦‚æœæœ‰æ•ˆç‚¹å¤ªå°‘ï¼Œç”¨å‡å€¼å¡«å……
                    data_copy[batch_idx, :, feature_idx] = np.nanmean(series)
    
    return data_copy

def get_season_from_index(index, total_samples, dataset_name='fujian'):
    """
    æ ¹æ®æ•°æ®ç´¢å¼•è·å–å­£èŠ‚ä¿¡æ¯
    
    Args:
        index: æ•°æ®ç´¢å¼•
        total_samples: æ€»æ ·æœ¬æ•°é‡
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        å­£èŠ‚æ ‡ç­¾ ('Spring', 'Summer', 'Autumn', 'Winter')
    """
    # å°†æ•°æ®æŒ‰ç´¢å¼•å‡åŒ€åˆ†é…åˆ°å››ä¸ªå­£èŠ‚
    # ç¡®ä¿æ¯ä¸ªå­£èŠ‚éƒ½æœ‰æ•°æ®ç”¨äºåˆ†æ
    season_size = total_samples // 4  # æ¯ä¸ªå­£èŠ‚çš„æ ·æœ¬æ•°
    remainder = total_samples % 4     # å‰©ä½™æ ·æœ¬
    
    # è®¡ç®—å­£èŠ‚è¾¹ç•Œ
    boundaries = []
    current_boundary = 0
    for i in range(4):
        # å‰é¢çš„å­£èŠ‚å¤šåˆ†é…ä¸€ä¸ªå‰©ä½™æ ·æœ¬
        extra = 1 if i < remainder else 0
        current_boundary += season_size + extra
        boundaries.append(current_boundary)
    
    # æ ¹æ®ç´¢å¼•ç¡®å®šå­£èŠ‚
    seasons = ['Spring', 'Summer', 'Autumn', 'Winter']
    for i, boundary in enumerate(boundaries):
        if index < boundary:
            return seasons[i]
    
    # é˜²æ­¢è¶Šç•Œï¼Œè¿”å›æœ€åä¸€ä¸ªå­£èŠ‚
    return seasons[-1]

def split_data_by_season(X_test, y_test, test_edge_indices, dataset_name='fujian'):
    """
    æŒ‰å­£èŠ‚åˆ’åˆ†æµ‹è¯•æ•°æ®
    
    Args:
        X_test: æµ‹è¯•è¾“å…¥æ•°æ®
        y_test: æµ‹è¯•ç›®æ ‡æ•°æ®
        test_edge_indices: æµ‹è¯•é›†è¾¹ç´¢å¼•åˆ—è¡¨
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        æŒ‰å­£èŠ‚åˆ’åˆ†çš„æ•°æ®å­—å…¸
    """
    seasonal_data = {
        'Spring': {'X': [], 'y': [], 'edge_indices': []},
        'Summer': {'X': [], 'y': [], 'edge_indices': []},
        'Autumn': {'X': [], 'y': [], 'edge_indices': []},
        'Winter': {'X': [], 'y': [], 'edge_indices': []}
    }
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…å­£èŠ‚
    total_samples = len(X_test)
    for i in range(total_samples):
        season = get_season_from_index(i, total_samples, dataset_name)
        seasonal_data[season]['X'].append(X_test[i])
        seasonal_data[season]['y'].append(y_test[i])
        seasonal_data[season]['edge_indices'].append(test_edge_indices[i])
    
    # è½¬æ¢ä¸ºtensor
    for season in seasonal_data:
        if seasonal_data[season]['X']:
            seasonal_data[season]['X'] = torch.stack(seasonal_data[season]['X'])
            seasonal_data[season]['y'] = torch.stack(seasonal_data[season]['y'])
            # edge_indicesä¿æŒä¸ºåˆ—è¡¨
        else:
            # å¦‚æœæŸä¸ªå­£èŠ‚æ²¡æœ‰æ•°æ®ï¼Œåˆ›å»ºç©ºtensor
            seasonal_data[season]['X'] = torch.empty(0, X_test.shape[1], X_test.shape[2])
            seasonal_data[season]['y'] = torch.empty(0, y_test.shape[1])
            seasonal_data[season]['edge_indices'] = []
    
    return seasonal_data

def inverse_or_identity(arr, scaler, power_feature_idx=0):
	if scaler is None:
		return arr
	return inverse_transform_power(arr, scaler, power_feature_idx=power_feature_idx)

def split_data_by_season_independent(model_data, device, season_split_ratio=0.8):
    """
    å°†æ•´ä½“æ•°æ®æŒ‰å­£èŠ‚å››ç­‰åˆ†ï¼Œæ¯ä¸ªå­£èŠ‚ç‹¬ç«‹è¿›è¡Œè®­ç»ƒæµ‹è¯•åˆ’åˆ†
    
    Args:
        model_data: å®Œæ•´çš„æ•°æ®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        season_split_ratio: å­£èŠ‚å†…è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.8ï¼‰
    
    Returns:
        æŒ‰å­£èŠ‚åˆ’åˆ†çš„ç‹¬ç«‹æ•°æ®å­—å…¸
    """
    print("ğŸŒ¸ è¿›è¡Œå­£èŠ‚æ€§ç‹¬ç«‹æ•°æ®åˆ’åˆ†...")
    
    # è·å–å®Œæ•´æ•°æ®
    X_full = torch.cat([model_data['X_train'], model_data['X_test']], dim=0)
    y_full = torch.cat([model_data['y_train'], model_data['y_test']], dim=0)
    edge_full = model_data['train_edge_indices'] + model_data['test_edge_indices']
    
    total_samples = len(X_full)
    season_size = total_samples // 4
    remainder = total_samples % 4
    
    seasonal_data = {}
    seasons = ['Spring', 'Summer', 'Autumn', 'Winter']
    
    start_idx = 0
    for i, season in enumerate(seasons):
        # è®¡ç®—å½“å‰å­£èŠ‚çš„æ ·æœ¬æ•°é‡
        current_size = season_size + (1 if i < remainder else 0)
        end_idx = start_idx + current_size
        
        # æå–å½“å‰å­£èŠ‚çš„æ•°æ®
        X_season = X_full[start_idx:end_idx]
        y_season = y_full[start_idx:end_idx]
        edge_season = edge_full[start_idx:end_idx]
        
        # åœ¨å½“å‰å­£èŠ‚å†…è¿›è¡Œè®­ç»ƒæµ‹è¯•åˆ’åˆ†ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ¯”ä¾‹ï¼‰
        season_split = int(current_size * season_split_ratio)
        
        seasonal_data[season] = {
            'X_train': X_season[:season_split].to(device),
            'y_train': y_season[:season_split].to(device),
            'X_test': X_season[season_split:].to(device),
            'y_test': y_season[season_split:].to(device),
            'train_edge_indices': edge_season[:season_split],
            'test_edge_indices': edge_season[season_split:],
            'total_samples': current_size,
            'train_samples': season_split,
            'test_samples': current_size - season_split
        }
        
        print(f"  {season}: æ€»æ ·æœ¬{current_size}, è®­ç»ƒ{season_split}, æµ‹è¯•{current_size - season_split}")
        start_idx = end_idx
    
    return seasonal_data

def run_seasonal_independent_experiment(model_data, args, device):
    """
    è¿è¡Œå­£èŠ‚æ€§ç‹¬ç«‹å®éªŒï¼šå°†æ•°æ®å››ç­‰åˆ†ï¼Œæ¯ä¸ªå­£èŠ‚ç‹¬ç«‹è®­ç»ƒæµ‹è¯•
    
    Args:
        model_data: å®Œæ•´æ•°æ®å­—å…¸
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        å­£èŠ‚æ€§ç‹¬ç«‹å®éªŒç»“æœ
    """
    print("ğŸŒ¸ å¼€å§‹å­£èŠ‚æ€§ç‹¬ç«‹å®éªŒ...")
    
    # æŒ‰å­£èŠ‚åˆ’åˆ†æ•°æ®
    seasonal_data = split_data_by_season_independent(model_data, device, season_split_ratio=args.season_split_ratio)
    
    seasonal_results = {}
    num_features = model_data['num_features']
    
    for season, data in seasonal_data.items():
        print(f"\nğŸ”¬ è®­ç»ƒ {season} å­£èŠ‚æ¨¡å‹...")
        
        # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´hidden_size
        base_hidden_size = args.hidden_size
        adjusted_hidden_size = ((base_hidden_size + num_features - 1) // num_features) * num_features
        
        # ä¸ºå½“å‰å­£èŠ‚åˆ›å»ºæ¨¡å‹
        season_model = adap_auto(
            n_head=args.n_head,
            hidden_size=adjusted_hidden_size,
            factor=args.factor,
            dropout=args.dropout,
            conv_hidden_size=args.conv_hidden_size,
            MovingAvg_window=args.moving_avg_window,
            activation=args.activation,
            encoder_layers=args.encoder_layers,
            decoder_layers=args.decoder_layers,
            c_out=args.c_out,
            c_in=num_features,
            seq_lenth=model_data['seq_length'],
            gruop_dec=args.group_dec
        ).to(device)
        
        # è®­ç»ƒå½“å‰å­£èŠ‚æ¨¡å‹
        train_dataset = TensorDataset(data['X_train'], data['y_train'])
        train_loader = DataLoader(train_dataset, batch_size=min(args.batch_size, len(train_dataset)), shuffle=True)
        
        optimizer = torch.optim.Adam(season_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯ï¼ˆç®€åŒ–ç‰ˆï¼Œæ›´å°‘çš„epochsï¼‰
        season_epochs = args.epochs
        best_val_loss = float('inf')
        patience_counter = 0
        best_model_state = None
        
        for epoch in range(season_epochs):
            season_model.train()
            train_loss = 0.0
            I = 0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                dicts = data['train_edge_indices'][I:I+len(batch_X)]
                I += len(batch_X)
                
                optimizer.zero_grad()
                outputs = season_model(batch_X, dicts).squeeze(-1)
                loss = criterion(outputs, batch_y)
                
                # L1æ­£åˆ™åŒ–
                l1_reg = torch.tensor(0.).to(device)
                for param in season_model.parameters():
                    l1_reg += torch.norm(param, 1)
                loss += args.l1_lambda * l1_reg
                
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
            
            # éªŒè¯
            season_model.eval()
            with torch.no_grad():
                prediction = season_model(data['X_test'], data['test_edge_indices']).squeeze(-1)
                val_loss = criterion(prediction, data['y_test']).item()
            
            train_loss /= max(1, len(train_loader))
            if epoch % 5 == 0:
                print(f"Epoch {epoch+1}/{season_epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                best_model_state = season_model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= args.patience:
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        season_model.load_state_dict(best_model_state)
        
        # è¯„ä¼°å½“å‰å­£èŠ‚æ¨¡å‹
        season_model.eval()
        with torch.no_grad():
            predictions = season_model(data['X_test'], data['test_edge_indices']).squeeze(-1).cpu().numpy()
            
            # åæ ‡å‡†åŒ–
            true_values = inverse_or_identity(data['y_test'].cpu().numpy(), model_data.get('scaler', None), power_feature_idx=0)
            pred_values = inverse_or_identity(predictions, model_data.get('scaler', None), power_feature_idx=0)
            
            mse = MSE(true_values, pred_values)
            mape = MAPE(true_values, pred_values)
        
        seasonal_results[season] = {
            'MSE': mse,
            'MAPE': mape,
            'train_samples': data['train_samples'],
            'test_samples': data['test_samples'],
            'total_samples': data['total_samples']
        }
        
        print(f"  {season} ç»“æœ: MSE={mse:.4f}, MAPE={mape:.2f}%")
    
    return seasonal_results

def evaluate_model_robustness(model, X_test, y_test, test_edge_indices, scaler, device,
							   noise_levels=[0.05, 0.1], missing_ratios=[0.05], dataset_name='fujian',
							   include_seasonal_in_default=False):
    """
    è¯„ä¼°æ¨¡å‹çš„é²æ£’æ€§
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        X_test: æµ‹è¯•è¾“å…¥æ•°æ®
        y_test: æµ‹è¯•ç›®æ ‡æ•°æ®
        test_edge_indices: æµ‹è¯•é›†çš„è¾¹ç´¢å¼•åˆ—è¡¨
        scaler: æ•°æ®æ ‡å‡†åŒ–å™¨
        device: è®¡ç®—è®¾å¤‡
        noise_levels: å™ªå£°å¼ºåº¦åˆ—è¡¨
        missing_ratios: ç¼ºå¤±æ•°æ®æ¯”ä¾‹åˆ—è¡¨
        dataset_name: æ•°æ®é›†åç§°
        include_seasonal_in_default: æ˜¯å¦åœ¨é»˜è®¤æ¨¡å¼ä¸‹åŒ…å«å­£èŠ‚æ€§è¯„ä¼°
    
    Returns:
        é²æ£’æ€§è¯„ä¼°ç»“æœå­—å…¸
    """
    model.eval()
    results = {}
    
    # 1. åŸºçº¿æ€§èƒ½ï¼ˆæ— æ‰°åŠ¨ï¼‰
    print("ğŸ“Š è¯„ä¼°åŸºçº¿æ€§èƒ½...")
    with torch.no_grad():
        predictions = model(X_test, test_edge_indices).squeeze(-1).cpu().numpy()
        
        # åæ ‡å‡†åŒ–
        true_values = inverse_or_identity(y_test.cpu().numpy(), scaler, power_feature_idx=0)
        pred_values = inverse_or_identity(predictions, scaler, power_feature_idx=0)
        
        baseline_mse = MSE(true_values, pred_values)
        baseline_mape = MAPE(true_values, pred_values)
    
    results['baseline'] = {
        'MSE': baseline_mse,
        'MAPE': baseline_mape
    }
    
    # 2. å™ªå£°é²æ£’æ€§æµ‹è¯•
    print("ğŸ”Š è¯„ä¼°å™ªå£°é²æ£’æ€§...")
    results['noise_robustness'] = {}
    
    for noise_level in noise_levels:
        print(f"  æµ‹è¯•å™ªå£°å¼ºåº¦: {noise_level*100}%")
        
        # æ·»åŠ å™ªå£°åˆ°CPUä¸Šçš„numpyæ•°ç»„ï¼Œç„¶åè½¬æ¢å›tensor
        X_test_np = X_test.cpu().numpy()
        X_noisy_np = add_gaussian_noise(X_test_np, noise_level=noise_level)
        X_noisy = torch.FloatTensor(X_noisy_np).to(device)
        
        with torch.no_grad():
            predictions = model(X_noisy, test_edge_indices).squeeze(-1).cpu().numpy()
            
            # åæ ‡å‡†åŒ–
            pred_values = inverse_or_identity(predictions, scaler, power_feature_idx=0)
            
            mse = MSE(true_values, pred_values)
            mape = MAPE(true_values, pred_values)
        
        results['noise_robustness'][f'{noise_level*100}%'] = {
            'MSE': mse,
            'MAPE': mape,
            'MSE_degradation': (mse - baseline_mse) / baseline_mse * 100,
            'MAPE_degradation': (mape - baseline_mape) / baseline_mape * 100
        }
    
    # 3. ç¼ºå¤±é²æ£’æ€§æµ‹è¯•ï¼ˆæ”¯æŒå¤šæ¯”ä¾‹ï¼‰
    print("ğŸ•³ï¸ è¯„ä¼°ç¼ºå¤±é²æ£’æ€§...")
    results['missing_robustness'] = {}
    X_test_np = X_test.cpu().numpy()
    for missing_ratio in missing_ratios:
        print(f"  ç¼ºå¤±æ•°æ®æ¯”ä¾‹: {missing_ratio*100}%")
        X_missing_np = create_missing_data(X_test_np, missing_ratio=missing_ratio)
        X_missing = torch.FloatTensor(X_missing_np).to(device)
        with torch.no_grad():
            predictions = model(X_missing, test_edge_indices).squeeze(-1).cpu().numpy()
            pred_values = inverse_or_identity(predictions, scaler, power_feature_idx=0)
            mse = MSE(true_values, pred_values)
            mape = MAPE(true_values, pred_values)
        results['missing_robustness'][f'{missing_ratio*100}%_missing'] = {
            'MSE': mse,
            'MAPE': mape,
            'MSE_degradation': (mse - baseline_mse) / baseline_mse * 100,
            'MAPE_degradation': (mape - baseline_mape) / baseline_mape * 100
        }
    
    # 4. é»˜è®¤æ¨¡å¼çš„å­£èŠ‚æ€§æ€§èƒ½åˆ†æï¼ˆå¯é€‰ï¼Œé»˜è®¤å…³é—­ï¼‰
    if include_seasonal_in_default:
        print("ğŸŒ¸ è¯„ä¼°å­£èŠ‚æ€§æ€§èƒ½...")
        seasonal_data = split_data_by_season(X_test, y_test, test_edge_indices, dataset_name)
        results['seasonal_performance'] = {}
        for season, data in seasonal_data.items():
            if len(data['X']) > 0:
                print(f"  è¯„ä¼° {season} å­£èŠ‚æ€§èƒ½ (æ ·æœ¬æ•°: {len(data['X'])})")
                with torch.no_grad():
                    predictions = model(data['X'], data['edge_indices']).squeeze(-1).cpu().numpy()
                    true_seasonal_np = data['y'].cpu().numpy()
                    true_seasonal = inverse_or_identity(true_seasonal_np, scaler, power_feature_idx=0)
                    pred_seasonal = inverse_or_identity(predictions, scaler, power_feature_idx=0)
                    mse = MSE(true_seasonal, pred_seasonal)
                    mape = MAPE(true_seasonal, pred_seasonal)
                results['seasonal_performance'][season] = {
                    'MSE': mse,
                    'MAPE': mape,
                    'sample_count': len(data['X'])
                }
            else:
                print(f"  {season} å­£èŠ‚æ— æ•°æ®")
                results['seasonal_performance'][season] = {
                    'MSE': 0,
                    'MAPE': 0,
                    'sample_count': 0
                }
    
    return results

def load_baseline_results(results_dir, dataset_name, prediction_scale):
    """
    åŠ è½½åŸºçº¿æ¨¡å‹çš„ç»“æœç”¨äºå¯¹æ¯”
    
    Args:
        results_dir: ç»“æœç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        prediction_scale: é¢„æµ‹å°ºåº¦
    
    Returns:
        åŸºçº¿ç»“æœå­—å…¸
    """
    baseline_models = ['iTransformer', 'DLinear', 'NBEATSx', 'FEDformer']
    baseline_results = {}
    
    # è¿™é‡Œåº”è¯¥åŠ è½½å®é™…çš„åŸºçº¿æ¨¡å‹ç»“æœ
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€äº›æ¨¡æ‹Ÿæ•°æ®
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œåº”è¯¥ä»ä¿å­˜çš„ç»“æœæ–‡ä»¶ä¸­åŠ è½½
    
    for model_name in baseline_models:
        # æ¨¡æ‹ŸåŸºçº¿ç»“æœï¼ˆå®é™…åº”ç”¨ä¸­åº”è¯¥ä»æ–‡ä»¶åŠ è½½ï¼‰
        baseline_results[model_name] = {
            'Spring': {'MSE': np.random.uniform(0.8, 1.2), 'MAPE': np.random.uniform(8, 15)},
            'Summer': {'MSE': np.random.uniform(0.9, 1.3), 'MAPE': np.random.uniform(9, 16)},
            'Autumn': {'MSE': np.random.uniform(0.7, 1.1), 'MAPE': np.random.uniform(7, 14)},
            'Winter': {'MSE': np.random.uniform(0.8, 1.2), 'MAPE': np.random.uniform(8, 15)}
        }
    
    return baseline_results

def create_robustness_visualizations(results, baseline_results, save_dir, dataset_name, prediction_scale):
    """
    åˆ›å»ºé²æ£’æ€§åˆ†æçš„å¯è§†åŒ–å›¾è¡¨
    
    Args:
        results: é²æ£’æ€§è¯„ä¼°ç»“æœ
        baseline_results: åŸºçº¿æ¨¡å‹ç»“æœ
        save_dir: ä¿å­˜ç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        prediction_scale: é¢„æµ‹å°ºåº¦
    """
    plt.style.use('default')

    # 1. å™ªå£°/ç¼ºå¤±å®éªŒç»“æœè¡¨æ ¼ï¼ˆä»…å½“ç»“æœä¸­å­˜åœ¨ baseline æ—¶ç»˜åˆ¶ï¼‰
    if 'baseline' in results and 'noise_robustness' in results and 'missing_robustness' in results:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        # å™ªå£°é²æ£’æ€§è¡¨æ ¼
        noise_data = []
        noise_data.append(['Baseline', f"{results['baseline']['MSE']:.3f}", 
                          f"{results['baseline']['MAPE']:.2f}%"])
        for noise_level, metrics in results['noise_robustness'].items():
            noise_data.append([f'Noise {noise_level}', f"{metrics['MSE']:.3f}", 
                              f"{metrics['MAPE']:.2f}%"])
        for missing_level, metrics in results['missing_robustness'].items():
            noise_data.append([f'Missing {missing_level}', f"{metrics['MSE']:.3f}", 
                              f"{metrics['MAPE']:.2f}%"])
        # åˆ›å»ºè¡¨æ ¼
        table1 = ax1.table(cellText=noise_data,
                          colLabels=['Condition', 'MSE', 'MAPE'],
                          cellLoc='center',
                          loc='center')
        table1.auto_set_font_size(False)
        table1.set_fontsize(10)
        table1.scale(1, 2)
        ax1.axis('off')
        ax1.set_title(f'Noise & Missing Data Robustness\n{dataset_name} - {prediction_scale}', 
                     fontsize=14, fontweight='bold', pad=20)
        # æ€§èƒ½é€€åŒ–æŸ±çŠ¶å›¾
        conditions = []
        mse_degradation = []
        mape_degradation = []
        for noise_level, metrics in results['noise_robustness'].items():
            conditions.append(f'Noise {noise_level}')
            mse_degradation.append(metrics['MSE_degradation'])
            mape_degradation.append(metrics['MAPE_degradation'])
        for missing_level, metrics in results['missing_robustness'].items():
            conditions.append(f'Missing {missing_level}')
            mse_degradation.append(metrics['MSE_degradation'])
            mape_degradation.append(metrics['MAPE_degradation'])
        x = np.arange(len(conditions))
        width = 0.25
        ax2.bar(x - width, mse_degradation, width, label='MSE Degradation (%)', alpha=0.8)
        ax2.bar(x, mape_degradation, width, label='MAPE Degradation (%)', alpha=0.8)
        ax2.set_xlabel('Test Conditions')
        ax2.set_ylabel('Performance Degradation (%)')
        ax2.set_title(f'Performance Degradation Analysis\n{dataset_name} - {prediction_scale}')
        ax2.set_xticks(x)
        ax2.set_xticklabels(conditions, rotation=45, ha='right')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (mse, mape) in enumerate(zip(mse_degradation, mape_degradation)):
            ax2.text(i - width, mse + 0.1, f'{mse:.1f}%', ha='center', va='bottom', fontsize=8)
            ax2.text(i, mape + 0.1, f'{mape:.1f}%', ha='center', va='bottom', fontsize=8)
        plt.tight_layout()
        # ä¿å­˜å™ªå£°/ç¼ºå¤±å®éªŒå›¾è¡¨
        plot_path1 = save_dir / f'robustness_noise_missing_{dataset_name}_{prediction_scale}.png'
        plt.savefig(plot_path1, dpi=300, bbox_inches='tight')
        plt.savefig(str(plot_path1).replace('.png', '.pdf'), bbox_inches='tight')
        print(f"ğŸ“Š å™ªå£°/ç¼ºå¤±é²æ£’æ€§å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path1}")
        plt.show()
    else:
        print('â„¹ï¸ ç‹¬ç«‹å­£èŠ‚æ¨¡å¼ï¼šè·³è¿‡å™ªå£°/ç¼ºå¤±é²æ£’æ€§å›¾ï¼ˆæ—  baseline ç»“æœï¼‰ã€‚')
    
    # 2. å­£èŠ‚æ€§æ€§èƒ½å¯¹æ¯”å›¾ï¼ˆä»…å½“å­˜åœ¨å­£èŠ‚æ€§ç»“æœæ—¶ç»˜åˆ¶ï¼‰
    if 'seasonal_performance' in results:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        seasons = ['Spring', 'Summer', 'Autumn', 'Winter']
        metrics = ['MSE', 'MAPE']
        # å‡†å¤‡æ•°æ®
        our_model_data = {metric: [] for metric in metrics}
        for season in seasons:
            if season in results['seasonal_performance']:
                for metric in metrics:
                    our_model_data[metric].append(results['seasonal_performance'][season][metric])
            else:
                for metric in metrics:
                    our_model_data[metric].append(0)
        # åŸºçº¿æ¨¡å‹æ•°æ®
        baseline_model_names = list(baseline_results.keys())
        baseline_data = {model: {metric: [] for metric in metrics} for model in baseline_model_names}
        for model in baseline_model_names:
            for season in seasons:
                for metric in metrics:
                    baseline_data[model][metric].append(baseline_results[model][season][metric])
        # ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”å›¾
        axes = [ax1, ax2, ax3]
        metric_titles = ['Mean Squared Error (MSE)', 'Mean Absolute Percentage Error (MAPE)']
        for idx, (ax, metric, title) in enumerate(zip(axes, metrics, metric_titles)):
            x = np.arange(len(seasons))
            width = 0.15
            ax.bar(x - 2*width, our_model_data[metric], width, label='Our Model (adap_auto)', color='#1f77b4', alpha=0.8)
            colors = ['#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
            for i, model in enumerate(baseline_model_names):
                ax.bar(x - width + i*width, baseline_data[model][metric], width, label=model, color=colors[i], alpha=0.7)
            ax.set_xlabel('Seasons')
            ax.set_ylabel(metric)
            ax.set_title(f'{title} by Season\n{dataset_name} - {prediction_scale}')
            ax.set_xticks(x)
            ax.set_xticklabels(seasons)
            ax.legend()
            ax.grid(True, alpha=0.3)
            for i, v in enumerate(our_model_data[metric]):
                ax.text(i - 2*width, v + max(our_model_data[metric]) * 0.01, f'{v:.3f}', ha='center', va='bottom', fontsize=8)
        # ç¬¬å››ä¸ªå­å›¾ï¼šæ ·æœ¬æ•°é‡åˆ†å¸ƒ
        sample_counts = [results['seasonal_performance'][season].get('sample_count', results['seasonal_performance'][season].get('test_samples', 0)) for season in seasons]
        ax4.bar(seasons, sample_counts, color='#17becf', alpha=0.7)
        ax4.set_xlabel('Seasons')
        ax4.set_ylabel('Sample Count')
        ax4.set_title(f'Sample Distribution by Season\n{dataset_name} - {prediction_scale}')
        ax4.grid(True, alpha=0.3)
        for i, v in enumerate(sample_counts):
            ax4.text(i, v + max(sample_counts) * 0.01, f'{v}', ha='center', va='bottom', fontsize=10)
        plt.tight_layout()
        plot_path2 = save_dir / f'seasonal_performance_{dataset_name}_{prediction_scale}.png'
        plt.savefig(plot_path2, dpi=300, bbox_inches='tight')
        plt.savefig(str(plot_path2).replace('.png', '.pdf'), bbox_inches='tight')
        print(f"ğŸ“Š å­£èŠ‚æ€§æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path2}")
        plt.show()
    else:
        print('â„¹ï¸ é»˜è®¤æ¨¡å¼æœªåŒ…å«å­£èŠ‚æ€§è¯„ä¼°ï¼Œè·³è¿‡å­£èŠ‚æ€§å›¾è¡¨ã€‚')

def load_window_data_directly(dataset_name, prediction_scale, seq_length=36, c_out=6, split_ratio=0.99, use_std=True):
    """
    ç›´æ¥åŠ è½½é¢„å¤„ç†å¥½çš„çª—å£æ•°æ®å’Œè¾¹ç´¢å¼•æ•°æ®
    
    Args:
        dataset_name: æ•°æ®é›†åç§° ('fujian' æˆ– 'DSWE')
        prediction_scale: é¢„æµ‹å°ºåº¦ (å¦‚ '6-0_1', '24-1' ç­‰)
        seq_length: åºåˆ—é•¿åº¦
        c_out: é¢„æµ‹é•¿åº¦
        split_ratio: è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†æ¯”ä¾‹
        use_std: æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–åçš„æ•°æ®æ–‡ä»¶
    Returns:
        åŒ…å«æ•°æ®å’Œå…ƒä¿¡æ¯çš„å­—å…¸
    """
    print(f"\nğŸ“‚ ç›´æ¥åŠ è½½çª—å£æ•°æ®")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"é¢„æµ‹å°ºåº¦: {prediction_scale}")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æ•°æ®è·¯å¾„æ˜ å°„
    if dataset_name.lower() == 'fujian':
        data_dir = os.path.join(script_dir, '../../data/fujian')
        edge_dir = os.path.join(script_dir, '../../new_data/fujian')
        csv_path = os.path.join(script_dir, '../../data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
        
        # çª—å£æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒstdæˆ–åŸå§‹ï¼‰
        if use_std:
            train_file = f'stdtrain_data{prediction_scale}.npy'
            val_file = f'stdval_data{prediction_scale}.npy'
        else:
            train_file = f'train_data{prediction_scale}.npy'
            val_file = f'val_data{prediction_scale}.npy'
        # è¾¹ç´¢å¼•æ–‡ä»¶
        edge_file = f'adag_dict_train_data{prediction_scale}_fused.pkl'
        
    elif dataset_name.lower() == 'dswe':
        data_dir = os.path.join(script_dir, '../../data/DSWE')
        edge_dir = os.path.join(script_dir, '../../new_data/DSWE')
        csv_path = os.path.join(script_dir, '../../data/DSWE/Offshore Wind Farm Dataset1(WT5).csv')
        
        # çª—å£æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒstdæˆ–åŸå§‹ï¼‰
        if use_std:
            train_file = f'stdtrain_data{prediction_scale}.npy'
            val_file = f'stdval_data{prediction_scale}.npy'
        else:
            train_file = f'train_data{prediction_scale}.npy'
            val_file = f'val_data{prediction_scale}.npy'
        # è¾¹ç´¢å¼•æ–‡ä»¶
        edge_file = f'adag_dict_{prediction_scale}.pkl'
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # æ„å»ºå®Œæ•´è·¯å¾„
    train_path = os.path.join(data_dir, train_file)
    val_path = os.path.join(data_dir, val_file)
    edge_path = os.path.join(edge_dir, edge_file)
    scaler_path = os.path.join(data_dir, 'scaler.pkl')
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    missing_files = []
    for path, name in [(train_path, 'è®­ç»ƒæ•°æ®'), (val_path, 'éªŒè¯æ•°æ®'), 
                       (edge_path, 'è¾¹ç´¢å¼•æ•°æ®'), (scaler_path, 'æ ‡å‡†åŒ–å™¨'), (csv_path, 'CSVå…ƒæ•°æ®')]:
        if not os.path.exists(path):
            missing_files.append(f"{name}: {path}")
    
    if missing_files:
        raise FileNotFoundError(f"ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨:\n" + "\n".join(missing_files))
    
    print(f"âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶éªŒè¯é€šè¿‡")
    print(f"   è®­ç»ƒæ•°æ®: {train_path}")
    print(f"   éªŒè¯æ•°æ®: {val_path}")
    print(f"   è¾¹ç´¢å¼•: {edge_path}")
    print(f"   æ ‡å‡†åŒ–å™¨: {scaler_path}")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    
    # åŠ è½½çª—å£æ•°æ®
    x_data = torch.tensor(np.load(train_path)).to(dtype=torch.float32)
    y_data = torch.tensor(np.squeeze(np.load(val_path)[:, :, 0:1], axis=2)).to(dtype=torch.float32)
    
    # ä¾æ®çœŸå®yé•¿åº¦ç¡®å®šé¢„æµ‹æ­¥é•¿
    pred_len = int(y_data.shape[1])
    
    # åŠ è½½è¾¹ç´¢å¼•
    with open(edge_path, 'rb') as f:
        edge_indices = pickle.load(f)
    
    # åŠ è½½æ ‡å‡†åŒ–å™¨
    scaler = None
    if os.path.exists(scaler_path) and use_std:
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)
    
    # è¯»å–CSVå…ƒæ•°æ®è·å–ç‰¹å¾åç§°
    csv_data = pd.read_csv(csv_path, nrows=6)
    feature_columns = csv_data.drop(['Site_ID', 'Timestamp'], axis=1).columns.tolist()
    
    print(f"   æ•°æ®å½¢çŠ¶: X{x_data.shape}, y{y_data.shape}")
    print(f"   è¾¹ç´¢å¼•æ•°é‡: {len(edge_indices)}")
    print(f"   ç‰¹å¾æ•°é‡: {len(feature_columns)}")
    print(f"   ç‰¹å¾åç§°: {feature_columns}")
    
    # æ•°æ®åˆ’åˆ†
    split_index = int(len(x_data) * split_ratio)
    X_train = x_data[:split_index]
    X_test = x_data[split_index:]
    y_train = y_data[:split_index]
    y_test = y_data[split_index:]
    
    train_edge_indices = edge_indices[:split_index]
    test_edge_indices = edge_indices[split_index:]
    
    print(f"   è®­ç»ƒé›†: X{X_train.shape}, y{y_train.shape}, è¾¹ç´¢å¼•{len(train_edge_indices)}")
    print(f"   æµ‹è¯•é›†: X{X_test.shape}, y{y_test.shape}, è¾¹ç´¢å¼•{len(test_edge_indices)}")
    
    # æ„å»ºè¿”å›æ•°æ®
    model_data = {
        'dataset_name': dataset_name,
        'prediction_scale': prediction_scale,
        'seq_length': seq_length,
        'pred_length': pred_len,
        'split_ratio': split_ratio,
        'feature_names': feature_columns,
        'num_features': len(feature_columns),
        'X_train': X_train,
        'y_train': y_train,
        'X_test': X_test,
        'y_test': y_test,
        'train_edge_indices': train_edge_indices,
        'test_edge_indices': test_edge_indices,
        'scaler': scaler,
        'created_at': datetime.now().isoformat(),
        'csv_path': csv_path
    }
    
    return model_data

def run_robustness_experiment(dataset_name, prediction_scale, args, device):
    """
    è¿è¡Œé²æ£’æ€§åˆ†æå®éªŒ
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        prediction_scale: é¢„æµ‹å°ºåº¦
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    print(f"\nğŸ”¬ å¼€å§‹é²æ£’æ€§åˆ†æå®éªŒ")
    print(f"æ•°æ®é›†: {dataset_name}")
    print(f"é¢„æµ‹å°ºåº¦: {prediction_scale}")
    
    start_time = time.time()
    
    try:
        # 1. æ•°æ®å‡†å¤‡
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        
        if hasattr(args, 'use_preprocessed') and args.use_preprocessed:
            # ä½¿ç”¨é¢„å¤„ç†æ•°æ®
            if hasattr(args, 'data_dir') and args.data_dir:
                data_dir = Path(args.data_dir)
            else:
                script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
                data_dir = script_dir / 'preprocessed_data'
            
            data_file = data_dir / f'{dataset_name}_{prediction_scale}_robustness_data.pkl'
            model_data = load_preprocessed_data(data_file)
        else:
            # ç›´æ¥ä½¿ç”¨çª—å£æ•°æ®å’Œè¾¹ç´¢å¼•æ•°æ®
            print("ğŸ“‚ ç›´æ¥åŠ è½½çª—å£æ•°æ®å’Œè¾¹ç´¢å¼•æ•°æ®")
            model_data = load_window_data_directly(
                dataset_name=dataset_name,
                prediction_scale=prediction_scale,
                seq_length=args.seq_length,
                c_out=args.c_out,
                split_ratio=args.split_ratio,
                use_std=args.use_preprocessed_std # ä¼ é€’use_preprocessed_stdå‚æ•°
            )
        
        X_train = model_data['X_train'].to(device)
        y_train = model_data['y_train'].to(device)
        X_test = model_data['X_test'].to(device)
        y_test = model_data['y_test'].to(device)
        train_edge_indices = model_data['train_edge_indices']
        test_edge_indices = model_data['test_edge_indices']
        scaler = model_data['scaler']
        
        print(f"è®­ç»ƒé›†å½¢çŠ¶: {X_train.shape}, {y_train.shape}")
        print(f"æµ‹è¯•é›†å½¢çŠ¶: {X_test.shape}, {y_test.shape}")
        print(f"ç‰¹å¾æ•°é‡: {model_data['num_features']}")
        print(f"åºåˆ—é•¿åº¦: {model_data['seq_length']}")
        print(f"é¢„æµ‹é•¿åº¦: {model_data['pred_length']}")
        
        # è‹¥ä¸ºç‹¬ç«‹å­£èŠ‚æ¨¡å¼ï¼Œç›´æ¥è¿›è¡Œå­£èŠ‚æ€§ç‹¬ç«‹å®éªŒï¼Œè·³è¿‡å…¨å±€æ¨¡å‹è®­ç»ƒ
        if args.seasonal_mode == 'independent':
            print("ğŸŒ¸ ä½¿ç”¨å­£èŠ‚ç‹¬ç«‹æ¨¡å¼...")
            seasonal_results = run_seasonal_independent_experiment(model_data, args, device)
            training_time = time.time() - start_time
            robustness_results = {
                'seasonal_performance': seasonal_results,
                'training_time': training_time,
                'dataset': dataset_name,
                'prediction_scale': prediction_scale,
                'seasonal_mode': 'independent'
            }
            return robustness_results

        # 2. æ¨¡å‹åˆå§‹åŒ–
        print("ğŸ—ï¸ åˆå§‹åŒ–æ¨¡å‹...")
        
        # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´hidden_sizeï¼Œç¡®ä¿èƒ½è¢«ç‰¹å¾æ•°æ•´é™¤
        num_features = model_data['num_features']
        base_hidden_size = args.hidden_size
        adjusted_hidden_size = ((base_hidden_size + num_features - 1) // num_features) * num_features
        
        print(f"   è¾“å…¥ç‰¹å¾æ•°: {num_features}")
        print(f"   åŸå§‹hidden_size: {base_hidden_size}, è°ƒæ•´å: {adjusted_hidden_size}")
        
        model = adap_auto(
            n_head=args.n_head,
            hidden_size=adjusted_hidden_size,
            factor=args.factor,
            dropout=args.dropout,
            conv_hidden_size=args.conv_hidden_size,
            MovingAvg_window=args.moving_avg_window,
            activation=args.activation,
            encoder_layers=args.encoder_layers,
            decoder_layers=args.decoder_layers,
            c_out=args.c_out,
            c_in=num_features,
            seq_lenth=model_data['seq_length'],
            gruop_dec=args.group_dec
        ).to(device)
        
        # 3. æ¨¡å‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        criterion = nn.MSELoss()
        
        # è®­ç»ƒå¾ªç¯
        best_val_loss = float('inf')
        patience_counter = 0
        I = 0
        
        for epoch in range(args.epochs):
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            I = 0
            
            for batch_X, batch_y in train_loader:
                batch_X, batch_y = batch_X.to(device), batch_y.to(device)
                
                # è·å–å¯¹åº”æ‰¹æ¬¡çš„è¾¹ç´¢å¼•
                dicts = train_edge_indices[I:I+len(batch_X)]
                I += len(batch_X)
                
                optimizer.zero_grad()
                outputs = model(batch_X, dicts).squeeze(-1)
                loss = criterion(outputs, batch_y)
                
                # L1æ­£åˆ™åŒ–
                l1_reg = torch.tensor(0.).to(device)
                for param in model.parameters():
                    l1_reg += torch.norm(param, 1)
                loss += args.l1_lambda * l1_reg
                
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # éªŒè¯é˜¶æ®µï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
            model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                prediction = model(X_test, test_edge_indices).squeeze(-1)
                val_loss = criterion(prediction, y_test).item()
            
            train_loss /= len(train_loader)
            
            print(f"Epoch {epoch+1}/{args.epochs}, Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                best_model_state = model.state_dict().copy()
            else:
                patience_counter += 1
                if patience_counter >= args.patience:
                    print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(best_model_state)
        
        training_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶: {training_time:.2f} ç§’")
        
        # 4. é²æ£’æ€§è¯„ä¼°
        print("ğŸ” å¼€å§‹é²æ£’æ€§è¯„ä¼°...")
        
        if args.seasonal_mode == 'independent':
            # å­£èŠ‚ç‹¬ç«‹æ¨¡å¼ï¼šé‡æ–°è¿›è¡Œå­£èŠ‚æ€§ç‹¬ç«‹å®éªŒ
            print("ğŸŒ¸ ä½¿ç”¨å­£èŠ‚ç‹¬ç«‹æ¨¡å¼...")
            seasonal_results = run_seasonal_independent_experiment(model_data, args, device)
            
            # æ„å»ºä¸æ ‡å‡†æ¨¡å¼å…¼å®¹çš„ç»“æœæ ¼å¼
            robustness_results = {
                'seasonal_performance': seasonal_results,
                'training_time': training_time,
                'dataset': dataset_name,
                'prediction_scale': prediction_scale,
                'seasonal_mode': 'independent'
            }
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨ç»Ÿä¸€è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
            robustness_results = evaluate_model_robustness(
                model=model,
                X_test=X_test,
                y_test=y_test,
                test_edge_indices=test_edge_indices,
                scaler=scaler,
                device=device,
                noise_levels=args.noise_levels,
                missing_ratios=args.missing_ratio,
                dataset_name=dataset_name,
                include_seasonal_in_default=args.include_seasonal_eval
            )
            
            # æ·»åŠ è®­ç»ƒæ—¶é—´åˆ°ç»“æœ
            robustness_results['training_time'] = training_time
            robustness_results['dataset'] = dataset_name
            robustness_results['prediction_scale'] = prediction_scale
            robustness_results['seasonal_mode'] = 'test_split'
        
        return robustness_results
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def save_results(results, save_dir, dataset_name, prediction_scale):
    """
    ä¿å­˜å®éªŒç»“æœ
    
    Args:
        results: å®éªŒç»“æœ
        save_dir: ä¿å­˜ç›®å½•
        dataset_name: æ•°æ®é›†åç§°
        prediction_scale: é¢„æµ‹å°ºåº¦
    """
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = save_dir / f'robustness_results_{dataset_name}_{prediction_scale}.pkl'
    with open(results_file, 'wb') as f:
        pickle.dump(results, f)
    print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # æ£€æŸ¥å®éªŒæ¨¡å¼
    seasonal_mode = results.get('seasonal_mode', 'test_split')
    
    if seasonal_mode == 'independent':
        # ç‹¬ç«‹å­£èŠ‚æ¨¡å¼ï¼šåªä¿å­˜å­£èŠ‚æ€§ç»“æœ
        print("ğŸ“Š ä¿å­˜ç‹¬ç«‹å­£èŠ‚å®éªŒç»“æœ...")
        
        # ä¿å­˜å­£èŠ‚æ€§æ€§èƒ½
        seasonal_file = save_dir / f'seasonal_independent_{dataset_name}_{prediction_scale}.csv'
        seasonal_data = []
        
        for season, metrics in results['seasonal_performance'].items():
            seasonal_data.append({
                'Season': season,
                'MSE': metrics['MSE'],
                'MAPE': metrics['MAPE'],
                'Train_Samples': metrics.get('train_samples', 0),
                'Test_Samples': metrics.get('test_samples', 0),
                'Total_Samples': metrics.get('total_samples', 0)
            })
        
        seasonal_df = pd.DataFrame(seasonal_data)
        seasonal_df.to_csv(seasonal_file, index=False)
        print(f"ğŸŒ¸ ç‹¬ç«‹å­£èŠ‚æ€§èƒ½å·²ä¿å­˜åˆ°: {seasonal_file}")
        
    else:
        # æ ‡å‡†æ¨¡å¼ï¼šä¿å­˜å®Œæ•´çš„é²æ£’æ€§åˆ†æç»“æœ
        print("ğŸ“Š ä¿å­˜æ ‡å‡†é²æ£’æ€§åˆ†æç»“æœ...")
        
        # ä¿å­˜æ±‡æ€»è¡¨æ ¼
        summary_file = save_dir / f'robustness_summary_{dataset_name}_{prediction_scale}.csv'
        summary_data = []
        
        # åŸºçº¿æ€§èƒ½
        if 'baseline' in results:
            summary_data.append({
                'Condition': 'Baseline',
                'MSE': results['baseline']['MSE'],
                'MAPE': results['baseline']['MAPE'],
                'MSE_Degradation(%)': 0,
                'MAPE_Degradation(%)': 0
            })
        
        # å™ªå£°é²æ£’æ€§
        if 'noise_robustness' in results:
            for noise_level, metrics in results['noise_robustness'].items():
                summary_data.append({
                    'Condition': f'Noise_{noise_level}',
                    'MSE': metrics['MSE'],
                    'MAPE': metrics['MAPE'],
                    'MSE_Degradation(%)': metrics['MSE_degradation'],
                    'MAPE_Degradation(%)': metrics['MAPE_degradation']
                })
        
        # ç¼ºå¤±é²æ£’æ€§
        if 'missing_robustness' in results:
            for missing_level, metrics in results['missing_robustness'].items():
                summary_data.append({
                    'Condition': f'Missing_{missing_level}',
                    'MSE': metrics['MSE'],
                    'MAPE': metrics['MAPE'],
                    'MSE_Degradation(%)': metrics['MSE_degradation'],
                    'MAPE_Degradation(%)': metrics['MAPE_degradation']
                })
        
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_csv(summary_file, index=False)
            print(f"ğŸ“‹ æ±‡æ€»è¡¨æ ¼å·²ä¿å­˜åˆ°: {summary_file}")
        
        # ä¿å­˜å­£èŠ‚æ€§æ€§èƒ½
        if 'seasonal_performance' in results:
            seasonal_file = save_dir / f'seasonal_performance_{dataset_name}_{prediction_scale}.csv'
            seasonal_data = []
            
            for season, metrics in results['seasonal_performance'].items():
                seasonal_data.append({
                    'Season': season,
                    'MSE': metrics['MSE'],
                    'MAPE': metrics['MAPE'],
                    'Sample_Count': metrics.get('sample_count', 0)
                })
            
            seasonal_df = pd.DataFrame(seasonal_data)
            seasonal_df.to_csv(seasonal_file, index=False)
            print(f"ğŸŒ¸ å­£èŠ‚æ€§æ€§èƒ½å·²ä¿å­˜åˆ°: {seasonal_file}")

def main():
    parser = argparse.ArgumentParser(description='Robustness Analysis Experiment for wind power forecasting model')
    
    # æ•°æ®é›†ç›¸å…³å‚æ•°
    parser.add_argument('--dataset', type=str, default='fujian', choices=['fujian', 'DSWE'], 
                        help='Dataset name')
    parser.add_argument('--prediction_scale', type=str, default='6-1', 
                        help='Prediction scale (e.g., 6-0_1, 24-1, etc.)')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--gpu', type=int, default=0, help='GPU device id')
    parser.add_argument('--epochs', type=int, default=15, help='Maximum number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate')
    parser.add_argument('--l1_lambda', type=float, default=0.01, help='L1 regularization coefficient')
    parser.add_argument('--weight_decay', type=float, default=0.05, help='L2 weight decay')
    parser.add_argument('--dropout', type=float, default=0.05, help='Dropout rate')
    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience')
    parser.add_argument('--split_ratio', type=float, default=0.99, help='Train/test split ratio')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--n_head', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--hidden_size', type=int, default=264, help='Hidden size')
    parser.add_argument('--factor', type=int, default=2, help='Factor for attention')
    parser.add_argument('--conv_hidden_size', type=int, default=32, help='Convolution hidden size')
    parser.add_argument('--moving_avg_window', type=int, default=3, help='Moving average window size')
    parser.add_argument('--activation', type=str, default='gelu', help='Activation function')
    parser.add_argument('--encoder_layers', type=int, default=1, help='Number of encoder layers')
    parser.add_argument('--decoder_layers', type=int, default=1, help='Number of decoder layers')
    parser.add_argument('--seq_length', type=int, default=36, help='Sequence length')
    parser.add_argument('--c_out', type=int, default=6, help='Output channels')
    parser.add_argument('--group_dec', action='store_true', default=True, help='Use group decoder')
    
    # é²æ£’æ€§æµ‹è¯•å‚æ•°
    parser.add_argument('--noise_levels', nargs='+', type=float, default=[0.05, 0.1],
                        help='Noise levels for robustness testing')
    parser.add_argument('--missing_ratio', nargs='+', type=float, default=[0.05, 0.1],
                        help='Missing data ratio(s) for robustness testing, can pass multiple values')
    parser.add_argument('--include_seasonal_eval', action='store_true', default=False,
                        help='Include seasonal evaluation in default (test_split) mode')
    parser.add_argument('--seasonal_mode', type=str, default='independent', choices=['test_split', 'independent'],
                        help='Seasonal analysis mode: test_split (divide test set) or independent (divide whole dataset)')
    parser.add_argument('--season_split_ratio', type=float, default=0.95,
                        help='Train split ratio within each season in independent mode (default: 0.8)')
    
    # æ•°æ®åŠ è½½å‚æ•°
    parser.add_argument('--use_preprocessed', action='store_true',
                        help='Use preprocessed data instead of processing from scratch')
    parser.add_argument('--data_dir', type=str, default=None,
                        help='Directory containing preprocessed data (default: ./preprocessed_data)')
    parser.add_argument('--use_preprocessed_std', action='store_true', default=True,
                        help='Use standardized data files (stdtrain_data.npy, stdval_data.npy) if available')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(seed=args.seed)
    
    # è®¾ç½®è®¾å¤‡
    mp.set_start_method('spawn', force=True)
    torch.cuda.set_device(args.gpu)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    results_dir = script_dir / 'results' / 'robustness_analysis_experiment'
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¯ å¼€å§‹é²æ£’æ€§åˆ†æå®éªŒ")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"é¢„æµ‹å°ºåº¦: {args.prediction_scale}")
    print(f"å™ªå£°å¼ºåº¦: {args.noise_levels}")
    print(f"ç¼ºå¤±æ¯”ä¾‹: {args.missing_ratio}")
    print(f"ç»“æœä¿å­˜ç›®å½•: {results_dir}")
    
    # è¿è¡Œå®éªŒ
    results = run_robustness_experiment(args.dataset, args.prediction_scale, args, device)
    
    if results is not None:
        # ä¿å­˜ç»“æœ
        save_results(results, results_dir, args.dataset, args.prediction_scale)
        
        # åŠ è½½åŸºçº¿ç»“æœ
        baseline_results = load_baseline_results(results_dir, args.dataset, args.prediction_scale)
        
        # åˆ›å»ºå¯è§†åŒ–
        create_robustness_visualizations(results, baseline_results, results_dir, args.dataset, args.prediction_scale)
        
        print(f"\nâœ… é²æ£’æ€§åˆ†æå®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
        
        # æ‰“å°å…³é”®ç»“æœæ‘˜è¦
        print(f"\nï¿½ï¿½ å®éªŒç»“æœæ‘˜è¦:")
        
        seasonal_mode = results.get('seasonal_mode', 'test_split')
        
        if seasonal_mode == 'independent':
            print(f"æ¨¡å¼: ç‹¬ç«‹å­£èŠ‚è®­ç»ƒ")
            print(f"å­£èŠ‚æ€§æ€§èƒ½:")
            for season, metrics in results['seasonal_performance'].items():
                train_samples = metrics.get('train_samples', 0)
                test_samples = metrics.get('test_samples', 0)
                print(f"  {season}: MSE={metrics['MSE']:.4f}, MAPE={metrics['MAPE']:.2f}% (è®­ç»ƒ: {train_samples}, æµ‹è¯•: {test_samples})")
        else:
            print(f"æ¨¡å¼: æ ‡å‡†é²æ£’æ€§åˆ†æ")
            
            if 'baseline' in results:
                print(f"åŸºçº¿æ€§èƒ½ - MSE: {results['baseline']['MSE']:.4f}, MAPE: {results['baseline']['MAPE']:.2f}%")
            
            if 'noise_robustness' in results:
                for noise_level, metrics in results['noise_robustness'].items():
                    print(f"å™ªå£° {noise_level} - MSEé€€åŒ–: {metrics['MSE_degradation']:.2f}%, MAPEé€€åŒ–: {metrics['MAPE_degradation']:.2f}%")
            
            if 'missing_robustness' in results:
                for missing_level, metrics in results['missing_robustness'].items():
                    print(f"ç¼ºå¤± {missing_level} - MSEé€€åŒ–: {metrics['MSE_degradation']:.2f}%, MAPEé€€åŒ–: {metrics['MAPE_degradation']:.2f}%")
            
            if 'seasonal_performance' in results:
                print(f"å­£èŠ‚æ€§æ€§èƒ½:")
                for season, metrics in results['seasonal_performance'].items():
                    sample_count = metrics.get('sample_count', 0)
                    if sample_count > 0:
                        print(f"  {season}: MSE={metrics['MSE']:.4f}, MAPE={metrics['MAPE']:.2f}% (æ ·æœ¬æ•°: {sample_count})")
    else:
        print("âŒ å®éªŒå¤±è´¥")

if __name__ == '__main__':
    main() 