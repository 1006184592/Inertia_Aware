import os
import sys
# Set CUDA launch blocking for debugging
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶å‘ä¸Šé€€ä¸€å±‚ï¼ˆçˆ¶ç›®å½•ï¼‰
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)  # å°†çˆ¶ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„
import pickle
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from adap_auto import adap_auto
from evaluate import MSE, MAPE
import torch.multiprocessing as mp
import time
import argparse
import random
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

def seed_everything(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def is_standardized_data(train_data_path, val_data_path):
    """
    æ£€æµ‹æ•°æ®æ˜¯å¦ä¸ºæ ‡å‡†åŒ–æ•°æ®
    Args:
        train_data_path: è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        val_data_path: éªŒè¯æ•°æ®æ–‡ä»¶è·¯å¾„
    Returns:
        bool: Trueè¡¨ç¤ºæ˜¯æ ‡å‡†åŒ–æ•°æ®ï¼ŒFalseè¡¨ç¤ºæ˜¯åŸå§‹æ•°æ®
    """
    train_filename = os.path.basename(train_data_path)
    val_filename = os.path.basename(val_data_path)
    
    print(f"ğŸ” æ–‡ä»¶åæ£€æµ‹: {train_filename}, {val_filename}")
    
    # ä¸»è¦åˆ¤æ–­ï¼šæ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ˜ç¡®è¡¨ç¤ºæ ‡å‡†åŒ–æ•°æ®
    if train_filename.startswith('std') or val_filename.startswith('std'):
        print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶åä»¥'std'å¼€å¤´)")
        return True
    
    if 'std' in train_filename.lower() or 'std' in val_filename.lower():
        print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶ååŒ…å«'std')")
        return True
    
    standardized_keywords = ['standard', 'standardized', 'norm', 'normalized']
    for keyword in standardized_keywords:
        if keyword in train_filename.lower() or keyword in val_filename.lower():
            print(f"âœ… æ£€æµ‹ç»“æœ: æ ‡å‡†åŒ–æ•°æ® (æ–‡ä»¶ååŒ…å«'{keyword}')")
            return True
    
    if ('train_data' in train_filename and 'std' not in train_filename.lower()) or \
       ('val_data' in val_filename and 'std' not in val_filename.lower()):
        print(f"âŒ æ£€æµ‹ç»“æœ: åŸå§‹æ•°æ® (æ–‡ä»¶åè¡¨æ˜æ˜¯éæ ‡å‡†åŒ–æ•°æ®)")
        return False
    
    data_dir = os.path.dirname(train_data_path)
    scaler_path = os.path.join(data_dir, 'scaler.pkl')
    
    if os.path.exists(scaler_path):
        print(f"â„¹ï¸  å‘ç°scaleræ–‡ä»¶: {scaler_path} (ä½†æ–‡ä»¶åæœªæ˜ç¡®æ ‡ç¤ºä¸ºæ ‡å‡†åŒ–æ•°æ®)")
        print(f"âŒ æ£€æµ‹ç»“æœ: åŸå§‹æ•°æ® (ä¼˜å…ˆä¿¡ä»»æ–‡ä»¶ååˆ¤æ–­)")
        return False
    
    print(f"âŒ æ£€æµ‹ç»“æœ: åŸå§‹æ•°æ® (æœªå‘ç°æ ‡å‡†åŒ–æ•°æ®çš„æ˜ç¡®æ ‡å¿—)")
    return False

def inverse_transform_power(data, scaler, power_feature_idx=0):
    """
    å¯¹åŠŸç‡æ•°æ®è¿›è¡Œåæ ‡å‡†åŒ–
    Args:
        data: æ ‡å‡†åŒ–åçš„æ•°æ® (numpy array)
        scaler: sklearn StandardScalerå¯¹è±¡
        power_feature_idx: åŠŸç‡ç‰¹å¾åœ¨scalerä¸­çš„ç´¢å¼•
    Returns:
        åæ ‡å‡†åŒ–åçš„åŠŸç‡æ•°æ®
    """
    mean = scaler.mean_[power_feature_idx]
    scale = scaler.scale_[power_feature_idx]
    return data * scale + mean

def train_model_with_ratio(dataset_name, prediction_scale, train_ratio, args, device):
    """
    ä½¿ç”¨æŒ‡å®šè®­ç»ƒæ•°æ®æ¯”ä¾‹è®­ç»ƒæ¨¡å‹
    Args:
        dataset_name: æ•°æ®é›†åç§° ('fujian' æˆ– 'DSWE')
        prediction_scale: é¢„æµ‹å°ºåº¦ (å¦‚ '6-0_1')
        train_ratio: è®­ç»ƒæ•°æ®æ¯”ä¾‹ (0.5-1.0)
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
    Returns:
        dict: åŒ…å«æ€§èƒ½æŒ‡æ ‡å’Œè®­ç»ƒä¿¡æ¯çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - æ•°æ®é›†: {dataset_name}, å°ºåº¦: {prediction_scale}, è®­ç»ƒæ¯”ä¾‹: {train_ratio*100:.0f}%")
    print(f"{'='*60}")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(script_dir)  # ä¸Šä¸€çº§
    grandparent_dir = os.path.dirname(parent_dir)  # ä¸Šä¸¤çº§
    # æ ¹æ®æ•°æ®é›†è®¾ç½®è·¯å¾„
    if dataset_name.lower() == 'fujian':
        data_dir = os.path.join(grandparent_dir, f'data/fujian')    
        train_dir = os.path.join(data_dir, f'stdtrain_data{prediction_scale}.npy')
        val_dir = os.path.join(data_dir, f'stdval_data{prediction_scale}.npy')
        edge_dir = os.path.join(grandparent_dir, f'new_data/fujian/adag_dict_train_data{prediction_scale}_fused.pkl')
        csv_dir = os.path.join(grandparent_dir, f'data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
    elif dataset_name.lower() == 'dswe':
        data_dir = os.path.join(grandparent_dir, f'data/DSWE')
        train_dir = os.path.join(data_dir, f'stdtrain_data{prediction_scale}.npy')
        val_dir = os.path.join(data_dir, f'stdval_data{prediction_scale}.npy')
        # edge_dir = os.path.join(grandparent_dir, f'new_data/DSWE/adag_dict_train_data{prediction_scale}_fused.pkl')
        edge_dir = os.path.join(grandparent_dir, f'new_data/DSWE/adag_dict_{prediction_scale}.pkl')
        csv_dir = os.path.join(grandparent_dir, f'data/DSWE/Offshore Wind Farm Dataset1(WT5).csv')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    for file_path, file_desc in [(train_dir, "è®­ç»ƒæ•°æ®"), (val_dir, "éªŒè¯æ•°æ®"), (edge_dir, "è¾¹æ•°æ®"), (csv_dir, "CSVæ•°æ®")]:
        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯: {file_desc}æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
    
    # æ£€æµ‹æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–æ•°æ®
    use_standardized = is_standardized_data(train_dir, val_dir)
    print(f"æ•°æ®ç±»å‹æ£€æµ‹: {'æ ‡å‡†åŒ–æ•°æ®' if use_standardized else 'åŸå§‹æ•°æ®'}")
    
    # è¯»å–CSVå…ƒæ•°æ®
    data = pd.read_csv(csv_dir, nrows=6)
    if dataset_name.lower() == 'dswe':
        cols_to_drop_in_raw_csv = ['Sequence No.']
    elif dataset_name.lower() == 'fujian':
        cols_to_drop_in_raw_csv = ['Site_ID', 'Timestamp']
    df = data.drop(cols_to_drop_in_raw_csv, axis=1)
    
    # æ ¹æ®æ•°æ®ç±»å‹å†³å®šæ˜¯å¦åŠ è½½æ ‡å‡†åŒ–å™¨
    scaler = None
    if use_standardized:
        scaler_path = os.path.join(data_dir, 'scaler.pkl')
        if os.path.exists(scaler_path):
            with open(scaler_path, 'rb') as f:
                scaler = pickle.load(f)
            print(f"âœ… å·²åŠ è½½æ ‡å‡†åŒ–å™¨ï¼Œç‰¹å¾æ•°é‡: {len(scaler.mean_)}")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°æ ‡å‡†åŒ–æ•°æ®ä½†æœªæ‰¾åˆ°scaler.pklæ–‡ä»¶")
            use_standardized = False
    else:
        print(f"â„¹ï¸  ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œæ— éœ€åŠ è½½æ ‡å‡†åŒ–å™¨")

    # è¯»å–æ•°æ®
    x_data = torch.tensor(np.load(train_dir)).to(dtype=torch.float32)
    y_data = torch.tensor(np.squeeze(np.load(val_dir)[:, :,0:1], axis=2)).to(dtype=torch.float32)
    x_data, y_data = x_data.to(device), y_data.to(device)
    
    # è¯»å–ADAGè¾¹ä¿¡æ¯
    with open(edge_dir, 'rb') as f: 
        edge_index = pickle.load(f)
    
    # æ•°æ®åˆ’åˆ† - å…ˆæŒ‰åŸæ¯”ä¾‹åˆ’åˆ†ï¼Œå†è°ƒæ•´è®­ç»ƒé›†å¤§å°
    original_split_index = int(len(x_data) * args.split_ratio)
    X_original_train, X_test = x_data[0:original_split_index], x_data[original_split_index:]
    y_original_train, y_test = y_data[0:original_split_index], y_data[original_split_index:]
    original_train_dict, test_dict = edge_index[0:original_split_index], edge_index[original_split_index:]
    
    # æ ¹æ®train_ratioè°ƒæ•´è®­ç»ƒé›†å¤§å°
    adjusted_train_size = int(len(X_original_train) * train_ratio)
    X_train = X_original_train[:adjusted_train_size]
    y_train = y_original_train[:adjusted_train_size]
    train_dict = original_train_dict[:adjusted_train_size]
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  åŸå§‹è®­ç»ƒé›†å¤§å°: {len(X_original_train)}")
    print(f"  è°ƒæ•´åè®­ç»ƒé›†å¤§å°: {len(X_train)} ({train_ratio*100:.0f}%)")
    print(f"  æµ‹è¯•é›†å¤§å°: {len(X_test)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size)

    # åˆå§‹åŒ–æ¨¡å‹
    auto_model = adap_auto(
        n_head=args.n_head,
        hidden_size=args.hidden_size,
        factor=args.factor,
        dropout=args.dropout,
        conv_hidden_size=args.conv_hidden_size,
        MovingAvg_window=args.moving_avg_window,
        activation=args.activation,
        encoder_layers=args.encoder_layers,
        decoder_layers=args.decoder_layers,
        c_in=x_data.shape[-1],
        seq_lenth=args.seq_length,
        c_out=args.c_out,
        gruop_dec=args.group_dec,
        train_ratio=train_ratio
    )

    auto_model.to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_function = nn.MSELoss()
    optimizer = torch.optim.Adam(auto_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    l1_lambda = args.l1_lambda
    
    # Early stopping parameters
    patience = args.patience
    best_mse = float('inf')
    patience_counter = 0
    training_start_time = time.time()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        start_time = time.time()
        auto_model.train()
        total_loss = 0
        I = 0
        
        for batch in train_dataloader:
            inputs, targets = batch
            dicts = train_dict[I:I+len(inputs)]
            I += len(inputs)
            optimizer.zero_grad()
            model_output = auto_model(inputs, dicts).squeeze(-1)
            loss = loss_function(model_output, targets)
            l1_norm = sum(p.abs().sum() for p in auto_model.parameters())
            loss = loss + l1_lambda * l1_norm
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        # Validation loss
        auto_model.eval()
        with torch.no_grad():
            prediction = auto_model(X_test, test_dict).squeeze(-1)
            
            if use_standardized and scaler is not None:
                # ä½¿ç”¨åæ ‡å‡†åŒ–åçš„æ•°æ®è®¡ç®—éªŒè¯MSE
                y_test_np = y_test.cpu().numpy()
                prediction_np = prediction.cpu().numpy()
                y_test_original = inverse_transform_power(y_test_np, scaler, power_feature_idx=0)
                prediction_original = inverse_transform_power(prediction_np, scaler, power_feature_idx=0)
                val_mse = MSE(y_test_original, prediction_original)
            else:
                val_mse = MSE(y_test, prediction)
            
        # Check for early stopping
        if val_mse < best_mse:
            best_mse = val_mse
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch + 1}')
            break

        end_time = time.time()
        epoch_time = end_time - start_time
        print(f"Epoch {epoch + 1}/{args.epochs}, Loss: {total_loss / len(train_dataloader):.4f}, Val MSE: {val_mse:.4f}, Time: {epoch_time:.2f}s")

    training_end_time = time.time()
    total_training_time = training_end_time - training_start_time
    
    # æœ€ç»ˆè¯„ä¼°
    auto_model.eval()
    with torch.no_grad():
        prediction = auto_model(X_test, test_dict).squeeze(-1)
        
    # Convert to numpy for evaluation
    y_test_np, prediction_np = y_test.cpu().numpy(), prediction.detach().cpu().numpy()
    
    if use_standardized and scaler is not None:
        # å¯¹äºæ ‡å‡†åŒ–æ•°æ®ï¼Œè¿›è¡Œåæ ‡å‡†åŒ–å¤„ç†
        y_test_original = inverse_transform_power(y_test_np, scaler, power_feature_idx=0)
        prediction_original = inverse_transform_power(prediction_np, scaler, power_feature_idx=0)
        
        # Calculate performance metrics using original scale data
        mse_result = MSE(y_test_original, prediction_original)
        mape_result = MAPE(y_test_original, prediction_original)
        rmse_result = np.sqrt(mse_result)
        mae_result = np.mean(np.abs(y_test_original - prediction_original))
        
    else:
        # å¯¹äºåŸå§‹æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨
        mse_result = MSE(y_test_np, prediction_np)
        mape_result = MAPE(y_test_np, prediction_np)
        rmse_result = np.sqrt(mse_result)
        mae_result = np.mean(np.abs(y_test_np - prediction_np))
    
    print(f"âœ… è®­ç»ƒå®Œæˆ - MSE: {mse_result:.6f}, RMSE: {rmse_result:.6f}, MAE: {mae_result:.6f}, MAPE: {mape_result:.6f}")
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’, æ”¶æ•›è½®æ•°: {epoch + 1}")
    
    return {
        'dataset': dataset_name,
        'prediction_scale': prediction_scale,
        'train_ratio': train_ratio,
        'mse': mse_result,
        'rmse': rmse_result,
        'mae': mae_result,
        'mape': mape_result,
        'training_time': total_training_time,
        'converged_epochs': epoch + 1,
        'train_size': len(X_train),
        'test_size': len(X_test)
    }

def create_visualization(results_df, dataset_name, prediction_scale, save_dir):
    """
    åˆ›å»ºè®­ç»ƒæ•°æ®è§„æ¨¡å½±å“çš„å¯è§†åŒ–å›¾è¡¨
    """
    # è®¾ç½®ç»˜å›¾æ ·å¼
    plt.rcParams['font.family'] = 'serif'
    plt.rcParams['font.size'] = 12
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. RMSEæŠ˜çº¿å›¾
    ax1.plot(results_df['train_ratio'] * 100, results_df['rmse'], 'o-', linewidth=2, markersize=8, color='#1f77b4')
    ax1.set_xlabel('Training Data Percentage (%)')
    ax1.set_ylabel('RMSE')
    ax1.set_title(f'RMSE vs Training Data Scale\n{dataset_name} - {prediction_scale}')
    ax1.grid(True, alpha=0.3)
    
    # 2. MAEæŠ˜çº¿å›¾
    ax2.plot(results_df['train_ratio'] * 100, results_df['mae'], 'o-', linewidth=2, markersize=8, color='#ff7f0e')
    ax2.set_xlabel('Training Data Percentage (%)')
    ax2.set_ylabel('MAE')
    ax2.set_title(f'MAE vs Training Data Scale\n{dataset_name} - {prediction_scale}')
    ax2.grid(True, alpha=0.3)
    
    # 3. è®­ç»ƒæ—¶é—´
    ax3.plot(results_df['train_ratio'] * 100, results_df['training_time'], 'o-', linewidth=2, markersize=8, color='#2ca02c')
    ax3.set_xlabel('Training Data Percentage (%)')
    ax3.set_ylabel('Training Time (seconds)')
    ax3.set_title(f'Training Time vs Training Data Scale\n{dataset_name} - {prediction_scale}')
    ax3.grid(True, alpha=0.3)
    
    # 4. æ”¶æ•›è½®æ•°
    ax4.plot(results_df['train_ratio'] * 100, results_df['converged_epochs'], 'o-', linewidth=2, markersize=8, color='#d62728')
    ax4.set_xlabel('Training Data Percentage (%)')
    ax4.set_ylabel('Converged Epochs')
    ax4.set_title(f'Converged Epochs vs Training Data Scale\n{dataset_name} - {prediction_scale}')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = save_dir / f'training_scale_analysis_{dataset_name}_{prediction_scale}.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.savefig(str(plot_path).replace('.png', '.pdf'), bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.show()

def create_results_table(results_df, dataset_name, prediction_scale, save_dir):
    """
    åˆ›å»ºç»“æœè¡¨æ ¼
    """
    # åˆ›å»ºæ ¼å¼åŒ–çš„è¡¨æ ¼
    table_data = results_df.copy()
    table_data['Train Ratio (%)'] = (table_data['train_ratio'] * 100).astype(int)
    table_data['RMSE'] = table_data['rmse'].round(6)
    table_data['MAE'] = table_data['mae'].round(6)
    table_data['Training Time (s)'] = table_data['training_time'].round(2)
    table_data['Converged Epochs'] = table_data['converged_epochs'].astype(int)
    table_data['Train Size'] = table_data['train_size'].astype(int)
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
    display_columns = ['Train Ratio (%)', 'RMSE', 'MAE', 'Training Time (s)', 'Converged Epochs', 'Train Size']
    table_display = table_data[display_columns]
    
    # ä¿å­˜ä¸ºCSV
    table_path = save_dir / f'training_scale_results_{dataset_name}_{prediction_scale}.csv'
    table_display.to_csv(table_path, index=False)
    print(f"ğŸ“‹ ç»“æœè¡¨æ ¼å·²ä¿å­˜åˆ°: {table_path}")
    
    # æ‰“å°è¡¨æ ¼
    print(f"\nğŸ“‹ è®­ç»ƒæ•°æ®è§„æ¨¡å½±å“å®éªŒç»“æœ - {dataset_name} {prediction_scale}")
    print("=" * 80)
    print(table_display.to_string(index=False))
    
    return table_display

def main():
    parser = argparse.ArgumentParser(description='Training data scale impact experiment')
    
    # æ•°æ®é›†ç›¸å…³å‚æ•°
    parser.add_argument('--dataset', type=str, default='DSWE', choices=['fujian', 'DSWE'], 
                        help='Dataset name')
    parser.add_argument('--prediction_scale', type=str, default='24-2', 
                        help='Prediction scale (e.g., 6-0_1, 24-1, etc.)')
    parser.add_argument('--train_ratios', nargs='+', type=float, 
                        default=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
                        help='Training data ratios to test')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--gpu', type=int, default=1, help='GPU device id')
    parser.add_argument('--epochs', type=int, default=100, help='Maximum number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate')
    parser.add_argument('--l1_lambda', type=float, default=0.05, help='L1 regularization coefficient')
    parser.add_argument('--weight_decay', type=float, default=0.1, help='L2 weight decay')
    parser.add_argument('--dropout', type=float, default=0.05, help='Dropout rate')
    parser.add_argument('--patience', type=int, default=10, help='Early stopping patience')
    parser.add_argument('--split_ratio', type=float, default=0.99, help='Train/test split ratio')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--n_head', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--hidden_size', type=int, default=264, help='Hidden size')
    parser.add_argument('--factor', type=int, default=2, help='Factor for attention')
    parser.add_argument('--conv_hidden_size', type=int, default=32, help='Convolution hidden size')
    parser.add_argument('--moving_avg_window', type=int, default=3, help='Moving average window size')
    parser.add_argument('--activation', type=str, default='gelu', help='Activation function')
    parser.add_argument('--encoder_layers', type=int, default=1, help='Number of encoder layers')
    parser.add_argument('--decoder_layers', type=int, default=1, help='Number of decoder layers')
    parser.add_argument('--seq_length', type=int, default=144, help='Sequence length')
    parser.add_argument('--c_out', type=int, default=12, help='Output channels')
    parser.add_argument('--group_dec', action='store_true', default=True, help='Use group decoder')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(seed=args.seed)
    
    # è®¾ç½®è®¾å¤‡
    mp.set_start_method('spawn', force=True)
    torch.cuda.set_device(args.gpu)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    results_dir = script_dir / 'results' / 'training_scale_experiment'
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒæ•°æ®è§„æ¨¡å½±å“å®éªŒ")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"é¢„æµ‹å°ºåº¦: {args.prediction_scale}")
    print(f"è®­ç»ƒæ¯”ä¾‹: {args.train_ratios}")
    print(f"ç»“æœä¿å­˜ç›®å½•: {results_dir}")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    # å¯¹æ¯ä¸ªè®­ç»ƒæ¯”ä¾‹è¿›è¡Œå®éªŒ
    for train_ratio in args.train_ratios:
        try:
            result = train_model_with_ratio(
                dataset_name=args.dataset,
                prediction_scale=args.prediction_scale,
                train_ratio=train_ratio,
                args=args,
                device=device
            )
            
            if result is not None:
                all_results.append(result)
            else:
                print(f"âš ï¸  è®­ç»ƒæ¯”ä¾‹ {train_ratio} å¤±è´¥ï¼Œè·³è¿‡...")
                
        except Exception as e:
            print(f"ğŸ’¥ è®­ç»ƒæ¯”ä¾‹ {train_ratio} å‡ºç°é”™è¯¯: {e}")
            continue
    
    if not all_results:
        print("âŒ æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # åˆ›å»ºç»“æœDataFrame
    results_df = pd.DataFrame(all_results)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_visualization(results_df, args.dataset, args.prediction_scale, results_dir)
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    table_display = create_results_table(results_df, args.dataset, args.prediction_scale, results_dir)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    full_results_path = results_dir / f'full_results_{args.dataset}_{args.prediction_scale}.csv'
    results_df.to_csv(full_results_path, index=False)
    
    print(f"\nğŸ‰ å®éªŒå®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    print(f"ğŸ“Š å®Œæ•´ç»“æœæ–‡ä»¶: {full_results_path}")

if __name__ == '__main__':
    import sys
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 