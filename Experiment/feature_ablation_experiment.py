import os
import sys
import warnings
# Set CUDA launch blocking for debugging
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# æŠ‘åˆ¶ statsmodels çš„ FutureWarning è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning, module='statsmodels')
warnings.filterwarnings('ignore', message='verbose is deprecated since functions should not print results')

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ï¼Œå¹¶å‘ä¸Šé€€ä¸€å±‚ï¼ˆçˆ¶ç›®å½•ï¼‰
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)  # å°†çˆ¶ç›®å½•åŠ å…¥æ¨¡å—æœç´¢è·¯å¾„

import pickle
import torch
import pandas as pd
import numpy as np
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from adap_auto import adap_auto
from evaluate import MSE, MAPE
import torch.multiprocessing as mp
import time
import argparse
import random
from pathlib import Path
import matplotlib.pyplot as plt
from dynamic_data_processor import create_dynamic_data

def seed_everything(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def inverse_transform_power(data, scaler, power_feature_idx=0):
    """
    å¯¹åŠŸç‡æ•°æ®è¿›è¡Œåæ ‡å‡†åŒ–
    Args:
        data: æ ‡å‡†åŒ–åçš„æ•°æ® (numpy array)
        scaler: sklearn StandardScalerå¯¹è±¡
        power_feature_idx: åŠŸç‡ç‰¹å¾åœ¨scalerä¸­çš„ç´¢å¼•
    Returns:
        åæ ‡å‡†åŒ–åçš„åŠŸç‡æ•°æ®
    """
    mean = scaler.mean_[power_feature_idx]
    scale = scaler.scale_[power_feature_idx]
    return data * scale + mean

def get_feature_combinations(dataset_name='fujian'):
    """
    å®šä¹‰ä¸åŒçš„ç‰¹å¾ç»„åˆç”¨äºæ¶ˆèå®éªŒ
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        dict: ç‰¹å¾ç»„åˆå­—å…¸
    """
    if dataset_name.lower() == 'fujian':
        # ç¦å»ºæ•°æ®é›†çš„ç‰¹å¾ç»„åˆ
        feature_combinations = {
            'power_only': ['y'],  # ä»…å†å²åŠŸç‡
            'power_wind': ['y', 'WS10m', 'WD10m', 'WS100m', 'WD100m'],  # åŠŸç‡ + é£é€Ÿé£å‘
            'power_wind_core': ['y', 'WS10m', 'WD10m', 'WS100m', 'WD100m', 'Temp_K', 'Pres_Pa'],  # åŠŸç‡ + é£ + æ ¸å¿ƒæ°”è±¡
            'all_features': None  # å…¨é‡ç‰¹å¾ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰ç‰¹å¾ï¼‰
        }
    elif dataset_name.lower() == 'dswe':
        # DSWEæ•°æ®é›†çš„ç‰¹å¾ç»„åˆ
        feature_combinations = {
            'power_only': ['y'],  # ä»…å†å²åŠŸç‡
            'power_wind': ['y','V','D','air density'],  # åŠŸç‡ + é£é€Ÿé£å‘
            'power_wind_core': ['y','V','D','air density','humidity','I','S_a','S_b'],  # åŠŸç‡ + é£ + æ ¸å¿ƒæ°”è±¡
            'all_features': None  # å…¨é‡ç‰¹å¾
        }
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    return feature_combinations

def train_model_with_features(dataset_name, prediction_scale, feature_combination_name, feature_list, args, device):
    """
    ä½¿ç”¨æŒ‡å®šç‰¹å¾ç»„åˆè®­ç»ƒæ¨¡å‹
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        prediction_scale: é¢„æµ‹å°ºåº¦
        feature_combination_name: ç‰¹å¾ç»„åˆåç§°
        feature_list: ç‰¹å¾åˆ—è¡¨
        args: å‘½ä»¤è¡Œå‚æ•°
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        dict: åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ - æ•°æ®é›†: {dataset_name}, å°ºåº¦: {prediction_scale}")
    print(f"ç‰¹å¾ç»„åˆ: {feature_combination_name}")
    print(f"ç‰¹å¾åˆ—è¡¨: {feature_list if feature_list else 'å…¨éƒ¨ç‰¹å¾'}")
    print(f"{'='*60}")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(script_dir)
    grandparent_dir = os.path.dirname(parent_dir)
    
    # æ ¹æ®æ•°æ®é›†è®¾ç½®CSVè·¯å¾„
    if dataset_name.lower() == 'fujian':
        csv_path = os.path.join(grandparent_dir, 'data/fujian/Offshore Wind Farm Dataset3(WT1).csv')
    elif dataset_name.lower() == 'dswe':
        csv_path = os.path.join(grandparent_dir, 'data/DSWE/Offshore Wind Farm Dataset1(WT5).csv')
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # éªŒè¯CSVæ–‡ä»¶å­˜åœ¨
    if not os.path.exists(csv_path):
        print(f"âŒ é”™è¯¯: CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return None
    
    print(f"ğŸ”§ ä½¿ç”¨åŠ¨æ€æ•°æ®å¤„ç†ç³»ç»Ÿ")
    print(f"   åºåˆ—é•¿åº¦: {args.seq_length}")
    print(f"   é¢„æµ‹é•¿åº¦: {args.c_out}")
    print(f"   æ•°æ®æ¥æº: {csv_path}")

    # åˆ›å»ºåŠ¨æ€æ•°æ®
    try:
        model_data = create_dynamic_data(
            csv_path=csv_path,
            seq_length=args.seq_length,
            pred_length=args.c_out,
            split_ratio=args.split_ratio,
            standardize=True,
            feature_groups=feature_list,  # æŒ‡å®šç‰¹å¾ç»„åˆ
            use_macro_only=False,  # ä½¿ç”¨å®Œæ•´çš„å®è§‚+å¾®è§‚å›¾èåˆ
            rho=0.5,
            save_dir=None,
            verbose=True
        )
        
        # æå–æ•°æ®
        X_train = model_data['X_train'].to(device)
        y_train = model_data['y_train'].to(device)
        X_test = model_data['X_test'].to(device)
        y_test = model_data['y_test'].to(device)
        train_dict = model_data['train_edge_indices']
        test_dict = model_data['test_edge_indices']
        scaler = model_data['scaler']
        feature_names = model_data['feature_names']
        
        print(f"âœ… åŠ¨æ€æ•°æ®å¤„ç†å®Œæˆ")
        print(f"   è®­ç»ƒé›†: X{X_train.shape}, y{y_train.shape}")
        print(f"   æµ‹è¯•é›†: X{X_test.shape}, y{y_test.shape}")
        print(f"   å®é™…ç‰¹å¾æ•°é‡: {model_data['num_features']}")
        print(f"   å®é™…ç‰¹å¾åç§°: {feature_names}")
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size)

    # åˆå§‹åŒ–æ¨¡å‹
    print(f"ğŸ”§ åˆå§‹åŒ–æ¨¡å‹ - è¾“å…¥ç‰¹å¾æ•°: {model_data['num_features']}")
    print(f"   ç‰¹å¾åç§°: {feature_names}")
    auto_model = adap_auto(
        n_head=args.n_head,
        hidden_size=args.hidden_size,
        factor=args.factor,
        dropout=args.dropout,
        conv_hidden_size=args.conv_hidden_size,
        MovingAvg_window=args.moving_avg_window,
        activation=args.activation,
        encoder_layers=args.encoder_layers,
        decoder_layers=args.decoder_layers,
        c_in=model_data['num_features'],  # ä½¿ç”¨å®é™…çš„ç‰¹å¾æ•°é‡
        seq_lenth=args.seq_length,
        c_out=args.c_out,
        gruop_dec=args.group_dec
    )

    auto_model.to(device)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    loss_function = nn.MSELoss()
    optimizer = torch.optim.Adam(auto_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    l1_lambda = args.l1_lambda
    
    # Early stopping parameters
    patience = args.patience
    best_mse = float('inf')
    patience_counter = 0
    training_start_time = time.time()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(args.epochs):
        start_time = time.time()
        auto_model.train()
        total_loss = 0
        I = 0
        
        for batch in train_dataloader:
            inputs, targets = batch
            dicts = train_dict[I:I+len(inputs)]
            I += len(inputs)
            optimizer.zero_grad()
            model_output = auto_model(inputs, dicts).squeeze(-1)
            loss = loss_function(model_output, targets)
            l1_norm = sum(p.abs().sum() for p in auto_model.parameters())
            loss = loss + l1_lambda * l1_norm
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

        # Validation loss
        auto_model.eval()
        with torch.no_grad():
            prediction = auto_model(X_test, test_dict).squeeze(-1)
            
            # ä½¿ç”¨åæ ‡å‡†åŒ–åçš„æ•°æ®è®¡ç®—éªŒè¯MSE
            y_test_np = y_test.cpu().numpy()
            prediction_np = prediction.cpu().numpy()
            y_test_original = inverse_transform_power(y_test_np, scaler, power_feature_idx=0)
            prediction_original = inverse_transform_power(prediction_np, scaler, power_feature_idx=0)
            val_mse = MSE(y_test_original, prediction_original)
            
        # Check for early stopping
        if val_mse < best_mse:
            best_mse = val_mse
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch + 1}')
            break

        end_time = time.time()
        epoch_time = end_time - start_time
        print(f"Epoch {epoch + 1}/{args.epochs}, Loss: {total_loss / len(train_dataloader):.4f}, Val MSE: {val_mse:.4f}, Time: {epoch_time:.2f}s")

    training_end_time = time.time()
    total_training_time = training_end_time - training_start_time
    
    # æœ€ç»ˆè¯„ä¼°
    auto_model.eval()
    with torch.no_grad():
        prediction = auto_model(X_test, test_dict).squeeze(-1)
        
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
    print(f"   è¾“å…¥æ•°æ®å½¢çŠ¶: {X_test.shape}")
    print(f"   è¾“å…¥æ•°æ®å‰5ä¸ªæ ·æœ¬çš„å‡å€¼: {X_test[:5].mean(dim=(1,2)).cpu().numpy()}")
    print(f"   é¢„æµ‹ç»“æœå½¢çŠ¶: {prediction.shape}")
    print(f"   é¢„æµ‹ç»“æœå‰5ä¸ªæ ·æœ¬: {prediction[:5, 0].cpu().numpy()}")
    print(f"   çœŸå®å€¼å‰5ä¸ªæ ·æœ¬: {y_test[:5, 0].cpu().numpy()}")
    
    # Convert to numpy for evaluation
    y_test_np, prediction_np = y_test.cpu().numpy(), prediction.detach().cpu().numpy()
    
    # å¯¹äºæ ‡å‡†åŒ–æ•°æ®ï¼Œè¿›è¡Œåæ ‡å‡†åŒ–å¤„ç†
    y_test_original = inverse_transform_power(y_test_np, scaler, power_feature_idx=0)
    prediction_original = inverse_transform_power(prediction_np, scaler, power_feature_idx=0)
    
    # Calculate performance metrics using original scale data
    mse_result = MSE(y_test_original, prediction_original)
    mape_result = MAPE(y_test_original, prediction_original)
    rmse_result = np.sqrt(mse_result)
    mae_result = np.mean(np.abs(y_test_original - prediction_original))
    
    print(f"âœ… è®­ç»ƒå®Œæˆ - MSE: {mse_result:.6f}, RMSE: {rmse_result:.6f}, MAE: {mae_result:.6f}, MAPE: {mape_result:.6f}")
    print(f"â±ï¸  è®­ç»ƒæ—¶é—´: {total_training_time:.2f}ç§’, æ”¶æ•›è½®æ•°: {epoch + 1}")
    
    return {
        'dataset': dataset_name,
        'prediction_scale': prediction_scale,
        'feature_combination': feature_combination_name,
        'feature_list': feature_list,
        'feature_count': len(feature_names),
        'actual_features': feature_names,
        'mse': mse_result,
        'rmse': rmse_result,
        'mae': mae_result,
        'mape': mape_result,
        'training_time': total_training_time,
        'converged_epochs': epoch + 1,
        'train_size': len(X_train),
        'test_size': len(X_test)
    }

def create_results_table(results_df, dataset_name, prediction_scale, save_dir):
    """
    åˆ›å»ºç‰¹å¾æ¶ˆèå®éªŒç»“æœè¡¨æ ¼
    """
    # åˆ›å»ºæ ¼å¼åŒ–çš„è¡¨æ ¼
    table_data = results_df.copy()
    
    # æ ¼å¼åŒ–æ•°å€¼
    table_data['RMSE'] = table_data['rmse'].round(6)
    table_data['MAE'] = table_data['mae'].round(6)
    table_data['MAPE (%)'] = (table_data['mape'] * 100).round(2)
    table_data['Training Time (s)'] = table_data['training_time'].round(2)
    table_data['Feature Count'] = table_data['feature_count'].astype(int)
    
    # è®¡ç®—ç›¸å¯¹äºå…¨ç‰¹å¾çš„æ€§èƒ½å˜åŒ–
    if 'all_features' in table_data['feature_combination'].values:
        baseline_rmse = table_data[table_data['feature_combination'] == 'all_features']['rmse'].iloc[0]
        baseline_mae = table_data[table_data['feature_combination'] == 'all_features']['mae'].iloc[0]
        
        table_data['RMSE Change (%)'] = ((table_data['rmse'] - baseline_rmse) / baseline_rmse * 100).round(2)
        table_data['MAE Change (%)'] = ((table_data['mae'] - baseline_mae) / baseline_mae * 100).round(2)
    else:
        table_data['RMSE Change (%)'] = 0.0
        table_data['MAE Change (%)'] = 0.0
    
    # é€‰æ‹©è¦æ˜¾ç¤ºçš„åˆ—
    display_columns = ['feature_combination', 'Feature Count', 'RMSE', 'MAE', 'MAPE (%)', 
                      'RMSE Change (%)', 'MAE Change (%)', 'Training Time (s)']
    table_display = table_data[display_columns]
    
    # é‡å‘½ååˆ—ä»¥ä¾¿æ˜¾ç¤º
    table_display = table_display.rename(columns={
        'feature_combination': 'Feature Combination'
    })
    
    # ä¿å­˜ä¸ºCSV
    table_path = save_dir / f'feature_ablation_results_{dataset_name}_{prediction_scale}.csv'
    table_display.to_csv(table_path, index=False)
    print(f"ğŸ“‹ ç»“æœè¡¨æ ¼å·²ä¿å­˜åˆ°: {table_path}")
    
    # æ‰“å°è¡¨æ ¼
    print(f"\nğŸ“‹ ç‰¹å¾æ¶ˆèå®éªŒç»“æœ - {dataset_name} {prediction_scale}")
    print("=" * 120)
    print(table_display.to_string(index=False))
    
    return table_display

def create_visualization(results_df, dataset_name, prediction_scale, save_dir):
    """
    åˆ›å»ºç‰¹å¾æ¶ˆèå®éªŒçš„å¯è§†åŒ–å›¾è¡¨
    """
    # è®¾ç½®ç»˜å›¾æ ·å¼
    plt.rcParams['font.family'] = 'serif'
    plt.rcParams['font.size'] = 12
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # å‡†å¤‡æ•°æ®
    feature_names = results_df['feature_combination'].tolist()
    feature_counts = results_df['feature_count'].tolist()
    rmse_values = results_df['rmse'].tolist()
    mae_values = results_df['mae'].tolist()
    training_times = results_df['training_time'].tolist()
    
    # 1. RMSE vs ç‰¹å¾æ•°é‡
    ax1.bar(range(len(feature_names)), rmse_values, color='#1f77b4', alpha=0.7)
    ax1.set_xlabel('Feature Combinations')
    ax1.set_ylabel('RMSE')
    ax1.set_title(f'RMSE by Feature Combination\n{dataset_name} - {prediction_scale}')
    ax1.set_xticks(range(len(feature_names)))
    ax1.set_xticklabels(feature_names, rotation=45, ha='right')
    ax1.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(rmse_values):
        ax1.text(i, v + max(rmse_values) * 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)
    
    # 2. MAE vs ç‰¹å¾æ•°é‡
    ax2.bar(range(len(feature_names)), mae_values, color='#ff7f0e', alpha=0.7)
    ax2.set_xlabel('Feature Combinations')
    ax2.set_ylabel('MAE')
    ax2.set_title(f'MAE by Feature Combination\n{dataset_name} - {prediction_scale}')
    ax2.set_xticks(range(len(feature_names)))
    ax2.set_xticklabels(feature_names, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(mae_values):
        ax2.text(i, v + max(mae_values) * 0.01, f'{v:.4f}', ha='center', va='bottom', fontsize=10)
    
    # 3. ç‰¹å¾æ•°é‡ vs æ€§èƒ½
    ax3.plot(feature_counts, rmse_values, 'o-', linewidth=2, markersize=8, color='#1f77b4', label='RMSE')
    ax3_twin = ax3.twinx()
    ax3_twin.plot(feature_counts, mae_values, 's-', linewidth=2, markersize=8, color='#ff7f0e', label='MAE')
    
    ax3.set_xlabel('Number of Features')
    ax3.set_ylabel('RMSE', color='#1f77b4')
    ax3_twin.set_ylabel('MAE', color='#ff7f0e')
    ax3.set_title(f'Performance vs Feature Count\n{dataset_name} - {prediction_scale}')
    ax3.grid(True, alpha=0.3)
    
    # 4. è®­ç»ƒæ—¶é—´
    ax4.bar(range(len(feature_names)), training_times, color='#2ca02c', alpha=0.7)
    ax4.set_xlabel('Feature Combinations')
    ax4.set_ylabel('Training Time (seconds)')
    ax4.set_title(f'Training Time by Feature Combination\n{dataset_name} - {prediction_scale}')
    ax4.set_xticks(range(len(feature_names)))
    ax4.set_xticklabels(feature_names, rotation=45, ha='right')
    ax4.grid(True, alpha=0.3)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(training_times):
        ax4.text(i, v + max(training_times) * 0.01, f'{v:.1f}s', ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = save_dir / f'feature_ablation_analysis_{dataset_name}_{prediction_scale}.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.savefig(str(plot_path).replace('.png', '.pdf'), bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    plt.show()

def main():
    parser = argparse.ArgumentParser(description='Feature ablation experiment for input features contribution analysis')
    
    # æ•°æ®é›†ç›¸å…³å‚æ•°
    parser.add_argument('--dataset', type=str, default='fujian', choices=['fujian', 'DSWE'], 
                        help='Dataset name')
    parser.add_argument('--prediction_scale', type=str, default='6-0_1', 
                        help='Prediction scale (e.g., 6-0_1, 24-1, etc.)')
    parser.add_argument('--feature_combinations', nargs='+', type=str,
                        default=['power_only', 'power_wind', 'power_wind_core', 'all_features'],
                        help='Feature combinations to test')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--gpu', type=int, default=1, help='GPU device id')
    parser.add_argument('--epochs', type=int, default=30, help='Maximum number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate')
    parser.add_argument('--l1_lambda', type=float, default=0.15, help='L1 regularization coefficient')
    parser.add_argument('--weight_decay', type=float, default=0.15, help='L2 weight decay')
    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate')
    parser.add_argument('--patience', type=int, default=2, help='Early stopping patience')
    parser.add_argument('--split_ratio', type=float, default=0.99, help='Train/test split ratio')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    # æ¨¡å‹æ¶æ„å‚æ•°
    parser.add_argument('--n_head', type=int, default=8, help='Number of attention heads')
    parser.add_argument('--hidden_size', type=int, default=264, help='Hidden size')
    parser.add_argument('--factor', type=int, default=2, help='Factor for attention')
    parser.add_argument('--conv_hidden_size', type=int, default=32, help='Convolution hidden size')
    parser.add_argument('--moving_avg_window', type=int, default=3, help='Moving average window size')
    parser.add_argument('--activation', type=str, default='gelu', help='Activation function')
    parser.add_argument('--encoder_layers', type=int, default=1, help='Number of encoder layers')
    parser.add_argument('--decoder_layers', type=int, default=1, help='Number of decoder layers')
    parser.add_argument('--seq_length', type=int, default=36, help='Sequence length')
    parser.add_argument('--c_out', type=int, default=6, help='Output channels')
    parser.add_argument('--group_dec', action='store_true', default=True, help='Use group decoder')
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    seed_everything(seed=args.seed)
    
    # è®¾ç½®è®¾å¤‡
    mp.set_start_method('spawn', force=True)
    torch.cuda.set_device(args.gpu)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºç»“æœä¿å­˜ç›®å½•
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    results_dir = script_dir / 'results' / 'feature_ablation_experiment'
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ¯ å¼€å§‹ç‰¹å¾æ¶ˆèå®éªŒ")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"é¢„æµ‹å°ºåº¦: {args.prediction_scale}")
    print(f"ç‰¹å¾ç»„åˆ: {args.feature_combinations}")
    print(f"ç»“æœä¿å­˜ç›®å½•: {results_dir}")
    
    # è·å–ç‰¹å¾ç»„åˆå®šä¹‰
    feature_combinations = get_feature_combinations(args.dataset)
    
    # éªŒè¯ç‰¹å¾ç»„åˆ
    invalid_combinations = [combo for combo in args.feature_combinations if combo not in feature_combinations]
    if invalid_combinations:
        print(f"âŒ æ— æ•ˆçš„ç‰¹å¾ç»„åˆ: {invalid_combinations}")
        print(f"å¯ç”¨çš„ç‰¹å¾ç»„åˆ: {list(feature_combinations.keys())}")
        return
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    # å¯¹æ¯ä¸ªç‰¹å¾ç»„åˆè¿›è¡Œå®éªŒ
    for combo_name in args.feature_combinations:
        feature_list = feature_combinations[combo_name]
        
        try:
            result = train_model_with_features(
                dataset_name=args.dataset,
                prediction_scale=args.prediction_scale,
                feature_combination_name=combo_name,
                feature_list=feature_list,
                args=args,
                device=device
            )
            
            if result is not None:
                all_results.append(result)
            else:
                print(f"âš ï¸  ç‰¹å¾ç»„åˆ {combo_name} å®éªŒå¤±è´¥ï¼Œè·³è¿‡...")
                
        except Exception as e:
            print(f"ğŸ’¥ ç‰¹å¾ç»„åˆ {combo_name} å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if not all_results:
        print("âŒ æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†ï¼Œé€€å‡ºç¨‹åº")
        return
    
    # åˆ›å»ºç»“æœDataFrame
    results_df = pd.DataFrame(all_results)
    
    # æŒ‰ç‰¹å¾ç»„åˆé¡ºåºæ’åº
    combo_order = ['power_only', 'power_wind', 'power_wind_core', 'all_features']
    results_df['combo_order'] = results_df['feature_combination'].map({combo: i for i, combo in enumerate(combo_order)})
    results_df = results_df.sort_values('combo_order').reset_index(drop=True)
    results_df = results_df.drop('combo_order', axis=1)
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    table_display = create_results_table(results_df, args.dataset, args.prediction_scale, results_dir)
    
    # åˆ›å»ºå¯è§†åŒ–
    create_visualization(results_df, args.dataset, args.prediction_scale, results_dir)
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    full_results_path = results_dir / f'full_results_{args.dataset}_{args.prediction_scale}.csv'
    results_df.to_csv(full_results_path, index=False)
    
    # ç”Ÿæˆåˆ†ææ€»ç»“
    print(f"\nğŸ‰ ç‰¹å¾æ¶ˆèå®éªŒå®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {results_dir}")
    print(f"ğŸ“Š å®Œæ•´ç»“æœæ–‡ä»¶: {full_results_path}")
    
    # è¾“å‡ºå…³é”®å‘ç°
    print(f"\nğŸ“ˆ å…³é”®å‘ç°:")
    best_rmse_idx = results_df['rmse'].idxmin()
    best_mae_idx = results_df['mae'].idxmin()
    fastest_idx = results_df['training_time'].idxmin()
    
    print(f"   æœ€ä½³RMSE: {results_df.loc[best_rmse_idx, 'feature_combination']} ({results_df.loc[best_rmse_idx, 'rmse']:.6f})")
    print(f"   æœ€ä½³MAE: {results_df.loc[best_mae_idx, 'feature_combination']} ({results_df.loc[best_mae_idx, 'mae']:.6f})")
    print(f"   æœ€å¿«è®­ç»ƒ: {results_df.loc[fastest_idx, 'feature_combination']} ({results_df.loc[fastest_idx, 'training_time']:.2f}s)")
    
    if len(results_df) > 1:
        rmse_improvement = (results_df['rmse'].max() - results_df['rmse'].min()) / results_df['rmse'].max() * 100
        mae_improvement = (results_df['mae'].max() - results_df['mae'].min()) / results_df['mae'].max() * 100
        print(f"   RMSEæ”¹è¿›å¹…åº¦: {rmse_improvement:.2f}%")
        print(f"   MAEæ”¹è¿›å¹…åº¦: {mae_improvement:.2f}%")

if __name__ == '__main__':
    import sys
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 