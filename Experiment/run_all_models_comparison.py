#!/usr/bin/env python3
"""
æ‰¹é‡è¿è¡Œæ‰€æœ‰æ¨¡å‹çš„é²æ£’æ€§å¯¹æ¯”å®éªŒ
åŒ…æ‹¬ adap_auto å’Œå››ä¸ªåŸºçº¿æ¨¡å‹ï¼šDLinear, iTransformer, FEDformer, NBEATSx
"""

import subprocess
import sys
import argparse
import os
import time
from pathlib import Path
import pickle
import pandas as pd
import numpy as np

def run_adap_auto_experiment(dataset, prediction_scale, args):
    """
    è¿è¡Œadap_autoæ¨¡å‹å®éªŒ
    """
    print("ğŸ† è¿è¡Œadap_autoæ¨¡å‹å®éªŒ...")
    
    cmd = [
        sys.executable, 'robustness_analysis_experiment.py',
        '--dataset', dataset,
        '--prediction_scale', prediction_scale,
        '--gpu', str(args.gpu),
        '--epochs', str(args.epochs),
        '--batch_size', str(args.batch_size),
        '--lr', str(args.lr),
        '--noise_levels'] + [str(level) for level in args.noise_levels] + [
        '--missing_ratio', str(args.missing_ratio),
        '--seasonal_mode', args.seasonal_mode,
        '--seed', str(args.seed)
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True)
        print("âœ… adap_autoå®éªŒå®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ adap_autoå®éªŒå¤±è´¥: {e}")
        return False

def run_baseline_model_experiment(model_name, dataset, prediction_scale, args):
    """
    è¿è¡Œå•ä¸ªåŸºçº¿æ¨¡å‹å®éªŒ
    """
    print(f"ğŸ”¬ è¿è¡Œ {model_name} æ¨¡å‹å®éªŒ...")
    
    cmd = [
        sys.executable, 'baseline_robustness_adapter.py',
        '--dataset', dataset,
        '--prediction_scale', prediction_scale,
        '--model', model_name,
        '--gpu', str(args.gpu),
        '--epochs', str(args.epochs),
        '--batch_size', str(args.batch_size),
        '--lr', str(args.lr),
        '--noise_levels'] + [str(level) for level in args.noise_levels] + [
        '--missing_ratio', str(args.missing_ratio),
        '--seed', str(args.seed)
    ]
    
    print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, check=True)
        print(f"âœ… {model_name} å®éªŒå®Œæˆ")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âŒ {model_name} å®éªŒå¤±è´¥: {e}")
        return False

def collect_all_results(dataset, prediction_scale):
    """
    æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„å®éªŒç»“æœ
    """
    print("ğŸ“Š æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ...")
    
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    all_results = {}
    
    # æ”¶é›†adap_autoç»“æœ
    adap_auto_file = script_dir / 'results' / 'robustness_analysis_experiment' / f'robustness_results_{dataset}_{prediction_scale}.pkl'
    if adap_auto_file.exists():
        with open(adap_auto_file, 'rb') as f:
            adap_auto_results = pickle.load(f)
        all_results['adap_auto'] = adap_auto_results
        print("âœ… å·²æ”¶é›†adap_autoç»“æœ")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°adap_autoç»“æœ")
    
    # æ”¶é›†åŸºçº¿æ¨¡å‹ç»“æœ
    baseline_models = ['DLinear', 'iTransformer', 'FEDformer', 'NBEATSx']
    baseline_dir = script_dir / 'results' / 'baseline_robustness'
    
    for model_name in baseline_models:
        result_file = baseline_dir / f'robustness_results_{dataset}_{prediction_scale}_{model_name}.pkl'
        if result_file.exists():
            with open(result_file, 'rb') as f:
                model_results = pickle.load(f)
            all_results[model_name] = model_results
            print(f"âœ… å·²æ”¶é›†{model_name}ç»“æœ")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°{model_name}ç»“æœ")
    
    return all_results

def create_comprehensive_analysis(all_results, dataset, prediction_scale):
    """
    åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š
    """
    print("ğŸ“ˆ åˆ›å»ºç»¼åˆåˆ†ææŠ¥å‘Š...")
    
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    analysis_dir = script_dir / 'results' / 'comprehensive_analysis'
    analysis_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. åŸºçº¿æ€§èƒ½å¯¹æ¯”
    baseline_comparison = []
    
    for model_name, results in all_results.items():
        if 'baseline' in results:
            baseline_comparison.append({
                'Model': model_name,
                'MAE': results['baseline']['MAE'],
                'RMSE': results['baseline']['RMSE'],
                'MAPE': results['baseline']['MAPE'],
                'Training_Time': results.get('training_time', 0)
            })
        elif model_name == 'adap_auto' and results.get('seasonal_mode') == 'independent':
            # å¤„ç†ç‹¬ç«‹å­£èŠ‚æ¨¡å¼
            seasonal_metrics = list(results['seasonal_performance'].values())
            if seasonal_metrics:
                avg_mae = np.mean([m['MAE'] for m in seasonal_metrics])
                avg_rmse = np.mean([m['RMSE'] for m in seasonal_metrics])
                avg_mape = np.mean([m['MAPE'] for m in seasonal_metrics])
                baseline_comparison.append({
                    'Model': f'{model_name} (Independent)',
                    'MAE': avg_mae,
                    'RMSE': avg_rmse,
                    'MAPE': avg_mape,
                    'Training_Time': results.get('training_time', 0)
                })
    
    # ä¿å­˜åŸºçº¿æ€§èƒ½å¯¹æ¯”
    if baseline_comparison:
        baseline_df = pd.DataFrame(baseline_comparison)
        baseline_df = baseline_df.sort_values('MAE')  # æŒ‰MAEæ’åº
        baseline_file = analysis_dir / f'baseline_performance_{dataset}_{prediction_scale}.csv'
        baseline_df.to_csv(baseline_file, index=False)
        print(f"ğŸ“‹ åŸºçº¿æ€§èƒ½å¯¹æ¯”å·²ä¿å­˜åˆ°: {baseline_file}")
    
    # 2. é²æ£’æ€§å¯¹æ¯”åˆ†æ
    robustness_data = []
    
    for model_name, results in all_results.items():
        # å™ªå£°é²æ£’æ€§
        if 'noise_robustness' in results:
            for noise_level, metrics in results['noise_robustness'].items():
                robustness_data.append({
                    'Model': model_name,
                    'Test_Type': 'Noise',
                    'Test_Level': noise_level,
                    'MAE': metrics['MAE'],
                    'RMSE': metrics['RMSE'],
                    'MAPE': metrics['MAPE'],
                    'MAE_Degradation': metrics['MAE_degradation'],
                    'RMSE_Degradation': metrics['RMSE_degradation'],
                    'MAPE_Degradation': metrics['MAPE_degradation']
                })
        
        # ç¼ºå¤±é²æ£’æ€§
        if 'missing_robustness' in results:
            for missing_level, metrics in results['missing_robustness'].items():
                robustness_data.append({
                    'Model': model_name,
                    'Test_Type': 'Missing',
                    'Test_Level': missing_level,
                    'MAE': metrics['MAE'],
                    'RMSE': metrics['RMSE'],
                    'MAPE': metrics['MAPE'],
                    'MAE_Degradation': metrics['MAE_degradation'],
                    'RMSE_Degradation': metrics['RMSE_degradation'],
                    'MAPE_Degradation': metrics['MAPE_degradation']
                })
    
    # ä¿å­˜é²æ£’æ€§å¯¹æ¯”
    if robustness_data:
        robustness_df = pd.DataFrame(robustness_data)
        robustness_file = analysis_dir / f'robustness_analysis_{dataset}_{prediction_scale}.csv'
        robustness_df.to_csv(robustness_file, index=False)
        print(f"ğŸ”Š é²æ£’æ€§åˆ†æå·²ä¿å­˜åˆ°: {robustness_file}")
    
    # 3. å­£èŠ‚æ€§æ€§èƒ½å¯¹æ¯”
    seasonal_data = []
    
    for model_name, results in all_results.items():
        if 'seasonal_performance' in results:
            for season, metrics in results['seasonal_performance'].items():
                seasonal_data.append({
                    'Model': model_name,
                    'Season': season,
                    'MAE': metrics['MAE'],
                    'RMSE': metrics['RMSE'],
                    'MAPE': metrics['MAPE'],
                    'Sample_Count': metrics.get('sample_count', metrics.get('test_samples', 0))
                })
    
    # ä¿å­˜å­£èŠ‚æ€§å¯¹æ¯”
    if seasonal_data:
        seasonal_df = pd.DataFrame(seasonal_data)
        seasonal_file = analysis_dir / f'seasonal_performance_{dataset}_{prediction_scale}.csv'
        seasonal_df.to_csv(seasonal_file, index=False)
        print(f"ğŸŒ¸ å­£èŠ‚æ€§æ€§èƒ½å¯¹æ¯”å·²ä¿å­˜åˆ°: {seasonal_file}")
    
    return baseline_comparison, robustness_data, seasonal_data

def print_final_summary(baseline_comparison, robustness_data, seasonal_data):
    """
    æ‰“å°æœ€ç»ˆå®éªŒæ€»ç»“
    """
    print(f"\n{'='*80}")
    print("ğŸ† å®Œæ•´å®éªŒç»“æœæ€»ç»“")
    print(f"{'='*80}")
    
    # 1. åŸºçº¿æ€§èƒ½æ’å
    if baseline_comparison:
        print(f"\nğŸ… åŸºçº¿æ€§èƒ½æ’å (æŒ‰MAEæ’åº):")
        for i, model in enumerate(baseline_comparison, 1):
            training_time = model.get('Training_Time', 0)
            time_str = f", è®­ç»ƒæ—¶é—´={training_time:.2f}s" if training_time > 0 else ""
            print(f"  {i}. {model['Model']}: MAE={model['MAE']:.4f}, RMSE={model['RMSE']:.4f}, MAPE={model['MAPE']:.2f}%{time_str}")
        
        # æœ€ä½³æ¨¡å‹
        best_model = baseline_comparison[0]
        print(f"\nğŸ¥‡ æœ€ä½³æ¨¡å‹: {best_model['Model']}")
        print(f"   æ€§èƒ½: MAE={best_model['MAE']:.4f}, RMSE={best_model['RMSE']:.4f}, MAPE={best_model['MAPE']:.2f}%")
    
    # 2. é²æ£’æ€§åˆ†ææ‘˜è¦
    if robustness_data:
        print(f"\nğŸ”Š å™ªå£°é²æ£’æ€§æ’å (æŒ‰å¹³å‡MAEé€€åŒ–æ’åº):")
        noise_data = [r for r in robustness_data if r['Test_Type'] == 'Noise']
        if noise_data:
            # è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„å¹³å‡å™ªå£°é€€åŒ–
            model_noise_degradation = {}
            for item in noise_data:
                model = item['Model']
                if model not in model_noise_degradation:
                    model_noise_degradation[model] = []
                model_noise_degradation[model].append(item['MAE_Degradation'])
            
            # è®¡ç®—å¹³å‡å€¼å¹¶æ’åº
            avg_degradation = [(model, np.mean(degradations)) for model, degradations in model_noise_degradation.items()]
            avg_degradation.sort(key=lambda x: x[1])
            
            for i, (model, avg_deg) in enumerate(avg_degradation, 1):
                print(f"  {i}. {model}: å¹³å‡MAEé€€åŒ–={avg_deg:.2f}%")
        
        print(f"\nğŸ•³ï¸ ç¼ºå¤±é²æ£’æ€§æ’å (æŒ‰MAEé€€åŒ–æ’åº):")
        missing_data = [r for r in robustness_data if r['Test_Type'] == 'Missing']
        if missing_data:
            missing_sorted = sorted(missing_data, key=lambda x: x['MAE_Degradation'])
            for i, item in enumerate(missing_sorted, 1):
                print(f"  {i}. {item['Model']}: MAEé€€åŒ–={item['MAE_Degradation']:.2f}%")
    
    # 3. å­£èŠ‚æ€§æ€§èƒ½æ‘˜è¦
    if seasonal_data:
        print(f"\nğŸŒ¸ å­£èŠ‚æ€§æ€§èƒ½åˆ†æ:")
        # è®¡ç®—æ¯ä¸ªå­£èŠ‚çš„å¹³å‡æ€§èƒ½
        season_avg = {}
        for item in seasonal_data:
            season = item['Season']
            if season not in season_avg:
                season_avg[season] = []
            season_avg[season].append(item['MAE'])
        
        for season, maes in season_avg.items():
            avg_mae = np.mean(maes)
            print(f"  {season}: å¹³å‡MAE={avg_mae:.4f}")

def main():
    parser = argparse.ArgumentParser(description='Run comprehensive robustness comparison for all models')
    
    # æ•°æ®é›†ç›¸å…³å‚æ•°
    parser.add_argument('--dataset', type=str, default='fujian', choices=['fujian', 'DSWE'], 
                        help='Dataset name')
    parser.add_argument('--prediction_scale', type=str, default='6-0_1', 
                        help='Prediction scale (e.g., 6-0_1, 24-1, etc.)')
    
    # è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument('--gpu', type=int, default=1, help='GPU device id')
    parser.add_argument('--epochs', type=int, default=10, help='Maximum number of training epochs')
    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training')
    parser.add_argument('--lr', type=float, default=0.0002, help='Learning rate')
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    # é²æ£’æ€§æµ‹è¯•å‚æ•°
    parser.add_argument('--noise_levels', nargs='+', type=float, default=[0.05, 0.1],
                        help='Noise levels for robustness testing')
    parser.add_argument('--missing_ratio', type=float, default=0.05,
                        help='Missing data ratio for robustness testing')
    
    # å®éªŒæ§åˆ¶å‚æ•°
    parser.add_argument('--seasonal_mode', type=str, default='test_split', 
                        choices=['test_split', 'independent'],
                        help='Seasonal analysis mode for adap_auto')
    parser.add_argument('--baseline_models', nargs='+', type=str, 
                        default=['DLinear', 'iTransformer', 'FEDformer', 'NBEATSx'],
                        choices=['DLinear', 'iTransformer', 'FEDformer', 'NBEATSx'],
                        help='Baseline models to compare')
    parser.add_argument('--skip_adap_auto', action='store_true',
                        help='Skip adap_auto experiment')
    parser.add_argument('--skip_baselines', action='store_true',
                        help='Skip baseline experiments')
    
    args = parser.parse_args()
    
    print(f"\nğŸ¯ å¼€å§‹å®Œæ•´é²æ£’æ€§å¯¹æ¯”å®éªŒ")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"é¢„æµ‹å°ºåº¦: {args.prediction_scale}")
    print(f"åŸºçº¿æ¨¡å‹: {args.baseline_models}")
    print(f"å­£èŠ‚æ¨¡å¼: {args.seasonal_mode}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    
    # åˆ‡æ¢åˆ°è„šæœ¬ç›®å½•
    script_dir = Path(os.path.dirname(os.path.abspath(__file__)))
    os.chdir(script_dir)
    
    successful_experiments = []
    failed_experiments = []
    
    start_time = time.time()
    
    # 1. è¿è¡Œadap_autoå®éªŒ
    if not args.skip_adap_auto:
        print(f"\n{'='*60}")
        print("ğŸ† æ­¥éª¤1: è¿è¡Œadap_autoæ¨¡å‹å®éªŒ")
        print(f"{'='*60}")
        
        if run_adap_auto_experiment(args.dataset, args.prediction_scale, args):
            successful_experiments.append('adap_auto')
        else:
            failed_experiments.append('adap_auto')
    
    # 2. è¿è¡ŒåŸºçº¿æ¨¡å‹å®éªŒ
    if not args.skip_baselines:
        print(f"\n{'='*60}")
        print("ğŸ”¬ æ­¥éª¤2: è¿è¡ŒåŸºçº¿æ¨¡å‹å®éªŒ")
        print(f"{'='*60}")
        
        for i, model_name in enumerate(args.baseline_models, 1):
            print(f"\n--- {i}/{len(args.baseline_models)}: {model_name} ---")
            
            if run_baseline_model_experiment(model_name, args.dataset, args.prediction_scale, args):
                successful_experiments.append(model_name)
            else:
                failed_experiments.append(model_name)
    
    # 3. æ”¶é›†å’Œåˆ†æç»“æœ
    if successful_experiments:
        print(f"\n{'='*60}")
        print("ğŸ“Š æ­¥éª¤3: æ”¶é›†å’Œåˆ†æç»“æœ")
        print(f"{'='*60}")
        
        all_results = collect_all_results(args.dataset, args.prediction_scale)
        
        if all_results:
            baseline_comparison, robustness_data, seasonal_data = create_comprehensive_analysis(
                all_results, args.dataset, args.prediction_scale
            )
            
            print_final_summary(baseline_comparison, robustness_data, seasonal_data)
        else:
            print("âŒ æœªèƒ½æ”¶é›†åˆ°ä»»ä½•ç»“æœ")
    
    # 4. å®éªŒæ€»ç»“
    total_time = time.time() - start_time
    
    print(f"\n{'='*80}")
    print("ğŸ“Š å®éªŒå®Œæˆæ€»ç»“")
    print(f"{'='*80}")
    print(f"æ€»ç”¨æ—¶: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
    print(f"æˆåŠŸå®éªŒ: {len(successful_experiments)} ä¸ª")
    print(f"å¤±è´¥å®éªŒ: {len(failed_experiments)} ä¸ª")
    
    if successful_experiments:
        print(f"âœ… æˆåŠŸçš„å®éªŒ: {', '.join(successful_experiments)}")
    
    if failed_experiments:
        print(f"âŒ å¤±è´¥çš„å®éªŒ: {', '.join(failed_experiments)}")
    
    # æ˜¾ç¤ºç»“æœæ–‡ä»¶ä½ç½®
    results_base = script_dir / 'results'
    print(f"\nğŸ“ ç»“æœæ–‡ä»¶ä½ç½®:")
    print(f"  - adap_autoç»“æœ: {results_base / 'robustness_analysis_experiment'}")
    print(f"  - åŸºçº¿æ¨¡å‹ç»“æœ: {results_base / 'baseline_robustness'}")
    print(f"  - ç»¼åˆåˆ†æç»“æœ: {results_base / 'comprehensive_analysis'}")
    
    if len(successful_experiments) > 0:
        print(f"\nâœ… å®éªŒæˆåŠŸå®Œæˆï¼")
    else:
        print(f"\nâŒ æ‰€æœ‰å®éªŒéƒ½å¤±è´¥äº†")

if __name__ == '__main__':
    main() 