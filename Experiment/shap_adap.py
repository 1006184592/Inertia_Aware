import pickle
import torch
import pandas as pd
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
import shap
import time
import warnings
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from pathlib import Path
import os
import random

# --- è‡ªå®šä¹‰æ¨¡å‹å’Œè¯„ä¼°å‡½æ•° ---
# ç¡®ä¿ä½ çš„æ¨¡å—å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
from adap_auto import adap_auto
from evaluate import MSE, MAPE

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

# ... (ä½ ä»£ç ä¸­çš„æ‰€æœ‰è¾…åŠ©å‡½æ•° seed_everything, is_standardized_data ç­‰ä¿æŒä¸å˜) ...
def seed_everything(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡å¤"""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def is_standardized_data(train_data_path, val_data_path):
    """æ£€æµ‹æ•°æ®æ˜¯å¦ä¸ºæ ‡å‡†åŒ–æ•°æ®"""
    train_filename = os.path.basename(train_data_path)
    val_filename = os.path.basename(val_data_path)
    if train_filename.startswith('std') or val_filename.startswith('std'):
        return True
    if 'std' in train_filename.lower() or 'std' in val_filename.lower():
        return True
    standardized_keywords = ['standard', 'standardized', 'norm', 'normalized']
    for keyword in standardized_keywords:
        if keyword in train_filename.lower() or keyword in val_filename.lower():
            return True
    return False

def create_full_graph_dict(data_length, num_nodes):
    """åŠ¨æ€åˆ›å»ºå®Œå…¨å›¾tensoræ ¼å¼"""
    edge_list = []
    for i in range(num_nodes):
        for j in range(num_nodes):
            if i != j:
                edge_list.append([i, j])
    if edge_list:
        edge_index = torch.tensor(edge_list, dtype=torch.long).T
    else:
        edge_index = torch.empty((2, 0), dtype=torch.long)
    full_graph_list = [edge_index.clone() for _ in range(data_length)]
    return full_graph_list

# ==============================================================================
# 1. å…¨å±€é…ç½® (Global Configuration) - ä¿æŒä¸å˜
# ==============================================================================
# ... (ä½ çš„ DATASET_CONFIGS å’Œ CONFIG ä¿æŒä¸å˜) ...
DATASET_CONFIGS = {
    "fujian": {
        "csv_file": "Offshore Wind Farm Dataset3(WT1).csv",
        "hidden_size": 264,
        "feature_map": {
            'Pres_Pa': 'Pressure',
            'RH_pct': 'Humidity (RH)',
            'Cloud': 'Cloud Cover',
            'WS10m': 'WS (10m)',
            'WD10m': 'WD (10m)',
            'Temp_K': 'Temperature',
            'Rad_Jm2': 'Solar Rad.',
            'Precip_m': 'Precipitation',
            'WS100m': 'WS (100m)',
            'WD100m': 'WD (100m)',  
            'y': 'Past Power' 
        },
        "base_model_params": {
            "n_head": 8, "factor": 2, "dropout": 0.5, "conv_hidden_size": 32,
            "MovingAvg_window": 3, "activation": "gelu", "encoder_layers": 1,
            "decoder_layers": 1, "gruop_dec": True
        },
        "scales": {
            "1": {"dataset_name": "1", "seq_lenth": 6, "c_out": 1, "c_in": 11, "prediction_type": "å•æ­¥", "display_name": "1å¤©å•æ­¥"},
            "6-0_1": {"dataset_name": "6-0_1", "seq_lenth": 36, "c_out": 1, "c_in": 11, "prediction_type": "å•æ­¥", "display_name": "6å¤©å•æ­¥"},
            "6-1": {"dataset_name": "6-1", "seq_lenth": 36, "c_out": 6, "c_in": 11, "prediction_type": "å¤šæ­¥", "display_name": "6å¤©å¤šæ­¥"},
            "24-1": {"dataset_name": "24-1", "seq_lenth": 36, "c_out": 6, "c_in": 11, "prediction_type": "å¤šæ­¥", "display_name": "24å¤©å¤šæ­¥"}
        }
    },
    "DSWE": {
        "csv_file": "Offshore Wind Farm Dataset1(WT5).csv",
        "hidden_size": 256,
        "feature_map": {
            'V': 'Wind Speed',
            'D': 'Wind Direction',
            'rho': 'Air Density',
            'H': 'Humidity',
            'I': 'Turbulence Int.', 
            'S_a': 'Wind Shear (Above)',
            'S_b': 'Wind Shear (Below)',
            'y': 'Past Power' 
        },
        "base_model_params": {
            "n_head": 8, "factor": 2, "dropout": 0.5, "conv_hidden_size": 32,
            "MovingAvg_window": 3, "activation": "gelu", "encoder_layers": 1,
            "decoder_layers": 1, "gruop_dec": True
        },
        "scales": {
            "1": {"dataset_name": "1", "seq_lenth": 6, "c_out": 1, "c_in": 8, "prediction_type": "å•æ­¥", "display_name": "1å°æ—¶å•æ­¥"},
            "6-0_1": {"dataset_name": "6-0_1", "seq_lenth": 36, "c_out": 1, "c_in": 8, "prediction_type": "å•æ­¥", "display_name": "6å°æ—¶å•æ­¥"},
            "6-1": {"dataset_name": "6-1", "seq_lenth": 36, "c_out": 6, "c_in": 8, "prediction_type": "å¤šæ­¥", "display_name": "6å°æ—¶å¤šæ­¥"},
            "24-1": {"dataset_name": "24-1", "seq_lenth": 144, "c_out": 6, "c_in": 8, "prediction_type": "å¤šæ­¥", "display_name": "24å°æ—¶å¤šæ­¥"}
        }
    }
}
CONFIG = {
    "target_dataset": "DSWE",
    "target_scales": ["1"],
    "device": torch.device("cuda:1" if torch.cuda.is_available() else "cpu"),
    "split_ratio": 0.99,
    "background_samples": 50,
    "test_samples": 200,
    "seed": 42,
    "script_dir": os.path.dirname(os.path.abspath(__file__)),
    "output_dir": Path("shap_analysis_results"),
    "use_adaptive_graph": True,
    "visualization": {"font_path": None, "font_size": 12, "dpi": 300}
}
# ... (get_current_config, å­—ä½“è®¾ç½®ç­‰ä¿æŒä¸å˜) ...
def get_current_config(dataset_name, scale_name):
    if dataset_name not in DATASET_CONFIGS: raise ValueError(f"æœªæ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    dataset_config = DATASET_CONFIGS[dataset_name]
    if scale_name not in dataset_config["scales"]: raise ValueError(f"æ•°æ®é›† {dataset_name} ä¸æ”¯æŒå°ºåº¦: {scale_name}")
    scale_config = dataset_config["scales"][scale_name]
    model_params = dataset_config["base_model_params"].copy()
    model_params.update({"hidden_size": dataset_config["hidden_size"], "seq_lenth": scale_config["seq_lenth"], "c_out": scale_config["c_out"], "c_in": scale_config["c_in"]})
    return {"dataset_config": dataset_config, "scale_config": scale_config, "model_params": model_params}

if CONFIG["visualization"]["font_path"] and Path(CONFIG["visualization"]["font_path"]).exists():
    font_prop = fm.FontProperties(fname=CONFIG["visualization"]["font_path"])
    plt.rcParams['font.family'] = font_prop.get_name()
else:
    print("å­—ä½“è·¯å¾„æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“ã€‚")
plt.rcParams['font.size'] = CONFIG["visualization"]["font_size"]
plt.rcParams['axes.unicode_minus'] = False


# ==============================================================================
# 2. æ¨¡å‹å°è£… (Model Wrapper for SHAP) - ä¿æŒä¸å˜
# ==============================================================================
# ... (ä½ çš„ ModelWrapper ç±»ä¿æŒä¸å˜) ...
class ModelWrapper(torch.nn.Module):
    def __init__(self, model, edge_index_subset, device):
        super(ModelWrapper, self).__init__()
        self.model = model
        self.device = device
        if isinstance(edge_index_subset, list):
            self.edge_index = [edge.to(device) if isinstance(edge, torch.Tensor) else edge for edge in edge_index_subset]
        else:
            self.edge_index = edge_index_subset
    def forward(self, x):
        if isinstance(x, np.ndarray): x = torch.from_numpy(x).float()
        x = x.to(self.device)
        M_edge = self.edge_index[0:x.shape[0]]
        if isinstance(M_edge, list):
            M_edge = [edge.to(self.device) if isinstance(edge, torch.Tensor) else edge for edge in M_edge]
        return self.model(x, M_edge)

# ==============================================================================
# 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (Core Functional Components) - ä¿æŒä¸å˜
# ==============================================================================
# ... (ä½ çš„ load_data_and_model å’Œ calculate_shap_values å‡½æ•°ä¿æŒä¸å˜) ...
def load_data_and_model(dataset_name, scale_name, current_config):
    print(f"--- åŠ è½½æ•°æ®é›†å’Œæ¨¡å‹: {dataset_name}æ•°æ®é›† {scale_name}é¢„æµ‹")
    seed_everything(seed=CONFIG["seed"])
    dataset_config = current_config["dataset_config"]
    scale_config = current_config["scale_config"]
    model_params = current_config["model_params"]
    script_dir = CONFIG["script_dir"]
    data_dir = os.path.join(script_dir, f'../data/{dataset_name}')
    actual_dataset_name = scale_config["dataset_name"]
    train_dir = os.path.join(script_dir, f'../data/{dataset_name}/stdtrain_data{actual_dataset_name}.npy')
    val_dir = os.path.join(script_dir, f'../data/{dataset_name}/stdval_data{actual_dataset_name}.npy')
    csv_dir = os.path.join(script_dir, f'../data/{dataset_name}/{dataset_config["csv_file"]}')
    if not os.path.exists(train_dir): raise FileNotFoundError(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {train_dir}")
    if not os.path.exists(val_dir): raise FileNotFoundError(f"âŒ éªŒè¯æ•°æ®ä¸å­˜åœ¨: {val_dir}")
    # åŠ è½½åŸå§‹ç‰¹å¾å
    try:
        data = pd.read_csv(csv_dir, nrows=6)
        if dataset_name == "DSWE":
            df = data.drop(['Sequence No.'], axis=1)
            # å‡è®¾DSWEæ•°æ®é›†çš„æœ€åä¸€åˆ—æ˜¯'y'
            original_feature_names = df.columns
        else: # fujian
            df = data.drop(['Site_ID', 'Timestamp'], axis=1)
            # å‡è®¾fujianæ•°æ®é›†çš„æœ€åä¸€åˆ—æ˜¯'y'
            original_feature_names = df.columns

        print(f"âœ… Original feature names loaded: {original_feature_names}")
        
        # âœ¨ã€æ–°å¢ã€‘ä½¿ç”¨æ˜ å°„å­—å…¸ç¿»è¯‘ç‰¹å¾å
        feature_map = dataset_config.get("feature_map", {})
        translated_feature_names = [feature_map.get(name, name) for name in original_feature_names]
        print(f"âœ… Translated feature names for paper: {translated_feature_names}")
        
        current_config["model_params"]["c_in"] = len(translated_feature_names)
        
    except FileNotFoundError:
        print(f"Warning: Feature name CSV not found. Using generic names.")
        original_feature_names = [f'feature_{i+1}' for i in range(current_config["model_params"]["c_in"])]
        translated_feature_names = original_feature_names

    x_data = torch.tensor(np.load(train_dir), dtype=torch.float32)
    y_data_raw = np.load(val_dir)
    if scale_config["prediction_type"] == "å•æ­¥":
        y_data = torch.tensor(y_data_raw[:, 0, 0], dtype=torch.float32).unsqueeze(-1)
        current_config["model_params"]["c_out"] = 1
    else:
        y_data = torch.tensor(y_data_raw[:, :, 0], dtype=torch.float32)
        current_config["model_params"]["c_out"] = scale_config["c_out"]
    use_adaptive_graph = CONFIG["use_adaptive_graph"]
    if use_adaptive_graph:
        if dataset_name == "fujian": edge_dir = os.path.join(script_dir, f'../new_data/fujian/adag_dict_train_data{actual_dataset_name}_fused.pkl')
        elif dataset_name == "DSWE": edge_dir = os.path.join(script_dir, f'../new_data/DSWE/adag_dict_{actual_dataset_name}.pkl')
        else: edge_dir = os.path.join(script_dir, f'../new_data/{dataset_name}/adag_dict_{actual_dataset_name}.pkl')
        if not os.path.exists(edge_dir): use_adaptive_graph = False
        else:
            with open(edge_dir, 'rb') as f: edge_index = pickle.load(f)
            if len(edge_index) != len(x_data): use_adaptive_graph = False
    if not use_adaptive_graph:
        edge_index = create_full_graph_dict(len(x_data), x_data.shape[-1])
    split_index = int(len(x_data) * CONFIG["split_ratio"])
    X_train, X_test = x_data[:split_index], x_data[split_index:]
    y_train, y_test = y_data[:split_index], y_data[split_index:]
    train_dict, test_dict = edge_index[:split_index], edge_index[split_index:]
    auto_model = adap_auto(**model_params).to(CONFIG["device"])
    auto_model.eval()
    X_train, X_test = X_train.to(CONFIG["device"]), X_test.to(CONFIG["device"])
    wrapped_model = ModelWrapper(auto_model, test_dict, CONFIG["device"])
    return X_train, X_test, test_dict, wrapped_model, translated_feature_names, None, True

def calculate_shap_values(wrapped_model, X_train, X_test, current_config):
    print("--- è®¡ç®—SHAPå€¼ ---")
    background_data = X_train[np.random.choice(X_train.shape[0], CONFIG["background_samples"], replace=False)]
    if len(X_test) > CONFIG["test_samples"]:
        X_test_subset = X_test[np.random.choice(X_test.shape[0], CONFIG["test_samples"], replace=False)]
    else:
        X_test_subset = X_test
    explainer = shap.GradientExplainer(wrapped_model, background_data)
    shap_values = explainer.shap_values(X_test_subset)
    if isinstance(shap_values, list): shap_values = shap_values[0]
    if shap_values.ndim == 3 and shap_values.shape[-1] == 1: shap_values = shap_values.squeeze(-1)
    return shap_values, X_test_subset.cpu().numpy()

# ==============================================================================
# 4. å¯è§†åŒ–å‡½æ•° (Visualization Function) - âœ¨ã€æ ¸å¿ƒä¿®æ”¹å¤„ã€‘âœ¨
# ==============================================================================

def plot_feature_importance(shap_values, feature_names, dataset_name, scale_name, current_config):
    """
    ç”Ÿæˆå…·æœ‰æ”¾å¤§å…ƒç´ ã€ä¼˜åŒ–é…è‰²ã€ä½¿ç”¨æ–°ç½—é©¬å­—ä½“å¹¶ç¬¦åˆè‹±æ–‡ç§‘æŠ€è®ºæ–‡é£æ ¼çš„ç‰¹å¾é‡è¦æ€§æ£’æ£’ç³–å›¾ã€‚
    """
    print(f"--- ç”Ÿæˆä¼˜åŒ–çš„ç‰¹å¾é‡è¦æ€§å›¾: {dataset_name} Dataset, {scale_name} Prediction ---")

    if shap_values.ndim >= 2:
        axes_to_agg = tuple(i for i in range(shap_values.ndim) if i != shap_values.ndim - 2)
        global_importance = np.abs(shap_values).mean(axis=axes_to_agg)
    else:
        print(f"SHAP value shape ({shap_values.shape}) not supported for plotting.")
        return

    total_importance = np.sum(global_importance)
    percentages = (global_importance / total_importance) * 100 if total_importance > 0 else 0
    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': global_importance, 'Percentage': percentages}).sort_values(by='Importance', ascending=True)

    # è®¾ç½®æ–°ç½—é©¬å­—ä½“
    plt.rcParams['font.family'] = 'Times New Roman'
    plt.rcParams['font.size'] = 14 # å¢å¤§é»˜è®¤å­—ä½“å¤§å°

    fig, ax = plt.subplots(figsize=(10, 8))

    # ç»˜åˆ¶æ£’æ£’ç³–å›¾ï¼Œå¢å¤§çº¿æ¡å®½åº¦å’Œç‚¹çš„å¤§å°
    ax.hlines(
        y=importance_df['Feature'],
        xmin=0,
        xmax=importance_df['Importance'],
        color='#1B9E77', # è°ƒæ•´é¢œè‰² (æ›´é²œæ˜çš„è“è‰²)
        alpha=0.6,
        linewidth=5 # å¢å¤§çº¿æ¡å®½åº¦
    )
    ax.scatter(
        x=importance_df['Importance'],
        y=importance_df['Feature'],
        s=300, # å¢å¤§ç‚¹çš„å¤§å°
        color='#1B9E77',
        alpha=1,
        zorder=3
    )

    # æ·»åŠ ç²¾ç¡®çš„æ–‡æœ¬æ ‡æ³¨ (ç»å¯¹å€¼ + ç™¾åˆ†æ¯”)ï¼Œå¢å¤§å­—ä½“å¤§å°
    for index, row in importance_df.iterrows():
        ax.text(
            x=row['Importance'],
            y=row['Feature'],
            s=f"   {row['Importance']:.4f} ({row['Percentage']:.1f}%)",
            color='black',
            fontsize=18, # å¢å¤§æ ‡æ³¨å­—ä½“å¤§å°
            fontweight='normal',
            verticalalignment='center'
        )

    scale_config = current_config["scale_config"]
    display_name = scale_config.get("display_name", scale_name)
    ax.set_title(f'Feature Importance Analysis for {dataset_name}', fontsize=24, pad=25, weight='bold')
    ax.set_xlabel('Mean Absolute SHAP Value', fontsize=22, weight='bold')
    ax.set_ylabel('Feature', fontsize=22, weight='bold')

    ax.set_xlim(0, importance_df['Importance'].max() * 1.1) # ç•™å‡ºæ›´å¤šç©ºé—´ç»™æ–‡æœ¬
    ax.tick_params(axis='y', length=0, labelsize=18) # å¢å¤§Yè½´ç‰¹å¾åå­—ä½“
    ax.tick_params(axis='x', labelsize=18)

    # æ›´æŸ”å’Œçš„ç½‘æ ¼çº¿
    ax.grid(axis='x', linestyle='--', alpha=0.4)

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    plt.tight_layout()

    output_dir = CONFIG["output_dir"] / dataset_name
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f'feature_importance_optimized_{scale_name}.pdf'
    fig.savefig(output_path, dpi=CONFIG["visualization"]["dpi"], bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… Saved optimized feature importance plot to: {output_path}")

def plot_importance_stacked_barchart(shap_values, feature_names, dataset_name, scale_name, current_config):
    """
    ç”Ÿæˆä¸€ä¸ªç‰¹å¾é‡è¦æ€§è´¡çŒ®åº¦çš„100%å †å æ¡å½¢å›¾ã€‚
    """
    print(f"--- ç”Ÿæˆå †å æ¡å½¢å›¾ä»¥ä¾›å‚è€ƒ: {dataset_name}æ•°æ®é›† {scale_name}é¢„æµ‹ ---")

    # 1. èšåˆSHAPå€¼ï¼Œè®¡ç®—æ¯ä¸ªç‰¹å¾çš„å…¨å±€é‡è¦æ€§
    if shap_values.ndim >= 2:
        axes_to_agg = tuple(i for i in range(shap_values.ndim) if i != shap_values.ndim - 2)
        global_importance = np.abs(shap_values).mean(axis=axes_to_agg)
    else:
        print(f"SHAPå€¼å½¢çŠ¶ ({shap_values.shape}) ä¸æ”¯æŒï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨ã€‚")
        return

    # 2. è®¡ç®—è´¡çŒ®åº¦ç™¾åˆ†æ¯”å¹¶æ’åº
    total_importance = np.sum(global_importance)
    if total_importance == 0:
        print("æ€»é‡è¦æ€§ä¸º0ï¼Œæ— æ³•ç”Ÿæˆå †å æ¡å½¢å›¾ã€‚")
        return
        
    percentages = (global_importance / total_importance) * 100
    
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'percentage': percentages
    }).sort_values(by='percentage', ascending=False) # æŒ‰é‡è¦æ€§é™åºæ’åˆ—

    # 3. å‡†å¤‡ç»˜å›¾æ•°æ®å’Œé¢œè‰²
    labels = importance_df['feature']
    sizes = importance_df['percentage']
    num_features = len(labels)
    # ä½¿ç”¨tab20è¿™ç§å¤šé¢œè‰²colormapï¼Œé€‚åˆåŒºåˆ†å¤šä¸ªç±»åˆ«
    colors = plt.cm.get_cmap('tab20', num_features)(range(num_features)) 

    # 4. å¼€å§‹ç»˜å›¾
    fig, ax = plt.subplots(figsize=(15, 5))
    
    left = 0 # æ¯ä¸€å—çš„èµ·å§‹ä½ç½®
    for i, (label, size) in enumerate(zip(labels, sizes)):
        # ç»˜åˆ¶å•ä¸ªè‰²å—
        ax.barh(y=0, width=size, height=0.5, left=left, color=colors[i], label=label, edgecolor='white')
        
        # åœ¨è‰²å—ä¸­é—´æ·»åŠ æ–‡æœ¬æ ‡ç­¾
        text_color = 'white' if size > 7 else 'black' # å¦‚æœè‰²å—å¤ªå°ï¼Œæ–‡å­—æ”¾å¤–é¢å¯èƒ½æ›´å¥½ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        ax.text(left + size/2, 0, f'{size:.1f}%', ha='center', va='center', color=text_color, fontsize=9, weight='bold')
        
        left += size # æ›´æ–°ä¸‹ä¸€ä¸ªè‰²å—çš„èµ·å§‹ä½ç½®

    # 5. ç¾åŒ–å›¾è¡¨
    scale_config = current_config["scale_config"]
    display_name = scale_config.get("display_name", scale_name)
    ax.set_title(f'Feature Importance Composition - {dataset_name} ({display_name})', fontsize=16, pad=20, weight='bold')
    
    # è®¾ç½®Xè½´
    ax.set_xlim(0, 100)
    ax.set_xlabel('Contribution Percentage (%)', fontsize=12)
    
    # éšè—Yè½´ï¼Œå› ä¸ºå®ƒæ²¡æœ‰æ„ä¹‰
    ax.get_yaxis().set_visible(False)
    
    # ç§»é™¤è¾¹æ¡†
    for spine in ['top', 'bottom', 'left', 'right']:
        ax.spines[spine].set_visible(False)
        
    # æ·»åŠ å›¾ä¾‹
    ax.legend(title='Features', loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)

    plt.tight_layout()

    # 6. ä¿å­˜å›¾è¡¨
    output_dir = CONFIG["output_dir"] / dataset_name
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / f'stacked_barchart_reference_{scale_name}.pdf'
    fig.savefig(output_path, dpi=CONFIG["visualization"]["dpi"], bbox_inches='tight')
    plt.close(fig)
    print(f"âœ… å·²ä¿å­˜å †å æ¡å½¢å›¾(å‚è€ƒ)åˆ°: {output_path}")
# ==============================================================================
# 5. ä¸»æ‰§è¡Œå‡½æ•° (Main Execution) - âœ¨ã€æ ¸å¿ƒä¿®æ”¹å¤„ã€‘âœ¨
# ==============================================================================
def main():
    """ä¸»æ‰§è¡Œæµç¨‹ - åŒæ—¶ç”Ÿæˆé«˜è´¨é‡æ£’æ£’ç³–å›¾å’Œå †å æ¡å½¢å›¾ä»¥ä¾›å¯¹æ¯”ã€‚"""
    print("ğŸ”¬ å¼€å§‹SHAPç‰¹å¾é‡è¦æ€§åˆ†æ...")
    
    CONFIG["output_dir"].mkdir(exist_ok=True)
    target_datasets = [CONFIG["target_dataset"]] if isinstance(CONFIG["target_dataset"], str) else CONFIG["target_dataset"]

    for dataset_name in target_datasets:
        dataset_output_dir = CONFIG["output_dir"] / dataset_name
        dataset_output_dir.mkdir(exist_ok=True)
        print(f"\n{'='*80}\nğŸ“‚ å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_name}\n{'='*80}")

        for scale_name in CONFIG["target_scales"]:
            try:
                print(f"\n--- ğŸ“ˆ å¤„ç†å°ºåº¦: {scale_name} ---")
                
                # 1. åŠ è½½æ•°æ®å’Œæ¨¡å‹
                current_config = get_current_config(dataset_name, scale_name)
                X_train, X_test, _, wrapped_model, feature_names, _, _ = load_data_and_model(dataset_name, scale_name, current_config)

                # 2. è®¡ç®—SHAPå€¼
                shap_values, X_test_subset = calculate_shap_values(wrapped_model, X_train, X_test, current_config)

                # 3. ç”Ÿæˆå¹¶ä¿å­˜ä¸¤ç§å›¾è¡¨
                
                # 3a. ç”Ÿæˆä¸»è¦çš„ã€æ¨èç”¨äºè®ºæ–‡çš„æ£’æ£’ç³–å›¾
                plot_feature_importance(
                    shap_values, feature_names, dataset_name, scale_name, current_config
                )

                # 3b. âœ¨ã€æ–°å¢è°ƒç”¨ã€‘âœ¨ ç”Ÿæˆå †å æ¡å½¢å›¾ï¼Œä½œä¸ºå¯¹æ¯”å‚è€ƒ
                # plot_importance_stacked_barchart(
                #     shap_values, feature_names, dataset_name, scale_name, current_config
                # )

            except FileNotFoundError as e:
                print(f"âŒ å¤„ç†æ•°æ®é›† {dataset_name}-{scale_name} æ—¶å‡ºé”™: {e}ã€‚è·³è¿‡æ­¤é…ç½®ã€‚")
                continue
            except Exception as e:
                print(f"ğŸ’¥ å¤„ç†æ•°æ®é›† {dataset_name}-{scale_name} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}ã€‚è·³è¿‡æ­¤é…ç½®ã€‚")
                import traceback
                traceback.print_exc()
                continue

    print("\nâœ… SHAPç‰¹å¾é‡è¦æ€§åˆ†æå…¨éƒ¨å®Œæˆ!")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {CONFIG['output_dir']}")

if __name__ == '__main__':
    main()